{
  "hash": "bfee528e616a502b32be5b660db88dac",
  "result": {
    "markdown": "---\ntitle: \"Stop Using Excel to Preprocess E-Prime Data\"\nauthor: \"Matthew J. Kmiecik\"\ndate: \"18 March 2020\"\ndescription: \"A walk-through of how to process E-Prime output with R.\"\nimage: \"./thumbnail.png\"\ncategories:\n  - Academic\n  - R\n  - E-Prime\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    toc-location: left\n    smooth-scroll: true\n---\n\n\n\n\n<br >\n\nExcel is a great tool for processing data. Whether it's calculating a quick average, standard deviation, or t-test, Excel is fast and simple to learn and use. In fact, most of my colleagues use Excel to preprocess their data from experiments programmed in E-Prime, a software tool for running psychological experiments. However, preprocessing your E-Prime data in Excel will:\n\n1. limit the statistical methods used to analyze your data\n2. become burdensome with large samples ( > 20 participants)\n3. create large workbooks that are slow and susceptible to crashes\n4. create roadbumps down the line when reviewer #2 asks for a new analysis (I swear, it's always reviewer #2)\n\nI'll demonstrate a different method on how to preprocess your E-Prime data with R and the R package dplyr (Wickham & Francois, 2016) using a small data set from a real experiment of mine. This method is much faster, efficient, and saves soooooo much time when it comes to reviewer #2's requests.\n\nJust a few notes about the experiment: participants completed a task that required them to reason about items on the screen and press #1 or #2 on the keyboard. There were 3 conditions randomly distributed over 4 blocks with 24 trials per block (8 trials per condition). I was interested in the participants' accuracy and reaction time for correct solutions across the conditions and blocks. Therefore, each participant has only 1 E-Prime file.\n\n# Step 1 - Merging E-DataAid Files\n\nE-Prime spat out an E-DataAid file (*.edat2) for every participant upon completion of the experiment. Let's first concatenate these files row-wise (on top of each other) so that we end up with one big file that has each participant's data. This is done using  E-Prime's E-Merge software.\n\n## Concatenating with E-Merge\n\n1. Open E-Merge and navigate to the directory where all the E-DataAid files are stored using the Folder Tree\n2. Select all the E-DataAid files and click Merge...\n3. Choose the appropriate option and click Next >:\n    A) Standard Merge if all your files are in one directory (**option chosen in this tutorial**)\n    B) Recursive Merge if all your files are stored in folders within folders\n4. Name your file and save it as a *.emrg2 file (the default)\n\nAs long as your E-DataAids are consistent with each other, they should seamlessly merge. Next, we have to convert this to a format R and other programs can read using E-DataAid:\n\n## Converting Merged E-DataAid\n\n1. Double click on the \\*.emrg2 file that you just created\n2. Go to File > Export\n3. Ensure the \"Export to:\" option is: StatView and SPSS\n4. **Ensure that the Unicode box at the bottom is unchecked**\n5. Click OK and name/save the file as a \\*.txt (the default)\n\nNow these data are in one central file and prepared for R. Next, let's import into R:\n\n# Step 2 - Importing into R\n\n1. Open R or RStudio and **ensure your working directory is set to where you saved your text file (.txt) from above.**\n2. Load the `tidyverse`. For more information about the tidyverse click [here](https://www.tidyverse.org/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n3. Import the file into R and save it as a variable:\n\n\n::: {.cell}\n\n```{.r .cell-code}\neData <- read_delim(\n  'data.txt',   # The name of your *.txt file from above\n  delim = '\\t', # These data are tab separated\n  ) \n```\n:::\n\n\n:::{.callout-tip}\n`read_delim()` has several arguments, so be sure to check them out using `?read_delim()`. One really useful one is `guess_max=` which will search the first _n_ rows of the data to gather the column type. For really long experiments, this option should be set to the number of rows generated for a single participant, that way it can scan at least an entire participant's data file before \"guessing\" the column type. \n:::\n\nFor the purposes of this tutorial, these data are available on this website's [GitHub repository](https://github.com/mkmiecik14/mkmiecik14.github.io) and can be downloaded into R like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr) # To retrive data from github repo\nlink <- \"https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/data.txt\"\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata <- GET(link)\neData <- read_delim(content(data, \"raw\"), delim = \"\\t\")\ndim(eData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> [1] 480  47\n```\n:::\n:::\n\n\nAs we can see, this dataframe has 480 rows and 47 columns. Each row is a trial from a participant and each column is a measure/information from your E-prime experiment. E-prime gives us way too much information, so I like to clean this up and only include the essentials (*Note: These variable names will vary based on your experiment*):\n\n\n::: {.cell}\n\n```{.r .cell-code}\neData_simple <- \n  eData %>%                  # original data\n  select(\n    ss = Subject,            # subject\n    cond = probType,         # condition\n    acc = stimulus.ACC,      # accuracy\n    rt = stimulus.RT         # reaction time\n    )\n\n# Let's take a look\nhead(eData_simple); tail(eData_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 6 × 4\n>      ss cond    acc    rt\n>   <dbl> <chr> <dbl> <dbl>\n> 1   413 a         1  5578\n> 2   413 s         0  9889\n> 3   413 a         0  4218\n> 4   413 p         1  1376\n> 5   413 s         0  6169\n> 6   413 p         1  2663\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 6 × 4\n>      ss cond    acc    rt\n>   <dbl> <chr> <dbl> <dbl>\n> 1   416 s         1  4671\n> 2   416 a         1  2765\n> 3   416 s         1 13400\n> 4   416 p         1   957\n> 5   416 a         1  2112\n> 6   416 s         1  8193\n```\n:::\n:::\n\n\n\n:::{.callout-tip}\nThe `select()` function allows you to rename the column names on the fly. Pretty sweet if you ask me!\n:::\n\nI've printed the top and bottom 6 rows of this dataframe. As you can see, the first participant's ID is 413, while the last participant's ID is 416. Each trial is a row and has a problem type (either p, a, or s), an accuracy (1 for correct, 0 for incorrect), as well as an associated reaction time (RT) measure in milliseconds. Now, I forgot to program the block each trial appeared in my E-Prime experiment, but I can add it like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Adding block\neData_simple <- \n  eData_simple %>% \n  group_by(ss) %>% \n  mutate(block = rep(c(1:4), each = 24)) %>% # 24 trials/block\n  ungroup()\n           \n# Let's take a look\nhead(eData_simple); tail(eData_simple)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 6 × 5\n>      ss cond    acc    rt block\n>   <dbl> <chr> <dbl> <dbl> <int>\n> 1   413 a         1  5578     1\n> 2   413 s         0  9889     1\n> 3   413 a         0  4218     1\n> 4   413 p         1  1376     1\n> 5   413 s         0  6169     1\n> 6   413 p         1  2663     1\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 6 × 5\n>      ss cond    acc    rt block\n>   <dbl> <chr> <dbl> <dbl> <int>\n> 1   416 s         1  4671     4\n> 2   416 a         1  2765     4\n> 3   416 s         1 13400     4\n> 4   416 p         1   957     4\n> 5   416 a         1  2112     4\n> 6   416 s         1  8193     4\n```\n:::\n:::\n\n\nNow these data are in a great format to summarize using the `dplyr` package included in the `tidyverse`.\n\n# Step 3 - Summarize with dplyr\n\nLet's calculate the mean accuracy for condition x block (3 x 4 repeated-measures factorial design). But first, let's group based on our factors, which are 1) the subjects, 2) the conditions, and 3) the blocks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc <- \n  eData_simple %>% \n  group_by(ss, cond, block) %>%\n  summarise(\n    m = mean(acc),    # mean\n    n = n()           # number of trials\n    ) %>%               \n  ungroup()           # generally a good idea to ungroup when finished\n\nprint(acc) # prints to screen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 60 × 5\n>       ss cond  block     m     n\n>    <dbl> <chr> <int> <dbl> <int>\n>  1   413 a         1 0.875     8\n>  2   413 a         2 1         8\n>  3   413 a         3 1         8\n>  4   413 a         4 0.75      8\n>  5   413 p         1 0.75      8\n>  6   413 p         2 1         8\n>  7   413 p         3 1         8\n>  8   413 p         4 1         8\n>  9   413 s         1 0.375     8\n> 10   413 s         2 0.5       8\n> # … with 50 more rows\n```\n:::\n:::\n\n\nNow to look at task-wide performance, we group by condition and block and repeat:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nacc_sum <-\n  acc %>%\n  group_by(cond, block) %>%\n  summarise(\n    M = mean(m),       # mean\n    SD = sd(m),        # standard deviation\n    N = n(),           # number of observations\n    sem = SD/sqrt(N)   # standard error of the mean\n    ) %>%\n  ungroup()            # generally a good idea to regroup\n```\n:::\n\n\n:::{.callout-tip}\n`summarise()` can compute variables defined within `summarise()`. See above for how this works for the standard error of the mean (sem).\n:::\n\nThese are the summary results:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(acc_sum) # prints to screen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 12 × 6\n>    cond  block     M     SD     N    sem\n>    <chr> <int> <dbl>  <dbl> <int>  <dbl>\n>  1 a         1 0.875 0.0884     5 0.0395\n>  2 a         2 1     0          5 0     \n>  3 a         3 1     0          5 0     \n>  4 a         4 0.95  0.112      5 0.05  \n>  5 p         1 0.95  0.112      5 0.05  \n>  6 p         2 1     0          5 0     \n>  7 p         3 1     0          5 0     \n>  8 p         4 1     0          5 0     \n>  9 s         1 0.775 0.240      5 0.108 \n> 10 s         2 0.625 0.125      5 0.0559\n> 11 s         3 0.625 0.0884     5 0.0395\n> 12 s         4 0.85  0.0559     5 0.025\n```\n:::\n:::\n\n\nNow what about that pesky reviewer #2? Let's say reviewer #2 asks for a new analysis that, instead of looking at reaction time, asks for the results with reaction time for only correct solutions? If you preprocessed your data in Excel, you would probably have to re-compute all these values in each sheet and then re-do the analyses. But this is simple in R and is only one additional line of code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorRT <-  \n  eData_simple %>%\n  filter(acc == 1) %>%  # filters out inaccurate trials\n  group_by(ss, cond, block) %>%\n  summarise(\n    m = mean(rt),       # mean\n    n = n()             # number of trials\n    ) %>%\n  ungroup()             # generally a good idea to ungroup when finished\n\nprint(corRT)            # prints to screen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 60 × 5\n>       ss cond  block     m     n\n>    <dbl> <chr> <int> <dbl> <int>\n>  1   413 a         1 5999      7\n>  2   413 a         2 2393.     8\n>  3   413 a         3 2517.     8\n>  4   413 a         4 2069      6\n>  5   413 p         1 5792.     6\n>  6   413 p         2 3228.     8\n>  7   413 p         3 1616.     8\n>  8   413 p         4 1113      8\n>  9   413 s         1 7689.     3\n> 10   413 s         2 4858.     4\n> # … with 50 more rows\n```\n:::\n:::\n\n\nFor completeness, here's how to compute the task-wide correct RT performance:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorRT_sum <-\n  corRT %>%\n  group_by(cond, block) %>%\n  summarise(\n    M = mean(m),     # mean \n    SD = sd(m),      # standard devation\n    N = n(),         # number of observations\n    sem = SD/sqrt(N) # standard error of the mean\n    ) %>%\n  ungroup()\n\nprint(corRT_sum)     # prints to screen\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n> # A tibble: 12 × 6\n>    cond  block     M    SD     N   sem\n>    <chr> <int> <dbl> <dbl> <int> <dbl>\n>  1 a         1 5261. 1019.     5  456.\n>  2 a         2 3380. 1303.     5  583.\n>  3 a         3 3379. 1209.     5  541.\n>  4 a         4 3022.  889.     5  397.\n>  5 p         1 5525. 1429.     5  639.\n>  6 p         2 3292. 1028.     5  460.\n>  7 p         3 2667. 1632.     5  730.\n>  8 p         4 2108.  942.     5  421.\n>  9 s         1 8324. 1707.     5  763.\n> 10 s         2 6682. 1865.     5  834.\n> 11 s         3 6315. 2780.     5 1243.\n> 12 s         4 5400. 1486.     5  664.\n```\n:::\n:::\n\n\nTo summarize, the entire R script to process these data is quite concise and can accommodate *many* more participants with ease.\n\nAfter programming your experiment with the E-Prime beast, dragging undergraduate participants through your study, and wrangling the data into one place, why not make your life easier? Ditch the Excel templates. You'll thank me when reviewer #2 comes around!\n\n# Plotting\n\nProcessing E-Prime data this way puts it in a great format for plotting with `ggplot2` — another package in the `tidyverse`.\n\nI like to plot the raw data underneath the summary data, so here is an example plot for correct reaction times:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Correct RT plot\npj <- position_jitter(width = .1)\nggplot(corRT_sum, aes(block, M, group = cond, color = cond)) +\n  geom_point(data = corRT, aes(y = m), position = pj, shape = 1, alpha = 1/2) +\n  geom_point() +\n  geom_errorbar(aes(ymin = M-sem, ymax = M+sem), width = .1) +\n  geom_line() +\n  scale_y_continuous(limits = c(0, 12000)) +\n  scale_color_brewer(palette = \"Dark2\") +\n  labs(x = \"Block\", y = \"Mean Correct RT (ms)\", caption = \"SEM error bars.\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](post-Stop-Using-Excel-to-Preprocess-E-Prime-Data_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nAnd here's an example of how I could inspect RT curves per condition and block for all participants (trial-wise). A great way to detect outliers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbnwidth <- 1 # in seconds\nggplot(eData_simple, aes(rt/1000)) +\n  geom_histogram(binwidth = bnwidth) +\n  labs(\n    x = \"RT (seconds)\", \n    y = \"Frequency\", \n    caption = paste(\"Binwidth = \", bnwidth, \"sec.\")\n    ) +\n  theme_minimal() +\n  facet_grid(cond~block)\n```\n\n::: {.cell-output-display}\n![](post-Stop-Using-Excel-to-Preprocess-E-Prime-Data_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nComparisons across conditions are faciltiated using a density plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(eData_simple, aes(rt/1000, group = cond, color = cond)) +\n  geom_density() +\n  labs(x = \"RT (seconds)\", y = \"Density\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  theme_minimal() +\n  facet_wrap(~block)\n```\n\n::: {.cell-output-display}\n![](post-Stop-Using-Excel-to-Preprocess-E-Prime-Data_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nGood luck with wrangling the e-prime beast and I hope you enjoyed this (updated) tutorial!\n\n# Acknowledgments\n\nThis tutorial was inspired by Dr. Jahn's amazing [blog](http://andysbrainblog.blogspot.com/) that helped me and I'm sure hundreds of other graduate students stumble through the crazy world that is fMRI analysis. Andy's Brain Blog is the best!\n\n# References\n\nWickham H, Averick M, Bryan J, Chang W, McGowan LD, François R, Grolemund G, Hayes A, Henry L, Hester J, Kuhn M, Pedersen TL, Miller E, Bache SM, Müller K, Ooms J, Robinson D, Seidel DP, Spinu V, Takahashi K, Vaughan D, Wilke C, Woo K, Yutani H (2019). “Welcome to the tidyverse.” _Journal of Open Source Software_, **4**(43), 1686. doi: [10.21105/joss.01686](https://joss.theoj.org/papers/10.21105/joss.01686). \n",
    "supporting": [
      "post-Stop-Using-Excel-to-Preprocess-E-Prime-Data_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}