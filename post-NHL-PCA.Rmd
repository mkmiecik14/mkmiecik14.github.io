---
title: "NHL PCA"
author: "Matthew J. Kmiecik & Ekarin E. Pongpipat"
date: "date here"
output:
  html_document:
    highlight: zenburn
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: yes
      smooth_scroll: yes
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center')
options(knitr.table.format = 'html') # For the html tables
```

## Setup
<hr >

Here are the packages that we'll use for these analyses and to style the output/plots.

```{r, warning=FALSE, message=FALSE}
# Packages ----
library(tidyverse)    # For data manipulation, plotting, etc. 
library(RCurl)        # To import data on github
library(TInPosition)  # PCA tools
library(ggrepel)      # Plotting tool for ggplot2
library(kableExtra)   # HTML table tools
library(RColorBrewer) # Nice plotting colors

# Custom functions ----
# nice_table() simplifies the printing of HTML tables using kable
nice_table <- function(x){
  
  kable(x) %>%
    kable_styling(bootstrap_options = c('striped', 'hover', 'responsive'))
  
}

# Color palettes ----
rdgy <- brewer.pal(n = 11, name = "RdGy") # display.brewer.pal(11, "RdGy")

# ggplot2 finishings
pca_furnish <- theme_classic() +
  theme(axis.title=element_blank(),
        axis.ticks=element_blank(),
        axis.line = element_blank()
        )
```

Let's first prepare the data for analysis. These data were downloaded from [Corsica's](http://corsica.hockey/) team stats tool. We've prepared these data for you and are available for import into R like this:

```{r import}
# Link to raw data on Github
link <- "https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/nhl-team-data-corsica.csv"

# reads in data from Github
nhl_data <- read_csv(file = getURL(link))
```

To make things as simple as possible, without losing information, we'll use Chicago Blackhawk's data from the 2007-2008 season up through the 2017-2018 season (11 years of data) and only a subset of the available metrics. These include:

+ Games Played (GP)
+ Time on Ice (TOI)
+ Corsi For (CF) = Shot attempts for at even strength: Shots + Blocks + Misses
+ Corsi Against (CA) = Shot attempts against at even strength: Shots + Blocks + Misses
+ Goals For (GF)
+ Goals Against (GA)
+ Pentaly minutes served (PENT)
+ Penalty minutes drawn (PEND)
+ Shooting Percentage (ShootPerc)
+ Save Percentage (SavePerc)

```{r}
# Preparing Hawks data ----
hawks_data <-  nhl_data %>%
  select(Team:CA, GF, GA, PENT, PEND, ShootPerc = `Sh%`, SavePerc = `Sv%`) %>%
  filter(Team == "CHI") %>%
  separate(Season, into = c("Start_Year", "Season")) %>%
  mutate(Team = NULL, 
         Start_Year = NULL,
         Season = as.numeric(Season)
         )

# Prints data
nice_table(hawks_data)
```

Let's also prepare a table of notable events of every year in this data set for the Chicago Blackhawks, including their Stanley Cup wins (3) and their playoff success:

```{r}
# Initializing Chicago Blackhawks notable events table ----
# SCW = Stanley Cup Wins
# PF = Playoff Finish:
#   0 = Did not make playoffs
#   1 = Lost in first round
#   2 = Lost in second round
#   3 = Lost in conference finals
#   4 = Lost in Stanley Cup final
#   5 = Won Stanley Cup
hawks_events <- tibble(Season = hawks_data$Season,
                       SCW = factor(c(0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0)),
                       PF  = factor(c(0, 3, 5, 1, 1, 5, 3, 5, 1, 1, 0))
                       )
```

Prior to exploring these data and how they've changed over time, we have to:

1. Adjust all scores by the number of games played due to a shorted 2012-2013 season. We do this by dividing each metric by the number of games played.
2. Compute z-scores of all measures to facilitate comparisons

```{r}
# Preprocesses, adjusts, and z-scores hawks data ----
hawks_data_long <- hawks_data %>% 
  gather(Meas, Val, -Season, -GP) %>%
  group_by(Meas) %>%
  mutate(Val_Adj = Val/GP,               # adjusts based on games played
         Val_Zscore = scale(Val_Adj)     # computes z-scores
         ) %>%
  ungroup() %>%
  mutate(sig = factor(ifelse(abs(Val_Zscore) > 1.96, "p < .05", "p > .05"))) %>% # z score > 1.96
  inner_join(., hawks_events, by = "Season") # adds notable hawks events

# Plots all measures together ----
ggplot(hawks_data_long, aes(factor(Season), Val_Zscore)) +
  geom_path(aes(group = 1), color = rdgy[8]) +
  geom_point(aes(color = sig, shape = SCW), size = 1.75) +
  scale_color_manual(values = c(rdgy[3], rdgy[10]), name = "Z-Score") +
  scale_shape_discrete(name = "Stanley Cup Wins") +
  coord_cartesian(ylim = c(-3,3)) +
  theme_minimal() + 
  labs(x = "Season", 
       y = "Measurement (Z-Score)",
       title = "Chicago Blackhawk's Performance 2007-2018"
       ) +
  facet_wrap(~Meas, nrow = 3) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

[small blurb here describing the z score plots]

## Ekarin's version

```{r}
# adjust metric
hawks_data_adj <- hawks_data %>%
  rowwise() %>%
  mutate(GP_adj = TOI/GP,
         CF_adj = CF/GP,
         GF_adj = GF/GP,
         GA_adj = GA/GP,
         PENT_adj = PENT/GP,
         PEND_adj = PEND/GP,
         ShootPerc_adj = ShootPerc/GP,
         SavePerc_adj = SavePerc/GP) %>%
  ungroup() %>%
  select(Season, contains("_adj")) %>%
  inner_join(., hawks_events, by = "Season") %>%
  apply(., 2, as.numeric) %>%
  apply(., 2, scale)

# correlation matrix
hawks_cor_mat <- cor(hawks_data_adj)

# convert to long format
hawks_cor_long <- hawks_cor_mat %>%
  reshape2::melt() %>%
  arrange(Var1, Var2)

# heatmap
ggplot(hawks_cor_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_raster() + 
  labs(x = element_blank(),
       y = element_blank(),
       fill = "Correlation") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_distiller(palette = "RdBu", limits = c(-1,1))
```

## Matt's version

Now let's examine how these measurements relate to each other by computing their correlations and visualizing them with a heatmap.

```{r}
# Converts back to wide format
hawks_data_wide <- hawks_data_long %>% 
  select(Season, Meas, Val_Zscore) %>% 
  spread(Meas, Val_Zscore)

# Computes correlations
hawks_cors <- cor(hawks_data_wide)

# Correlations to long format for plotting
hawks_cors_long <- hawks_cors %>%
  reshape2::melt() %>%
  arrange(Var1, Var2)

# Correlation heatmap
ggplot(hawks_cors_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_raster() + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_blank()
        ) +
  scale_fill_distiller(palette = "BrBG", 
                       limits = c(-1, 1), 
                       name = "Correlation"
                       )
```

[ a little blurb here about the patterns that emerge from the heatmap ]

## PCA

We've explored these measures individually by plotting their change over time (2007-2018) as well as examining their correlations; however, it's difficult to extract meaningful relationships between these variables, especially when there are a lot of variables to consider. Also, all realtionships between the variables are not considered simultaneously, but rather one at a time (e.g., bivariate correlations). Is there some way to analyze these variables' relationships simultaneously, while also reducing their complexity?

One method of doing this is principal components analysis (PCA). PCA is often called a data reduction technique because it reduces the data structure into manageable "components" that explain proportions of variability in the data. In order to understand what each component means, we examine how information (e.g., years or measurements) is spread across the components.

Additionally, the PCA that we will demonstrate below places a constraint on the components such that they are orthogonal to each other, meaning that each component is perfectly uncorrelated with every other component (_r_ = 0).

We go over the very basics of PCA below. There is much more to this technique than we present, so we recommend the interested reader the following papers on this statistical technique:

+ Abdi, H., & Williams, L.J. (2010). [Principal component analysis](https://www.utdallas.edu/~herve/abdi-awPCA2010.pdf). _Wiley Interdisciplinary Reviews: Computational Statistics_, 2, 433-459.

+ Abdi, H. (2007). [Singular Value Decomposition (SVD) and Generalized Singular Value Decomposition (GSVD)](https://www.utdallas.edu/~herve/Abdi-SVD2007-pretty.pdf). In N.J. Salkind (Ed.): _Encyclopedia of Measurement and Statistics_. Thousand Oaks (CA): Sage. pp. 907-912.

We first need to preprocess our data such that each column (e.g., measurement) has a mean of 0 and a standard deviation of 1. In other words, each column should be in z-score format. We've already done this above when exploring these data.

A unique feature of this dataset is the shortened 2012 - 2013 NHL season that we accounted for by scaling each column by the number of games played that season. We already did this above and do not need to repeat this step.

Therefore, the first step here is to convert these data into a matrix format with the season years on the rows as row names:

```{r}
# Converts to matrix
hawks_data_mat <- hawks_data_wide %>% select(-Season) %>% as.matrix()
rownames(hawks_data_mat) <- hawks_data_wide$Season # Adds rownames

nice_table(hawks_data_mat) # Prints HTML table
```

Next we'll decompose this matrix using a singular value decomposition (SVD), the mathematical procedure at the heart of PCA. A SVD will decompose our original matrix (X) into 3 separate matrices (U, \Delta, V). The original data matrix can be reconstructed via matrix multiplication/linear algebra of these 3 matrices:

<font style="font-size: 14pt">
$$
\mathbf{X = U \Delta V^T}
$$

Briefly, U contains information about the rows (e.g., years), V contains information about the columns (e.g., measures), and \Delta is a diagonal matrix of "weights" called singular values that are the square root of the eigenvalues.

Computing the SVD of our data matrix in R requires one line of code:

```{r}
hawks_data_svd <- svd(hawks_data_mat) # singular value decomposition (SVD)
```

### Eigen Values

Our original data matrix was 11 rows (years) x 9 columns (measures); therefore, the SVD of that matrix will produce 9 singular values/components -- with the smaller side dictating the number of components produced.

Which components are important? Which ones explain the most variance? Are some of the components just noise?

A good first pass at these questions is examine the scree plot -- plotting the components as a function of variability explained.

To do this, let's first square the singular values and sum them together. This is called __inertia__.

```{r}
inertia <- sum(hawks_data_svd$d^2) # Calculates inertia
```

Next, we'll use the inertia to calculate the percentage of variability explained for each component and plot the scree:

```{r}
# Calculates values for the scree plot
scree <- tibble(eigs = hawks_data_svd$d^2, 
                perc_explained = (eigs/inertia)*100,
                comps = 1:length(eigs)
                )

# Scree plot
ggplot(scree, aes(factor(comps), eigs)) +
  geom_point() +
  geom_path(aes(group = 1)) +
  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, 
                                         name = "Percentage of Explained Variance"
                                         )
                     ) +
  labs(x = "Components", y = "Eigenvalues") +
  theme_minimal()
```

As we can see from the scree plot, the first 3 components comprise `r round(sum(scree$perc_explained[1:3]), 2)`% of the total variance, suggesting that these first three components are important to understanding the structure of the data set. The remaining components explain only `r round(sum(scree$perc_explained[4:9]), 2)`% of the variability and may perhaps be noise.

Is there any way to statistically show that these components are important? One method is permutation testing. If you unfamiliar with permutation testing, we recommend checking out our Shiny dashboard [here](https://mattkmiecik.shinyapps.io/boot-perm-dash/).

Briefly, to perform permutation testing we will scramble the information down the columns, thus breaking the relationship between the years and their measures. Then, we will re-compute the SVD for the new scrambled data matrix. We will repeat these steps 2,000 times, forming a null distribution of eigen values from which to compare our originally observed eigenvalues.

```{r}
perm_iters <- 2000  # number of permutation iterations
set.seed(2019)      # sets seed for reproducible results

# Initializes matrix to hold permutation results
perm_res <- matrix(data = 0, nrow = perm_iters, ncol = length(hawks_data_svd$d))

for(i in 1:perm_iters){
  
  this_matrix <- apply(hawks_data_mat, 2, sample) # scrambles down columns
  perm_res[i,] <- svd(this_matrix)$d^2 # saves eigenvalues to perm_res
  
}
```

Now let's visualize these results against the observed values. We'll determine that a component is significant if its original observed eigenvalue is greater than 95% of the values derived from the null distribution (i.e., permutation testing).

```{r}
perm_res_long <- as_tibble(perm_res) %>% 
  gather(comps, eigen) %>%
  mutate(comps = as.numeric(gsub("V", "", comps)))

ggplot(perm_res_long, aes(eigen)) +
  geom_histogram(binwidth = 1) +
  geom_vline(data = scree, 
             aes(xintercept = eigs), 
             color = rdgy[3], 
             linetype = 2
             ) +
  coord_cartesian(ylim = c(0, 800)) +
  labs(x = "Eigenvalue", 
       y = "Frequency", 
       caption = "\nNote: Originally observed eigenvalues denoted by red dashed line."
       ) +
  facet_wrap(~comps) +
  theme_minimal()
```

It looks like the only components that have a shot at being greater than 95% of the null distribution are components 1 and 2. Let's see if this is the case:

```{r}
scree_sig <- perm_res_long %>% 
  group_by(comps) %>% 
  summarise(ul = quantile(eigen, .975)) %>% # computes upper limit of 95%
  inner_join(., scree, by = "comps") %>%
  mutate(sig = ifelse(eigs>ul, "p < .05", "p > .05"))

ggplot(scree_sig, aes(factor(comps), eigs)) +
  geom_path(aes(group = 1), color = rdgy[7], linetype = 2) +
  geom_point(aes(color = sig), size = 2) +
  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, 
                                         name = "Percentage of Explained Variance"
                                         )
                     ) +
  scale_color_manual(values = c(rdgy[3], rdgy[9]), name = NULL) +
  labs(x = "Components", y = "Eigenvalues") +
  theme_minimal() +
  theme(legend.position = "bottom")
```

What we suspected was correct: given a null distribution/hypothesis, only components 1 and 2 were greater than 95% permuted eigenvalues. In other words, there is less than a 5% chance that the pattern of the results seen on components 1 and 2 are this extreme given that the null hypothesis is true (i.e., no relationship). Therefore, we'll pay special attention to components 1 and 2.

Now that we know what components may be more important than others, let's take a look at what we can learn from examining factor scores for the rows (years) and the columns (measures).

### Row-wise Factor Scores

We can explore how the years are seen through the components by first scaling the U matrix by the singular values:

```{r}
years <- hawks_data_svd$u %*% diag(hawks_data_svd$d)
rownames(years) <- rownames(hawks_data_mat)
```

Now let's visualize the factor scores for components 1 and 2:

```{r}
# Builds dataframe of row-wise factor scores with notable events
years_fs <- as_tibble(years) %>%
  mutate(Season = hawks_data_wide$Season) %>%
  left_join(., hawks_events, by = "Season") # imports notable events

ggplot(years_fs, aes(V1, V2, color = SCW)) +
  geom_vline(xintercept = 0, alpha = 1/3) +
  geom_hline(yintercept = 0, alpha = 1/3) +
  geom_point() +
  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +
  scale_color_manual(values = c(rdgy[9], rdgy[2]), name = "Stanley Cup Wins") +
  geom_text_repel(aes(label = Season), segment.alpha = 0) +
  pca_furnish +
  theme(legend.position = "bottom")
```

[blurb here]

```{r}
ggplot(years_fs, aes(V1, V2, color = PF)) +
  geom_vline(xintercept = 0, alpha = 2/3) +
  geom_hline(yintercept = 0, alpha = 2/3) +
  geom_point() +
  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +
  scale_color_brewer(palette = "Dark2", direction = -1) +
  geom_text_repel(aes(label = Season), segment.alpha = 0) +
  pca_furnish
```


### Column-wise Factor Scores

```{r}
svd_v <- hawks_data_svd$v
rownames(svd_v) <- colnames(hawks_data_mat)
round(svd_v, 3) %>% nice_table
```

## Contributions

We can actually take a look at how much each season (row) or variable (column) contributes to the new factor. To calculate the contribution of each factor, we square each factor score and divide it by the sum of squared factor. The sum of the squared factor is also the eigenvalue.

$$contribution = \frac{f^2}{\Sigma f^2} = \frac{f^2}{\lambda}$$

```{r}
contribution <- function(vector, sign = TRUE) {
  vector_sq <- vector^2
  vector_sq_sum <- sum(vector_sq)
  
  if (sign == TRUE) {
    vector <- vector_sq/vector_sq_sum*sign(vector)
  } else {
    vector <- vector_sq/vector_sq_sum
  }
  
}
```

### Row-wise Contributions
```{r}
contributions_years <- apply(years, 2, contribution)
colnames(contributions_years) <- paste0("fi",1:ncol(contributions_years))
contributions_years <- contributions_years %>%
  as.tibble() %>%
  mutate(Season = rownames(years)) %>%
  reshape2::melt(value.name = "Contributions")

contributions_years %>%
  filter(variable %in% c("fi1", "fi2")) %>%
  ggplot(., aes(x = Season, y = Contributions)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ variable) +
    theme_minimal() +
    theme(axis.text.x = element_text(hjust = 1, angle = 45))
```

### Column-wise Contributions
```{r}
contributions_v <- apply(svd_v, 2, contribution)
colnames(contributions_v) <- paste0("fj",1:ncol(contributions_v))

contributions_v <- contributions_v %>%
  as.tibble() %>%
  mutate(Metric = rownames(svd_v)) %>%
  reshape2::melt(value.name = "Contributions")

colnames(contributions_v)[2] <- "factor"

contributions_v %>%
  filter(factor %in% c("fj1", "fj2")) %>%
  ggplot(., aes(x = Metric, y = Contributions)) +
    geom_bar(stat = "identity") +
    facet_wrap(~ factor) +
    theme_minimal() +
    theme(axis.text.x = element_text(hjust = 1, angle = 45))
```


```{r, eval=FALSE}
nhl_data_mat <- as.matrix(nhl_data[,2:ncol(nhl_data)])
rownames(nhl_data_mat) <- nhl_data$Season

nhl_data_mat_scaled <- apply(nhl_data_mat, 2, scale)

nhl_data_svd <- svd(nhl_data_mat_scaled)

rows <- nhl_data_svd$u %*% diag(nhl_data_svd$d)
columns <- nhl_data_svd$v %*% diag(nhl_data_svd$d)


fis <- as.tibble(rows) %>% 
  mutate(Season = nhl_data$Season,
         SCW = factor(ifelse(Season %in% c("2009_2010", "2012_2013", "2014_2015"), "Won", "Lost"))
         )

fjs <- as.tibble(columns) %>% 
  mutate(meas = colnames(nhl_data)[-1])






ggplot(fjs, aes(V2, V3)) +
  geom_vline(xintercept = 0, alpha = 2/3) +
  geom_hline(yintercept = 0, alpha = 2/3) +
  geom_point() +
  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +
  scale_color_brewer(palette = "Dark2") +
  geom_text_repel(aes(label = meas), segment.alpha = 0) +
  theme_classic() +
  theme(axis.title=element_blank(),
        #axis.text=element_blank(),
        axis.ticks=element_blank(),
        axis.line = element_blank()
        )

ggplot(nhl_data, aes(PEND, PENT)) +
  geom_point() +
  geom_smooth(method = "lm")

cor.test(nhl_data$PENT, nhl_data$PEND)
summary(lm(PENT~PEND, data = nhl_data))
```

```{r, eval=FALSE}
gp <- nhl_data$GP

nhl_data_mat_2 <- as.matrix(nhl_data[3:ncol(nhl_data)])

nhl_data_scaled1 <- apply(nhl_data_mat_2[,1:7], 2, function(x){x/gp})

nhl_ready <- cbind(nhl_data_scaled1, nhl_data_mat_2[,8:9])

nhl_scaled <- apply(nhl_ready, 2, scale)

nhl_svd <- svd(nhl_scaled)

rows <- nhl_svd$u %*% diag(nhl_svd$d)
columns <- nhl_svd$v %*% diag(nhl_svd$d)

fis <- as.tibble(rows) %>% 
  mutate(Season = nhl_data$Season,
         SCW = factor(ifelse(Season %in% c("2009_2010", "2012_2013", "2014_2015"), "Won", "Lost"))
         )

fjs <- as.tibble(columns) %>% 
  mutate(meas = colnames(nhl_data)[3:ncol(nhl_data)])

inertia <- sum(nhl_svd$d)

scree <- tibble(eigs = nhl_svd$d, 
                perc_explained = (eigs/inertia)*100,
                comps = 1:length(eigs)
                )

ggplot(scree, aes(factor(comps), eigs)) +
  geom_point() +
  geom_path(aes(group = 1)) +
  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, 
                                         name = "Percentage of Explained Variance"
                                         )
                     ) +
  theme_minimal()

ggplot(fis, aes(V1, V2, color = SCW)) +
  geom_vline(xintercept = 0, alpha = 2/3) +
  geom_hline(yintercept = 0, alpha = 2/3) +
  geom_point() +
  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +
  scale_color_brewer(palette = "Dark2") +
  geom_text_repel(aes(label = Season), segment.alpha = 0) +
  theme_classic() +
  theme(axis.title=element_blank(),
        #axis.text=element_blank(),
        axis.ticks=element_blank(),
        axis.line = element_blank()
        )

ggplot(fjs, aes(V1, V2)) +
  geom_vline(xintercept = 0, alpha = 2/3) +
  geom_hline(yintercept = 0, alpha = 2/3) +
  geom_point() +
  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +
  scale_color_brewer(palette = "Dark2") +
  geom_text_repel(aes(label = meas), segment.alpha = 0) +
  theme_classic() +
  theme(axis.title=element_blank(),
        #axis.text=element_blank(),
        axis.ticks=element_blank(),
        axis.line = element_blank()
        )
```

