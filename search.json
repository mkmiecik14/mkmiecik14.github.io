[
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "I am available to consult on data science and programming projects. Please email me mkmiecik14 at gmail dot com to schedule a consultation.\nConsulting areas:\n\nR/RStudio coach\nStatistics\nData visualization\nData wrangling and cleaning\nData analysis pipelines\nGit/GitHub\nWebsite development\nAPIs\nShiny Apps\nSports analytics\nMATLAB\nEEG/fMRI processing\nE-Prime\nAcademic writing\nBlogging"
  },
  {
    "objectID": "cv.html#papers",
    "href": "cv.html#papers",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Papers",
    "text": "Papers\n\nKmiecik, M. J., Tu, F. F., Clauw, D. J., & Hellman, K. M. (2023). Multimodal Hypersensitivity Derived from Quantitative Sensory Testing Predicts Pelvic Pain Outcome: an Observational Cohort Study. PAIN, 164(9), 270-283. http://dx.doi.org/10.1097/j.pain.0000000000002909\nShlobin, A. E., Tu, F. F., Sain, C. R., Kmiecik, M. J., Kantarovich, D., Singh, L., Wang, C. E., & Hellman, K. M. (2023). Bladder Pain Sensitivity is a Potential Risk Factor for Irritable Bowel Syndrome. Digestive Diseases and Sciences, 68, 3092–3102. https://doi.org/10.1007/s10620-023-07868-7\nJones, L. L., Kmiecik, M. J., Irwin, J. L., Morrison, R. G. (2022). Differential effects of semantic distance, distractor salience, and relations in verbal analogy. Psychonomic Bulletin & Review, 29, 1480–1491. https://doi.org/10.3758/s13423-022-02062-8\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., Hellman, K. M. (2022). Cortical Mechanisms of Visual Hypersensitivity in Women at Risk for Chronic Pelvic Pain. PAIN, 163(6), 1035-1048. doi: 10.1097/j.pain.0000000000002469\nKmiecik, M. J., Perez, R., Krawczyk, D.C. (2021). Navigating Increasing Levels of Relational Complexity: Perceptual, Analogical, and System Mappings. Journal of Cognitive Neuroscience, 33(3), 357-376. https://doi.org/10.1162/jocn_a_01618\nKrawczyk, D. C., Han, K., Martinez, D., Rakic, J., Kmiecik, M. J., Chang, Z., Nguyen, L., Lundie, M., Cole, R., Nagele, M., Didehbani, N. (2019). Executive Function Training in Chronic Traumatic Brain Injury Patients: Study Protocol. TRIALS. 20:435 https://doi.org/10.21203/rs.2.273/v1\nKmiecik, M. J., Brisson, R. J., & Morrison, R. G. (2019). The time course of semantic and relational processing during verbal analogical reasoning. Brain and Cognition, 129, 25-34. https://doi.org/10.1016/j.bandc.2018.11.012\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nStockdale, L. A., Morrison, R. G., Kmiecik, M. J., Garbarino, J., & Silton, R. L. (2015). Emotionally anesthetized: media violence induces neural changes during emotional face processing. Social Cognitive and Affective Neuroscience, 10(10), 1373-1382. doi: 10.1093/scan/nsv025"
  },
  {
    "objectID": "cv.html#books",
    "href": "cv.html#books",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Books",
    "text": "Books\n\nPongpipat, E. E., Miranda, G. G., Kmiecik, M. J. (2019). A Practical Extension of Introductory Statistics in Psychology using R. Free online textbook written in bookdown and R. Read it here: https://rpsystats.com/"
  },
  {
    "objectID": "cv.html#preprints",
    "href": "cv.html#preprints",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Preprints",
    "text": "Preprints\n\nKmiecik, M. J., Tu, F. F., Clauw, D. J., Hellman, K. M. (2022). Multimodal Hypersensitivity Derived from Quantitative Sensory Testing Predicts Long-Term Pelvic Pain Outcome. MedRxiv, 2022.04.01.22272964. doi: https://doi.org/10.1101/2022.04.01.22272964 GitHub Repo OSF Project\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2020). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. MedRxiv, 2020.12.03.20242032. doi: https://doi.org/10.1101/2020.12.03.20242032"
  },
  {
    "objectID": "cv.html#dissertation",
    "href": "cv.html#dissertation",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Dissertation",
    "text": "Dissertation\n\nKmiecik, M. J. (2019). The Time Course of Meaning Construction with Varying Expectations."
  },
  {
    "objectID": "cv.html#papers-1",
    "href": "cv.html#papers-1",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Papers",
    "text": "Papers\n\nKmiecik, M. & Morrison, R.G. (2013). Semantic Distance Modulates the N400 Event-Related Potential in Verbal Analogical Reasoning. In M. Knauff, M. Pauen, & N. Sebanz (Eds.), Proceedings of the 35th Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society."
  },
  {
    "objectID": "cv.html#talks",
    "href": "cv.html#talks",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Talks",
    "text": "Talks\n\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (Sept. 2021). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Flash talk presented at The University of Chicago Postdoctoral Symposium.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (Dec. 2021). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Data blitz talk presented at the International Association for the Study of Pain (IASP) Neuropathic Pain SIG (NeuPSIG) webinar series. Link to talk here.\nKmiecik, M. J. (November, 2019). Effective Presentations of Quantitative Information: A Study in Edward Tufte. Talk presented at the inaugural UT Dallas BrainHack, Dallas, TX.\nKmiecik, M. J., Nagele, M., Sharma, R., Vaughn, A. Thakur, P., & Krawczyk, D. C. (December, 2018). A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury. Talk presented at the Brain Performance Institute’s Inaugural TBI Research Showcase. Dallas, TX.\nKmiecik, M. J. & Krawczyk, D. C. (2017, October). Reasoning With Complex Relational Structures. Talk given at the UT Dallas School of Behavioral and Brain Sciences Cognition and Neuroscience program annual Fall retreat. Dallas, TX.\nKmiecik, M. & Morrison, R.G. (2013, August). Semantic Distance Modulates the N400 Event-Related Potential in Verbal Analogical Reasoning. Paper presented at the 35th Annual Conference of the Cognitive Science Society, Berlin, Germany."
  },
  {
    "objectID": "cv.html#workshops",
    "href": "cv.html#workshops",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Workshops",
    "text": "Workshops\n\nDutcher, A., Kmiecik, M.J., & Beaton, D. (2016, April). Analyzing multi-block data: A tutorial of Multiple Factor Analysis in R. Workshop presented at the Society for Applied Multivariate Research, Dallas, Texas. (MFA Workshop Slides / MFA Workshop R Code )"
  },
  {
    "objectID": "cv.html#poster-presentations",
    "href": "cv.html#poster-presentations",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nRoth, G. E., Silton, R. L., Tu, F. F., Bohnert, A. M., Walker, L. S., Kmiecik, M. J., & Hellman, K. M. (September 2023). Somatic symptoms are associated with psychological symptoms, visceral pain sensitivity, and observations of parental pain behavior in premenarchal adolescents. Poster to be presented at the Society for Research in Psychopathology, St. Louis, MO, USA.\nKmiecik, M. J., Coker, D., Heilbron, K., Aslibekyan, S., Shelton, J. F., Cannon, P., Norcliffe-Kaufmann, L. (August 2023). The natural history of Parkinson’s disease in LRRK2 G2019S carriers. Poster presented at the International Congress of Parkinson’s Disease and Movement Disorders, Copenhagen, Denmark. Movement Disorders, 38(S1), S455–521. https://doi.org/10.1002/mds.29543\nKmiecik, M. J., Tu, F. F., Darnell, S., Harber, K., Hellman, K. M. (2022, Nov.). Early cortical mechanisms of visual discomfort in premenarchal adolescents. Poster presented at the Society for Neuroscience Annual Meeting, San Diego, California.\nKmiecik, M. J., Tu, F. F., Clauw, D. J., Hellman, K. M. (2022, Sept.). Multimodal Hypersensitivity Predicts Pelvic Pain Outcome 4 Years Later. Poster presented at the International Association for the Study of Pain (IASP) World Congress on Pain, Toronto, Canada.\nDarnell, S., Tu, F. F., Harber, K., Sain, C., Kmiecik, M. J., Hellman, K. M. (2022, Sept.). Menstrual, Sensory, and Psychological Factors Predict New Onset Chronic Pelvic Pain. Poster to be presented at the International Association for the Study of Pain (IASP) World Congress on Pain, Toronto, Canada.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2022, March). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Poster presented at the Chicago Chapter of the Society for Neuroscience annual meeting, Chicago, IL.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2021, Oct.). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Poster presented at the 24th annual scientific meeting of the International Pelvic Pain Society (IPPS), Baltimore, MD.\nKmiecik, M. J., Kim, L. M., Maguire, M. J., Hart, J., Krawczyk, D. C. (2020, May). The Time Course of Meaning Construction with Varying Expectations. Poster presented virtually at the Cognitive Neuroscience Society’s annual conference, Boston, MA.\nKim, L. M., Lundie, M. J., Kmiecik, M. J., Dasara, H., & Krawczyk, D. C. (2020, May). The Nuances of Norepinephrine: Salivary Alpha Amylase’s Role as a Biomarker in tDCS - Directed Judgment & Decision Making. Poster presented virtually at the Cognitive Neuroscience Society’s annual conference, Boston, MA.\nKmiecik, M. J., Kim, L. M., Maguire, M. J., Hart, J., Krawczyk, D. C. (2020, March). The Time Course of Meaning Construction with Varying Expectations. Poster was to be presented at the Society for Neuroscience Chicago Chapter annual meeting, Chicago, IL. * Cancelled due to COVID-19\nKim, L. M., Lundie, M. J., Kmiecik, M. J., Dasara, H., & Krawczyk, D. C. (2020, February). A Nudge of Norepinephrine: Investigating Salivary Alpha Amylase as a Biomarker of tDCS - Augmented Decision Making. Poster to be presented at the Society for Personality and Social Psychology’s annual conference, New Orleans, LA.\nNagele, M., Kmiecik, M. J., Chang, Z., Martinez, D., Juarez, M., Khachaturyan, M., Lundie, M., Kim, L., Didehbani, N., & Krawczyk, D. C. (2019, August). Using the Virtual Multiple Errands Task (VMET) to Assess Executive Functioning in Traumatic Brain Injury. Poster presented at the Military Health System Research Symposium annual meeting, Kissimmee, FL.\nChang, Z., Kmiecik, M. J., Martinez, D., Nagele, M., Lundie, M., Cole, R., Kim, L., Khachaturyan, M., Balloun, B., Juarez, M., Didehbani, N., Clover, M., Rule, G., Scott, G., & Krawczyk, D. C. (2019, August). Using a Virtual-Reality Based Rehabilitation Program to Enhance Daily Functioning in Veterans Surviving Traumatic Brain Injury. Poster presented at the Military Health System Research Symposium annual meeting, Kissimmee, FL.\nNagele, M. M., Kmiecik, M. J., & Krawczyk, D. C. (2019, April). Self-Awareness of Executive Dysfunction in Traumatic Brain Injury. Poster presented at UT Dallas School of Behavioral and Brain Sciences Psychological Sciences annual meeting, Dallas, TX.\nKmiecik, M. J., Martin A. D., Kim, L. M., Perez, R., Martinez, D. M., Pongpipat, E. E., & Krawczyk, D. C. (2019, March). The Influence of Reasoning Ability and Relational Cueing in Solving Relational Match-to-Sample Problems. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nKim, L. M., Kmiecik, M. J., Martinez, D. M., Martin A. D., & Krawczyk, D. C. (2019, March). The Similar Situations Task: Measuring Differing Levels of Reasoning Using Scene Analogies. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nKais, L.A., Lee, C., Kmiecik, M.J., Silton, R.L. (2018, February). The Influence of Affect on Interference Processing in Blocked and Mixed Presentations of a Stroop Color-Word Task. Poster presented at the annual meeting of the International Neuropsychological Society: Washington, D.C.\nKmiecik, M. J., Perez, R., Dandu, H., Krawczyk, D. C. (2017, August). Reasoning with Complex Relational Structures. Poster presented at the Fourth International Conference on Analogical Reasoning, Paris, France.\nMartinez, D. M., Kmiecik, M. J., Kamat, P. S., Schauer, G. F., Krawczyk, D. C. (2017, August). Aspects of Cognition and Clinical Symptomology in Analogical Reasoning. Poster presented at the Fourth International Conference on Analogical Reasoning, Paris, France.\nKmiecik, M.J., Martinez, D., Young, L.R., Krawczyk, D.C. (2016, November). Functional and Structural Neural Patterns in Mild-Moderate Chronic-Phase Traumatic Brain Injury. Archives of Physical Medicine and Rehabilitation, 97(10), e55. doi: 10.1016/j.apmr.2016.08.167\nMartinez, D., Kmiecik, M.J., Chapman, S., Krawczyk, D.C. (2016, November). Observing Changes in Cognition, Mood, and White Matter in Chronic TBI Using Multiple Factor Analysis After Cognitive Intervention. Archives of Physical Medicine and Rehabilitation, 97(10), e75. doi: 10.1016/j.apmr.2016.08.229\nKmiecik, M.J., Schauer, G.F., Martinez, D., & Krawczyk, D.C. (2016, April). The Similar Situations Task: An Assessment of Analogical Reasoning in Healthy and Clinical Populations. Poster presented at the Cognitive Neuroscience Society annual meeting, New York, NY.\nKmiecik, M. J., Chapman, S., & Krawczyk, D., (2015). Executive Functioning in Traumatic Brain Injury: A Detailed Investigation of the Hayling Test. Archives of Physical Medicine and Rehabilitation, 96(10), e97-e98. doi: 10.1016/j.apmr.2015.08.326\nStockdale, L., Morrison, R. G., Kmiecik, M., Palumbo, R., Garbarino, J., & Silton, R. L. (September, 2014). Negative valence systems distinctly influence bottom-up and top-down attentional processes. Paper presented at the Society for Psychophysiological Research, Atlanta, GA.\nPalumbo, R. T., Stockdale, L., Kmiecik, M. J., Silton, R. L., Morrison, R. G. (2014, July). The effect of short-term exposure to film violence on emotional facial processing. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nBrisson, R., Kmiecik, M.J., & Morrison, R. G. (2014, July). The effect of semantic and relational similarity on the N400 event-related potential in verbal analogical reasoning. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nIrwin, J.L., Jones, L.L., Kmiecik, M.J., Unsworth, N., & Morrison, R. G. (2014, July). Attention to detail predicts better verbal analogy performance. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nJones, L.L., Irwin, J.L., Kmiecik, M.J., Unsworth, N., & Morrison, R. G. (2014, July). Working memory and interference control in verbal analogy. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nMorrison, R. G., Kmiecik, M.J., Irwin, J.L., Unsworth, N., Jones, L.L. (2014, July). Working memory and crystallized knowledge in visual analogy. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nBrisson, R.J., Kmiecik, M.J., Sweis, A.S. & Morrison, R.G. (2014, April). The Effect of Semantic and Relational Similarity on the N400 in Verbal Analogical Reasoning. Poster presented at the Cognitive Neuroscience Society annual meeting, Boston, MA.\nStockdale, L., Palumbo, R., Kmiecik, M., Silton, R.L. & Morrison, R.G. (2014, April). The Effects of Media Violence on the Neural Correlates of Emotional Facial Processing: An ERP Investigation. Poster presented at the Cognitive Neuroscience Society annual meeting, Boston, MA.\nKmiecik, M.J., Brisson, R.J., & Morrison, R.G. (2013, April). Semantic distance in verbal analogical reasoning modulates the N400 event-related potential. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nMorrison, R.G., Kmiecik, M. & Bharani, K.L. (2012, April). When analogy is like priming: The N400 in verbal analogical reasoning. Poster presented at the Cognitive Neuroscience Society annual meeting, Chicago, IL.\nMorrison, R.G., Kmiecik, M. & Bharani, K.L. (2012, March). When analogy is like priming: The N400 in verbal analogical reasoning. Poster presented at the Society for Neuroscience Chicago Chapter 2012 annual meeting, Chicago, IL."
  },
  {
    "objectID": "cv.html#conference-organization",
    "href": "cv.html#conference-organization",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Conference Organization",
    "text": "Conference Organization"
  },
  {
    "objectID": "cv.html#paper-reviews",
    "href": "cv.html#paper-reviews",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Paper Reviews",
    "text": "Paper Reviews"
  },
  {
    "objectID": "cv.html#committee-involvement",
    "href": "cv.html#committee-involvement",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Committee Involvement",
    "text": "Committee Involvement"
  },
  {
    "objectID": "cv.html#professor-of-record",
    "href": "cv.html#professor-of-record",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Professor of Record",
    "text": "Professor of Record"
  },
  {
    "objectID": "cv.html#teaching-assistant",
    "href": "cv.html#teaching-assistant",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant"
  },
  {
    "objectID": "cv.html#guest-lectures",
    "href": "cv.html#guest-lectures",
    "title": "\nMatthew J. Kmiecik, PhD\n",
    "section": "Guest Lectures",
    "text": "Guest Lectures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "",
    "text": "About Me\n\nI’m a cognitive neuroscientist doing a (second) postdoc at 23andMe researching genotypic and phenotypic markers of Parkinson’s Disease. In my first postdoc, I studied how individuals’ sensitivity across multiple sensory modalities predicted their long-term pelvic pain. In graduate school, I studied volitional control of semantic processing, executive dysfunction in chronic-phase traumatic brain injury, and human reasoning. My undergraduate work utilized EEG to understand the neural correlates and timecourse of analogical reasoning. In my spare time I enjoy reading, exercising, crossword puzzles, and the endless pursuit of new hobbies.\n\n\n Blog Posts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nStop Using Excel to Preprocess E-Prime Data\n\n\n\n\n\n\n\nAcademic\n\n\nR\n\n\nE-Prime\n\n\n\n\nA walk-through of how to process E-Prime output with R.\n\n\n\n\n\n\nMar 18, 2020\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nMy Reads\n\n\n\n\n\n\n\nAcademic\n\n\nR\n\n\nReading\n\n\nPlotly\n\n\n\n\nThe results of a 1-year reading experiment.\n\n\n\n\n\n\nDec 26, 2019\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nExploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis\n\n\n\n\n\n\n\nAcademic\n\n\nHockey Analytics\n\n\nR\n\n\n\n\nTwo grad students get over zealous with PCA and NHL data.\n\n\n\n\n\n\nMar 7, 2019\n\n\nMatthew J. Kmiecik & Ekarin E. Pongpipat\n\n\n\n\n\n\n  \n\n\n\n\nA Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury\n\n\n\n\n\n\n\nAcademic\n\n\nPaper Review\n\n\n\n\nA (hopefully) accessible summary of my first first-authored paper.\n\n\n\n\n\n\nJul 22, 2018\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nBootstrapping and Permutation Testing: A Shiny App\n\n\n\n\n\n\n\nAcademic\n\n\nShiny\n\n\nR\n\n\n\n\nUsing Shiny to help understand the difference between bootstrapping and permutation testing.\n\n\n\n\n\n\nJun 7, 2018\n\n\nMatthew J. Kmiecik & Ekarin E. Pongpipat\n\n\n\n\n\n\n  \n\n\n\n\nSimulating 2018 NCAA March Madness Tournament Performance in R\n\n\n\n\n\n\n\nAcademic\n\n\nR\n\n\nMarch Madness\n\n\n\n\nA bunch of grad students compete to design the best March Madness algorithm.\n\n\n\n\n\n\nMar 20, 2018\n\n\nNick Ray, David Hoagey, Linh Lazarus, Ekarin Pongpipat, & Matthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nMultilevel Modeling in R with NHL Power Play Data\n\n\n\n\n\n\n\nAcademic\n\n\nHockey Analytics\n\n\nR\n\n\n\n\nA walkthrough of sequential multilevel modeling in R using NHL data.\n\n\n\n\n\n\nFeb 11, 2018\n\n\nEkarin Pongpipat & Matthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nWriting a Literature Review\n\n\n\n\n\n\n\nAcademic\n\n\nWriting\n\n\n\n\nTips on writing a literature review.\n\n\n\n\n\n\nSep 25, 2017\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nUsing Xbox Controllers and Sending EEG Triggers with E-Prime\n\n\n\n\n\n\n\nAcademic\n\n\nE-Prime\n\n\nEEG\n\n\n\n\nA how-to article for EEG enthusiasts.\n\n\n\n\n\n\nJun 12, 2017\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\n  \n\n\n\n\nThe Last Decade: NHL’s Best and Worst Power Play Teams\n\n\n\n\n\n\n\nAcademic\n\n\nHockey Analytics\n\n\nR\n\n\n\n\nMy initial (although incorrect) attempt at modeling NHL power play data.\n\n\n\n\n\n\nMay 15, 2017\n\n\nMatthew J. Kmiecik\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury/post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury.html",
    "href": "posts/post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury/post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury.html",
    "title": "A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury",
    "section": "",
    "text": "Short and Sweet\nLast summer we published a paper in Psychological Assessment that presented methods to better characterize verbal responses on neuropsychological assessments. These methods were applied to a specific cognitive test called the Hayling Sentence Completion Test. We demonstrated that these new methods provide additional insights into the cognitive performance of individuals with mild to moderate traumatic brain injury (TBI). Furthermore, these new measures were related to other cognitive abilities, such as verbal knowledge, processing speed, inhibitory control, working memory, and task switching. These methods can be extended to other assessments that involve word generations and clinical populations that exhibit cognitive impariments.\nTo read our paper, see:\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nYou can also read it on ResearchGate.\n\nIf you are interested in learning more…\n\n\nLong and Drawn (Out)\nTraumatic brain injuries (TBIs) occur when the head receives a harmful blow that results in disruptions to normal brain functioning. Individuals can recover following these injuries; however, some individuals’ symptoms persist long after the injury (&gt; 3 months post-injury) and become chronic TBI.\nIndividuals with chronic TBI often report difficulties in completing everyday activities, such as going to the grocery store or holding a job. In terms of what is required cognitively, these tasks are quite complex and require planning, blocking distractions, knowledge, and sustained attention. Neuropscyhologists call these complex cognitive abilities executive functions and a large focus of TBI research is understanding how these cognitive functions are impacted and how these deficits affect behavior, such as difficulties in holding a job.\nDespite these self-reported impairments, it is difficult to capture these symptoms in a lab setting. In other words, an individual with TBI may report having difficulties in everyday life, but he or she may perform well on standardized neuropsychological assessments. Are the tests not sensitive enough? Are we measuring the right thing? Is TBI more difficult to assess?\nThe difficulties in understanding the effects of TBI is likely a mixture of the above concerns. The paper that this blog post covers addresses a specific concern: the sensitivity of existing tests. We took an existing neuropsychological assessment of executive functioning, called the Hayling Sentence Completion Test (Burgess & Shallice, 1996, 1997), and applied methods from the fields of cognitive science and linguistics to gain further insights about how TBI impacts performance.\nWhen taking the Hayling Test, individuals are read sentences with the last word missing. In the first section, participants are instructed to complete the sentence with a logical word. In the second section, participants are instructed to complete the sentence with an illogical word. For example:\n\n\n\n\n\n\nSection\nCompletion\nSentence Stem\nCorrect Completion\n\n\n\n\n1\nSensible\nHe mailed a letter without a\nstamp.\n\n\n2\nUnconnected\nThe captain wanted to stay with the sinking\nbanana.\n\n\n\n\n\n\n\n\nPerformance on this test is guaged by the time to complete each section (faster is better) and the number of errors committed on section two (fewer errors is better). The frontal lobe is responsible for inhibiting distractions and following task instructions. The thinking behind this test is that those with damage to the frontal lobe will commit more errors on section two.\nOne potential source of improvement we saw was to introduce more objective methods in scoring errors, as unclear responses are often given.\nWe measured 2 different properties of the words:\n\nHow similar the sentence stem (He mailed a letter without a) and the generated word (stamp) were to each other using a technique called Latent Semantic Analysis (LSA; Landauer, Foltz, & Laham, 1998)\nHow frequently the generated words are used in everyday speech (word frequency) using the SUBTLEXus database (Brysbaert & New, 2009)\n\nWhat we discovered was that these measurements provide additional metrics that can be used to guage performance on the Hayling test. More specifically, other measures of cognitive performance were related to performance on these additional measurements. These additional measures were verbal ability, processing speed, inhibitory control, working memory, and task switching. Taken together, these results suggest that these additional measurements are closely related to high-level cognitive functions and can be used to better understand solving strategies and response patterns in clinical populations.\nNext steps in this line of research include further validating these measurements. Using our new measurements, we are currently conducting a study to compare those with chronic mild/moderate TBI to those without a history of TBI to further understand the effects of TBI on cognitive functioning. Our hope is that these methods can be extended to other linguistically based neuropsychological assessments and other clinical populations that experience cognitive impairments to better understand, diagnose, and formulate treatment plans.\n\n\nReferences\n\nBrysbaert, M., & New, B. (2009). Moving beyond Kucˇera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods, Instruments and Computer, 41, 977–990. http://dx.doi.org/10.3758/BRM.41.4.977\nBurgess, P. W., & Shallice, T. (1996). Response suppression, initiation and strategy use following frontal lobe lesions. Neuropsychologia, 34(4), 263-272. 10.1016/0028-3932(95)00104-2\nBurgess, P. W., & Shallice, T. (1997). The Hayling and Brixton tests. London, England: Pearson.\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nLandauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 25, 259–284. http://dx.doi.org/10.1080/01638539809545028"
  },
  {
    "objectID": "posts/post-Bootstrapping-and-Permutation-Testing-Shiny-App/post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#bootstrapping",
    "href": "posts/post-Bootstrapping-and-Permutation-Testing-Shiny-App/post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#bootstrapping",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nWhen you think of boostrapping, think confidence intervals. Bootstrapping samples observations with replacement without breaking the relationship between measures (e.g., X and Y). The number of samples is equal to the number of observations (i.e., sample size). After sampling with replacement is finished, the statistic of interest, such as a correlation, is computed and stored.\n\n\n\n\n\n\nTip\n\n\n\nWhen you think of boostrapping, think confidence intervals.\n\n\nThis process explained above is then repeated hundreds or thousands of iterations resulting in a distribution of values for your statistic of interest. This distibution will be centered about the original statistical value that you computed before any resampling occured. In other words, the mean of these stored values will equal your observed statistic.\nAs with any distribution, you can calculate what are the lower bounds and upper bounds of values for a given percentage. This percentage is determined by the researcher/statistician/hockey analyst enthusiast and is called a confidence interval (95% is a common confidence interval). These bounds are usually reported in square brackets in the format: confidence interval % [lowerbound, upper bound]. For example, “There was a positive correlation observed between X and Y, r = .31, 95% CI [.21, .41].”"
  },
  {
    "objectID": "posts/post-Bootstrapping-and-Permutation-Testing-Shiny-App/post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#permutation-testing",
    "href": "posts/post-Bootstrapping-and-Permutation-Testing-Shiny-App/post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#permutation-testing",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "Permutation Testing",
    "text": "Permutation Testing\nWhen you think of permutation testing, think of p-values. Permutation testing does not sample observations with replacement, but instead breaks the relationship between measures (e.g., X and Y). This is done by shuffling/randomizing/sampling the observed data points for one variable, while keeping the other (or others) intact. In terms of correlation, this would mean that X would be shuffled within the sample, while Y remained the original values. After the responses for one variable are randomized the statistic of interest, such as a correlation, is computed and stored.\n\n\n\n\n\n\nTip\n\n\n\nWhen you think of boostrapping, think confidence intervals.\n\n\nThis process explained above is then repeated hundreds or thousands of iterations resulting in a distribution of values for your statistic of interest. This distibution will not be centered about the original statistic value that you computed before any shuffling occured, but rather will be centered around the null. In terms of correlation, a null distribution would center about r = 0; meaning no linear relationship between variables.\nIn other words, a null distribution is created by shuffling the values in X but not Y. This is because the relationship has been broken between X and Y.\nA p-value is calculated by first counting the number of statistical values that are more extreme than your observed statistic. Put another way, how many times did the statistical value that emerged from a “null distribution” surpass your original computed statistic (before any shuffling). Then, you take the number of times that the null distribution is more extreme than your original value and divide it by the number of permutation iterations (number of observations in your null distribution).\nFor example, let’s say I ran a permutation test on a correlation of r = .5 and shuffled X, kept Y, computed their correlation, stored this value, and repeated this 100 times. Out of 100 times, there were 4 correlations that emerged that were greater than .5. Therefore, my p-value for this correlation would be 4/100 = 0.04."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "",
    "text": "In this post we explore 11 seasons (2007 - 2018) of team summary data from the Chicago Blackhawks of the National Hockey League (NHL). Our question was, “Are there any summary measures, such as goals scored or save percentage, that predict playoff performance or championship wins?”\nWe explore these data using a variety of techniques, such as:\nZ-scores across time\nCorrelations\nPrincipal Components Analysis (PCA)"
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-import",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-import",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Data Import",
    "text": "Data Import\nLet’s first prepare the data for analysis. These data were downloaded from Corsica’s team stats tool. We’ve prepared these data for you and are available for import into R like this:\n\n# Link to raw data on Github\nlink &lt;- \"https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/nhl-team-data-corsica.csv\"\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata &lt;- GET(link)\nnhl_data &lt;- read_csv(content(data, \"raw\"))\n\nRows: 331 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Team, Season\ndbl (26): GP, TOI, CF, CA, C+/-, CF%, CF/60, CA/60, GF, GA, G+/-, GF%, GF/60...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-preparation",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-preparation",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo make things simple, without losing information, we’ll use Chicago Blackhawk’s data from the 2007-2008 season up through the 2017-2018 season (11 years of data) and only a subset of the available metrics. These metrics include:\n\nGames Played (GP)\nTime on Ice (TOI)\nCorsi For (CF) = Shot attempts for at even strength: Shots + Blocks + Misses\nCorsi Against (CA) = Shot attempts against at even strength: Shots + Blocks + Misses\nGoals For (GF)\nGoals Against (GA)\nPenalty minutes served (PENT)\nPenalty minutes drawn (PEND)\nShooting Percentage (ShootPerc)\nSave Percentage (SavePerc)\n\n\n# Preparing Hawks data ----\nhawks_data &lt;-  nhl_data %&gt;%\n  select(Team:CA, GF, GA, PENT, PEND, ShootPerc = `Sh%`, SavePerc = `Sv%`) %&gt;%\n  filter(Team == \"CHI\") %&gt;%\n  separate(Season, into = c(\"Start_Year\", \"Season\")) %&gt;%\n  mutate(Team = NULL, \n         Start_Year = NULL,\n         Season = as.numeric(Season)\n         )\n\n# Prints data\nnice_table(hawks_data)\n\n\n\n\n\nSeason\nGP\nTOI\nCF\nCA\nGF\nGA\nPENT\nPEND\nShootPerc\nSavePerc\n\n\n\n\n2008\n80\n3423.00\n2600\n2629\n143\n130\n368\n365\n9.04\n91.92\n\n\n2009\n82\n3646.73\n3286\n2655\n146\n124\n335\n368\n7.45\n92.42\n\n\n2010\n82\n3881.33\n3784\n2907\n178\n147\n276\n299\n8.30\n90.53\n\n\n2011\n82\n3955.85\n3704\n3314\n167\n144\n239\n277\n8.06\n92.03\n\n\n2012\n82\n3944.77\n3698\n3290\n166\n164\n270\n287\n8.15\n90.99\n\n\n2013\n48\n2300.80\n2103\n1783\n105\n68\n151\n159\n8.97\n92.94\n\n\n2014\n82\n3979.35\n3890\n3129\n182\n149\n244\n231\n8.44\n91.39\n\n\n2015\n82\n3980.10\n4003\n3462\n150\n128\n225\n248\n6.87\n93.54\n\n\n2016\n82\n3985.37\n3680\n3585\n135\n141\n216\n234\n6.81\n92.81\n\n\n2017\n82\n4053.47\n3759\n3692\n164\n136\n206\n222\n8.17\n93.31\n\n\n2018\n82\n3940.41\n4199\n3821\n156\n173\n214\n261\n7.08\n91.82\n\n\n\n\n\n\n\n\nLet’s also prepare a table of notable events of every year in this data set for the Chicago Blackhawks, including their Stanley Cup wins (3) and their playoff success:\n\n# Initializing Chicago Blackhawks notable events table ----\n# SCW = Stanley Cup Wins\n# PF = Playoff Finish:\n#   0 = Did not make playoffs\n#   1 = Lost in first round\n#   2 = Lost in second round\n#   3 = Lost in conference finals\n#   4 = Lost in Stanley Cup final\n#   5 = Won Stanley Cup\nhawks_events &lt;- tibble(Season = hawks_data$Season,\n                       SCW = factor(c(0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0)),\n                       PF  = factor(c(0, 3, 5, 1, 1, 5, 3, 5, 1, 1, 0))\n                       )"
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#z-scores",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#z-scores",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Z-Scores",
    "text": "Z-Scores\nPrior to exploring these data and how they’ve changed over time, we have to:\n\nAdjust all scores by the number of games played due to a shorted 2012-2013 season. We do this by dividing each metric by the number of games played\nCompute z-scores of all measures to facilitate comparisons\n\n\n# Preprocesses, adjusts, and z-scores hawks data ----\nhawks_data_long &lt;- hawks_data %&gt;% \n  gather(Meas, Val, -Season, -GP) %&gt;%\n  group_by(Meas) %&gt;%\n  mutate(Val_Adj = Val/GP,               # adjusts based on games played\n         Val_Zscore = scale(Val_Adj)     # computes z-scores\n         ) %&gt;%\n  ungroup() %&gt;%\n  mutate(sig = factor(ifelse(abs(Val_Zscore) &gt; 1.96, \"p &lt; .05\", \"p &gt; .05\"))) %&gt;% # z score &gt; 1.96\n  inner_join(., hawks_events, by = \"Season\") # adds notable hawks events\n\nPlotting the z-scores of each measure across time allows us to compare across measures using a standardized unit:\n\n# Plots all measures together ----\nggplot(hawks_data_long, aes(factor(Season), Val_Zscore)) +\n  geom_path(aes(group = 1), color = rdgy[8]) +\n  geom_point(aes(color = sig, shape = SCW), size = 1.75) +\n  scale_color_manual(values = c(rdgy[3], rdgy[10]), name = \"Z-Score\") +\n  scale_shape_discrete(name = \"Stanley Cup Wins\") +\n  scale_y_continuous(breaks = c(-2, 0, 2), minor_breaks = NULL) +\n  coord_cartesian(ylim = c(-3,3)) +\n  theme_minimal() + \n  labs(x = \"\\n Season\", \n       y = \"\\n Measurement (Z-Score)\",\n       title = \"Chicago Blackhawk's Performance 2007-2018\"\n       ) +\n  facet_wrap(~Meas, nrow = 3) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\"\n        )\n\n\n\n\n\n\n\n\nWhen inspecting these plots in relation to the Chicago Blackhawk’s 3 Stanley Cup wins (triangles), we see very little that stands out. It seems like the 2012-2013 season was a unique one, such that it was a Stanley Cup winning team and their regular season Save Percentage and Shooting Percentage were much greater than their other years. Perhaps these two metrics are important for winning the Presidents’ Trophy (awarded to the NHL team finishing with the highest total points), which the Blackhawk’s received at the end of their 2012-2013 season.\nAdditionally, the Blackhawk’s 2007-2008 season seemed their worst season with the team recording statistically the least amount of time on ice (TOI), the most penalty minutes served (PENT), and the lowest Corsi For (CF) compared to the following decade of play. The 2007-2008 season was a tumultuous time for the Hawks, including a change in ownership and the later hire of Coach Joel Quennevill (see NYT article). It is likely that the change in ownership, hiring of Coach Q, and the acquisition of Jonathon Towes and Patrick Kane that led to the drastic improvement of team stats in the years following."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#correlations",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#correlations",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Correlations",
    "text": "Correlations\nNow let’s examine how the various team stats/measurements relate to each other by computing their correlations and visualizing them with a heatmap:\n\n# Converts back to wide format\nhawks_data_wide &lt;- hawks_data_long %&gt;% \n  select(Season, Meas, Val_Zscore) %&gt;% \n  spread(Meas, Val_Zscore)\n\n# Computes correlations\nhawks_cors &lt;- cor(hawks_data_wide)\n\n# Correlations to long format for plotting\nhawks_cors_long &lt;- hawks_cors %&gt;%\n  reshape2::melt() %&gt;%\n  arrange(Var1, Var2)\n\n# Correlation heatmap\nggplot(hawks_cors_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_raster() + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_blank()\n        ) +\n  scale_fill_distiller(palette = \"BrBG\", \n                       limits = c(-1, 1), \n                       name = \"Correlation\"\n                       )\n\n\n\n\n\n\n\n\nAn interesting section that sticks out in the heatmap is the negative relationships between penalties, both served and drawn, and corsi, both for and against. This suggests that as the Hawks improved at drawing penalties, as well worsened at taking penalties, the amount of shots taken at the net decreased. This also came with a decrease in shots let up by the Hawks. My only explanation for this could be that as teams are on the powerplay, they become more selective with their shots, thus decreasing Corsi For and Corsi Against. This sort of aligns with our previous post that determined a decrease in NHL powerplay goals in the last decade. How is decline relates to Corsi is yet to be determined."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#eigen-values",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#eigen-values",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Eigen Values",
    "text": "Eigen Values\nOur original data matrix was 11 rows (years) x 9 columns (measures); therefore, the SVD of that matrix will produce 9 singular values/components – with the smaller side dictating the number of components produced.\nWhich components are important? Which ones explain the most variance? Are some of the components just noise?\nA good first pass at answering these questions is to examine the scree plot – plotting the components as a function of variability explained.\nTo do this, let’s first square the singular values (\\(\\Delta\\)) and sum them together. This is called inertia.\n \\[\nI = \\Sigma\\lambda = \\Sigma\\Delta^2\n\\] \n\ninertia &lt;- sum(hawks_data_svd$d^2) # Calculates inertia\n\nNext, we’ll use the inertia to calculate the percentage of variability explained for each component and plot the scree:\n\n# Calculates values for the scree plot\nscree &lt;- tibble(eigs = hawks_data_svd$d^2, \n                perc_explained = (eigs/inertia)*100,\n                comps = 1:length(eigs)\n                )\n\n# Scree plot\nggplot(scree, aes(factor(comps), eigs)) +\n  geom_point() +\n  geom_path(aes(group = 1)) +\n  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, \n                                         name = \"Explained Variance (%) \\n\"\n                                         )\n                     ) +\n  labs(x = \"\\n Components\", y = \"Eigenvalues \\n\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs we can see from the scree plot, the first 3 components comprise 91.75% of the total variance, suggesting that these first three components are important to understanding the structure of the data set. The remaining components explain only 8.25% of the variability and may perhaps be noise.\nIs there any way to statistically show that these components are important? One method is permutation testing. If you are unfamiliar with permutation testing, we recommend checking out our Shiny dashboard here.\nBriefly, to perform permutation testing we will scramble the information down the columns, thus breaking the relationship between the years and their measures. Then, we will re-compute the SVD for the new scrambled data matrix. We will repeat these steps 2,000 times, forming a null distribution of eigenvalues from which to compare our originally observed eigenvalues.\n\nperm_iters &lt;- 2000  # number of permutation iterations\nset.seed(2019)      # sets seed for reproducible results\n\n# Initializes matrix to hold permutation results\nperm_res &lt;- matrix(data = 0, nrow = perm_iters, ncol = length(hawks_data_svd$d))\n\nfor(i in 1:perm_iters){\n  \n  this_matrix &lt;- apply(hawks_data_mat, 2, sample) # scrambles down columns\n  perm_res[i,] &lt;- svd(this_matrix)$d^2 # saves eigenvalues to perm_res\n  \n}\n\nNow let’s visualize these results against the observed values. We’ll determine that a component is significant if its original observed eigenvalue is greater than 95% of the values derived from the null distribution (i.e., permutation testing).\n\n# Converts to long format for plotting\nperm_res_long &lt;- as_tibble(perm_res) %&gt;% \n  gather(comps, eigen) %&gt;%\n  mutate(comps = as.numeric(gsub(\"V\", \"\", comps)))\n\n# Plots permutation results\nggplot(perm_res_long, aes(eigen)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(data = scree, \n             aes(xintercept = eigs), \n             color = rdgy[3], \n             linetype = 2\n             ) +\n  coord_cartesian(ylim = c(0, 800)) +\n  scale_y_continuous(minor_breaks = NULL) +\n  scale_x_continuous(minor_breaks = NULL) +\n  labs(x = \"\\n Eigenvalue\", \n       y = \"Frequency \\n\", \n       caption = \"\\n Note: Originally observed eigenvalues denoted by red dashed line.\"\n       ) +\n  facet_wrap(~comps) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like the only components that have a shot at being greater than 95% of the null distribution are components 1 and 2. Let’s see if this is the case:\n\nscree_sig &lt;- perm_res_long %&gt;% \n  group_by(comps) %&gt;% \n  summarise(ul = quantile(eigen, .975)) %&gt;% # computes upper limit of 95%\n  inner_join(., scree, by = \"comps\") %&gt;%\n  mutate(sig = ifelse(eigs&gt;ul, \"p &lt; .05\", \"p &gt; .05\"))\n\nggplot(scree_sig, aes(factor(comps), eigs)) +\n  geom_path(aes(group = 1), color = rdgy[8], linetype = 2) +\n  geom_point(aes(color = sig), size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, \n                                         name = \"Explained Variance (%) \\n\"\n                                         )\n                     ) +\n  scale_color_manual(values = c(rdgy[3], rdgy[9]), name = NULL) +\n  labs(x = \"\\n Components\", y = \"Eigenvalues \\n\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhat we suspected was correct: given a null distribution/hypothesis, only components 1 and 2 were greater than 95% permuted eigenvalues. In other words, there is less than a 5% chance that the pattern of the results seen on components 1 and 2 are this extreme given that the null hypothesis is true (i.e., no relationship). Therefore, we’ll pay special attention to components 1 and 2.\nNow that we know what components may be more important than others, let’s take a look at what we can learn from examining factor scores for the rows (years) and the columns (measures).\n\n\nRow-wise Factor Scores\nWe can explore how the years are seen through the components by first scaling (multiplying) the U matrix by the singular values.  \\[F_{years} = U\\Delta\\]  The %*% operator performs matrix algebra:\n\nyears &lt;- hawks_data_svd$u %*% diag(hawks_data_svd$d)  # scaling years data\nrownames(years) &lt;- rownames(hawks_data_mat)           # adds rownames\n\nNow let’s visualize the factor scores for components 1 and 2:\n\n# Builds dataframe of row-wise factor scores with notable events\nyears_fs &lt;- as_tibble(years) %&gt;%\n  mutate(Season = hawks_data_wide$Season) %&gt;%\n  left_join(., hawks_events, by = \"Season\") # imports notable events\n\n# Plots factor scores colored by Stanley Cup wins\n# tip: surrounding an action in R with () will automatically plot to screen\n(years_fs_scw &lt;- ggplot(years_fs, aes(V1, V2, color = SCW)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  scale_color_manual(values = c(rdgy[9], rdgy[2]), name = \"Stanley Cup Wins\") +\n  geom_text_repel(aes(label = Season), segment.alpha = 0, show.legend = FALSE) +\n  pca_furnish +\n  theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\nThe above graph plots the years as they are seen through the first 2 dimensions of the PCA and colors the points based on Stanley Cup wins. The years cluster together into three distinct groups\n\nThe 2007-2008 and 2008-2009 seasons\nThe 2013 season\nThe 2010-2018 seasons\n\nwhile the 2010 season is near the origin – meaning it doesn’t contribute much to either group. Interestingly, there seems to be no clear pattern in regards to regular season play and Stanley Cup wins because these winning seasons are not clustered together.\nA pattern that does seem to emerge is the improvement of the team over time across principal component 1 (x-axis). Principal component 2 is dominated by the 2012-2013 season, probably due to the uniqueness of this season: shortened season due to lockout, Presidents’ trophy winners, and Stanley Cup champions.\nLet’s now color the points based on playoff performance:\n\n# Plots factor scores colored by playoff performance\n# tip: surrounding an action in R with () will automatically plot to screen\n(years_fs_pf &lt;- ggplot(years_fs, aes(V1, V2, color = PF)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n  geom_text_repel(aes(label = Season), segment.alpha = 0, show.legend = FALSE) +\n  pca_furnish +\n  theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\nOne pattern of results that emerges, somewhat unexpectedly, is that the 2007-2008 and 2017-2018 seasons are contrasted on principal component 1 (x-axis). In both seasons, the Hawks missed the playoffs (did not qualify); however, the 2018 season is more similar to the seasons in which the Hawks made the playoffs. These results also highlight the 2015 Stanley Cup winning season, which is surrounded by seasons with early first round playoff exits. This may be indicative of an average regular season in 2015 (in respect to the Hawks), but an exceptional post season performance."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-factor-scores",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-factor-scores",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Column-wise Factor Scores",
    "text": "Column-wise Factor Scores\nThrough the PCA, we are also able to examine the factor scores from the team metrics/stats (i.e., columns). Let’s scale the V matrix by the singular values to obtain the factor scores. \\[F_{metric} = V\\Delta\\] Again, the %*% operator performs matrix algebra:\n\nmetric &lt;- hawks_data_svd$v %*% diag(hawks_data_svd$d) # scaling years data\nrownames(metric) &lt;- colnames(hawks_data_mat)          # adds rownames\n\n# Converts to long format for plotting\nmetric_fs &lt;- as_tibble(metric) %&gt;% mutate(Metric = colnames(hawks_data_mat))\n\nNow let’s visualize the factor scores for components 1 and 2:\n\n# Plots the column-wise factor scores\n(metric_plot &lt;- ggplot(metric_fs, aes(V1, V2)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  geom_text_repel(aes(label = Metric), segment.alpha = 0) +\n  pca_furnish\n)\n\n\n\n\n\n\n\n\nSimilar to the year-wise factor scores, we obtained 3 clusters of team-wise metrics across components 1 and 2. Component 1 contrasts penalty minutes (both drawn and taken) with Corsi (both For and Against), time on ice (TOI) and goals against (GA).\nThis contrast suggests a negative relationship with these metrics (e.g., as penalty minutes served increased, Corsi against decreased). This is sort of paradoxical, but it is similar to what we saw above with the correlation plot.\nAdditionally, the second component is dominated by Shooting Percentage, Save Percentage, and Goals For – and contrasted by GA. It makes sense that increases in Shooting Percentage would be related to increases in Goals For, which would also be related to decreases in Goals Against."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#factor-score-comparisons",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#factor-score-comparisons",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Factor Score Comparisons",
    "text": "Factor Score Comparisons\nTo provide more context, let’s compare the year-wise and metric-wise factor scores side-by-side:\n\ngrid_furnish &lt;- theme(legend.position = \"none\",\n                      axis.text = element_blank()\n                      )\n# Plots side-by-side\ngrid.arrange(years_fs_pf + grid_furnish, \n             metric_plot + grid_furnish, \n             nrow = 1\n             )\n\n\n\n\n\n\n\n\nFrom here we gain a rich perspective of how each year-wise clustering is described through patterns in team stats:\n\nCluster 1: the 2007-2008 and 2008-2009 seasons are well described by elevated penalty minutes (both drawn and taken)\nCluster 2: the 2011-2018 seasons are well described by elevated Corsi (both For and Against), Goals Against (GA), and time on ice (TOI)\nCluster 3: the 2012-2013 season was characterized by a high powered offense (elevated Shooting % and Goals For) and excellent defense (high Save % and low Goals Against)"
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#contributions",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Contributions",
    "text": "Contributions\nWe can also take a look at how much each season (row) or metric (column) contributes to each component. To calculate the contribution of each component, we square each factor score and divide it by the sum of squared factor, which is also the eigenvalue.\n \\[contribution = \\frac{f^2}{\\Sigma f^2} = \\frac{f^2}{\\lambda}\\] \nHere is a function that will compute the contributions:\n\n# Contribution calculator function ----\ncontribution &lt;- function(vector, sign = TRUE) {\n  \n  vector_sq &lt;- vector^2\n  vector_sq_sum &lt;- sum(vector_sq)\n  \n  if (sign == TRUE) {\n    vector &lt;- vector_sq/vector_sq_sum*sign(vector)\n  } else {\n    vector &lt;- vector_sq/vector_sq_sum\n  }\n  \n}"
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#row-wise-contributions",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#row-wise-contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Row-wise Contributions",
    "text": "Row-wise Contributions\nNow let’s apply the above function to calculate the contributions of the years (rows):\n\n# calculates the contribution of each row (e.g., season) ----\ncontributions_years &lt;- apply(years, 2, contribution)\n\n# add column names\ncolnames(contributions_years) &lt;- paste0(\"Component_\",1:ncol(contributions_years))\n\n# converts contributions to long format ----\ncontributions_years_long &lt;- contributions_years %&gt;% \n  as_tibble() %&gt;%                                   \n  mutate(Season = rownames(years)) %&gt;%              # add Seasons \n  reshape2::melt(value.name = \"Contributions\") %&gt;%  # ensure values are called contributions\n  group_by(variable) %&gt;%\n  mutate(Contributes = ifelse(abs(Contributions) &gt; abs(mean(Contributions)), \n                              \"Yes\", \n                              \"No\")\n         ) %&gt;%\n  ungroup()\n\nNow let’s plot these contributions. Any years that have contributions greater than the mean are said to contribute to the variability on that component:\n\n# calculate the mean of each component for graphing ----\ncontributions_years_means &lt;- contributions_years_long %&gt;%\n  group_by(variable) %&gt;%\n  summarise(mean = mean(Contributions)) %&gt;%\n  ungroup()\n\n# looking only at first two components ----\nyear_contrib &lt;- contributions_years_long %&gt;% \n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n  \nyear_contrib_means &lt;- contributions_years_means %&gt;%\n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n\n\n# plot the contribution bars with the +/- mean\n# filtering only the first two components \nggplot(year_contrib, aes(x = Season, y = Contributions, fill = Contributes)) +             \n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip(ylim = c(-.9, .9)) +\n  geom_hline(data = year_contrib_means, \n             aes(yintercept = mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +  \n  geom_hline(data = year_contrib_means, \n             aes(yintercept = -mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +\n  scale_fill_manual(values = c(rdgy[8], rdgy[3])) +\n  labs(x = \"Season \\n\",\n       y = \"\\n Contributions\",\n       caption = \"\\nNote: Dashed lines represent the mean contribution.\"\n       ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\"\n        ) +\n  facet_wrap(~variable, \n             labeller = as_labeller(c(`Component_1` = \"Component 1\\n\", \n                                      `Component_2` = \"Component 2\\n\"\n                                      )\n                                    )\n             )\n\n\n\n\n\n\n\n\nSeason 2008 and 2009 contribute more than the mean season on the negative side of component 1, while seasons 2016 to 2018 contribute more than the mean season on the positive side of component 1. In other words, the 2008 and 2009 seasons are in stark contrast to the 2016-2018 seasons on component 1.\nFor component 2, only season 2013 contributes more than the mean season and on the negative side of component 2. Thus, component 2 likely reflects the 2013 season."
  },
  {
    "objectID": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-contributions",
    "href": "posts/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis/post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Column-wise Contributions",
    "text": "Column-wise Contributions\nNow let’s repeat this procedure for the metrics (columns):\n\n# calculates the contribution of each row (e.g., metric)\ncontributions_metric &lt;- apply(metric, 2, contribution)\n\n# add column names\ncolnames(contributions_metric) &lt;- paste0(\"Component_\",1:ncol(metric)) \n\n# converts contributions to long format\ncontributions_metric_long &lt;- as_tibble(contributions_metric) %&gt;%\n  mutate(Metric = rownames(contributions_metric)) %&gt;%\n  reshape2::melt(value.name = \"Contributions\") %&gt;%\n  group_by(variable) %&gt;%\n  mutate(Contributes = ifelse(abs(Contributions) &gt; abs(mean(Contributions)), \n                              \"Yes\", \n                              \"No\")\n         ) %&gt;%\n  ungroup()\n\nAnd here are the contribution plots for the metrics on components 1 and 2:\n\n# calculate the mean of each component for graphing\ncontributions_metric_means &lt;- contributions_metric_long %&gt;%\n  group_by(variable) %&gt;%\n  summarise(mean = mean(Contributions))\n\n# looking only at first two components ----\nmetric_contrib &lt;- contributions_metric_long %&gt;% \n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n  \nmetric_contrib_means &lt;- contributions_metric_means %&gt;%\n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n\n# plot the contributions bars with +/-mean\n# filtering only the first two components\nggplot(metric_contrib, aes(x = reorder(Metric, Contributions), y = Contributions, fill = Contributes)) +             \n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip(ylim = c(-.9, .9)) +\n  geom_hline(data = metric_contrib_means, \n             aes(yintercept = mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +  \n  geom_hline(data = metric_contrib_means, \n             aes(yintercept = -mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +\n  scale_fill_manual(values = c(rdgy[8], rdgy[3])) +\n  labs(x = \"Metric \\n\",\n       y = \"\\n Contributions\",\n       caption = \"\\nNote: Dashed lines represent the mean contribution.\"\n       ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\"\n        ) +\n  facet_wrap(~variable, \n             labeller = as_labeller(c(`Component_1` = \"Component 1\\n\", \n                                      `Component_2` = \"Component 2\\n\"\n                                      )\n                                    )\n             )\n\n\n\n\n\n\n\n\nPenalty minutes drawn (PEND) and penalty minutes served (PENT) contribute more than the mean metric to the negative side of component 1; while Corsi Against (CA), Corsi For (CF), Goals Against (GA), and Time on Ice (TOI) contribute more than the mean metric to the positive side of component 1.\nGoals For (GF), Save Percentage (SavePerc), and Shooting Percentage (ShootPerc) contribute more than the mean metric to the negative side of component 2; while Goals Against (GA) contributes more than the mean metric to the positive side of component 2.\nGoals Against (GA) appears to contribute to both the positive side of component 1 and 2.\nTaking into both the row-wise and column-wise contributions confirm the factor score plots:\nComponent 1\n\nSeason 2008 and 2009 are associated with penalty minutes drawn and served (PEND and PENT, respectively) and both contribute negatively to component 1.\nOn the other hand, seasons 2016 to 2018 are associated with Corsi For (CF), Corsi Against (CA), Goals Against (GA), and Time on Ice (TOI) and contribute to the positively to component 1.\n\nComponent 2\n\nSeason 2013 is associated with Goals For (GF), Save Percentage (SavePerc), and Shooting Percentage (ShootPerc) and contribute negatively to component 2.\nOn the other hand, Goals Against (GA) contributes positively to component 2."
  },
  {
    "objectID": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-1",
    "href": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-1",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Level 1",
    "text": "Level 1\nThe level 1 modeling ignores the conference and division distinction. Each team will be modeled individually using a linear regression to predict powerplay goals per games played as a function of time (across 10 years). There are 30 teams in this dataset. Therefore, 30 distinct linear regressions will be performed; one for each team in the NHL. This allows us account for the variability of power plays within each team.\nThe code for this is made extremely efficient thanks to dplyr pipe-ing (%&gt;%):\n\nmodTeam &lt;-  \n  data5v4 %&gt;% \n  nest_by(Team) %&gt;%\n  mutate(level1 = list(lm((GF/GP) ~ StartYear, data = data)))\n\nExtracting omnibus level statistics, such as R2, for each of the 30 linear regressions and placing them in a easy-to-use dataframe was done using the ‘broom’ package function ‘glance’:\n\nlevel1Omni &lt;- \n  modTeam %&gt;% \n  summarise(broom::glance(level1)) %&gt;% \n  ungroup() %&gt;% \n  mutate(sig = p.value &lt; .05)\n\nWe’ll use these omnibus estimates to examine all 30 regression models simultaneously via R2 estimates. These allow us to see how much variability in powerplay goals per games played was explained by time:\n\n# Color palette\nsigColorPal &lt;- brewer.pal(11,'RdGy') # display.brewer.pal(11,'RdGy')\n\n# R^2 Plot\nggplot(level1Omni, aes(r.squared, reorder(Team, r.squared), color = sig)) +\n  geom_point(size = 2) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  labs(x = 'R-squared', y = 'Team') +\n  guides(color = guide_legend(title = 'p &lt; .05')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also similarly examine the regression coefficient of time for each NHL team. This coefficient tells us what was the predicted change in powerplay goals per games played across the ten years (2007-2017).\nLet’s first extract these coefficients using dplyr pipeing (%&gt;%) and the ‘broom’ package function ‘tidy’:\n\n# Extracting level 1 coefficients\nlevel1Coef &lt;- \n  modTeam %&gt;% \n  summarise(broom::tidy(level1)) %&gt;% \n  ungroup() %&gt;%\n  filter(term == 'StartYear') %&gt;%   # Facilitates plotting\n  mutate(sig = p.value &lt; .05)       # For later plotting\n\nNow let’s plot these coefficients ordered by size:\n\nggplot(level1Coef, aes(estimate, reorder(Team, -1*estimate), color = sig)) +\n  geom_point(size = 2) +\n  geom_errorbarh(\n    aes(xmin = estimate - std.error, xmax = estimate + std.error),\n    alpha = 1/2\n    ) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  labs(\n    x = 'Estimate (Yearly Change in Power Play Goals/Game)', \n    y = 'Team',\n    caption = 'SEM error bars'\n    ) +\n  guides(color = guide_legend(title = 'p &lt; .05')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs we can see from the plot above, each team had a different predicted rate of change in powerplay goals per games played from 2007-2017. The x-axis here represents the regression coefficient of time. For example, the Chicago Blackhawks had an estimate close to -.02. This means that for every increase in 1 year (every season) the model predicted a decrease of .02 powerplay goals per games played. However, this estimate is colored grey. This means that it was not found to be significantly different from zero.\nFor a more comprehensive picture, let’s look at all the teams in the NHL plotted with both the observed (actual) data and model predicted trendlines. To do this, let’s first extract the model fits by using dplyr pipeing (%&gt;%) and the ‘broom’ package function ‘augment’:\n\n# Extracting level 1 model fits\nlevel1Fits &lt;- \n  modTeam %&gt;% \n  summarise(broom::augment(level1)) %&gt;%\n  ungroup() %&gt;%\n  mutate(sig = rep(level1Coef$sig, each = length(unique(StartYear))))\n\nAnd then plot them using ggplot:\n\nggplot(level1Fits, aes(StartYear, color = sig)) +\n  geom_line(aes(y = .fitted), alpha = 2/3) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  geom_line(aes(y = `(GF/GP)`), color = 'black') +\n  labs(\n    x = 'Season (Start Year)',\n    y = 'Power Play Goals/Games Played (5v4)'\n    ) +\n  scale_x_continuous(breaks = c(2008, 2015)) +\n  facet_wrap(~Team, ncol = 5) +\n  guides(color = guide_legend(title = 'p &lt; .05')) +\n  theme_minimal() +\n  theme(legend.position = 'bottom')"
  },
  {
    "objectID": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-2",
    "href": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-2",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Level 2",
    "text": "Level 2\nHere’s what we’ve learned from level 1:\n\nThe rate of change in powerplay goals per games played seems different from team to team\nThere seems to be a general decrease in powerplay goals per games played across time\n\nWe can take this a step further by asking the question: “Are powerplay goals per games played changing as a function of time across all NHL teams on average.”\nOne way to test this question is to see if the regression coefficients of time from level 1 across all 30 NHL teams, on average, differ from zero.\nOur coefficients are already in a dataframe named ‘level1Coef’, so let’s use this dataframe to test whether these estimates are different from zero:\n\nlevel2 &lt;- lm(estimate ~ 1, data = level1Coef)\nsummary(level2)\n\n\nCall:\nlm(formula = estimate ~ 1, data = level1Coef)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.027532 -0.004719  0.003220  0.007924  0.017478 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.016550   0.002254  -7.342 4.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01235 on 29 degrees of freedom\n\n\nAs we can see from this summary output of the level 2 model, there is a significant decrease in powerplay goals per games played over time on average across teams.\nThis information was gleaned in the following ways:\n\nThe estimate is -0.0166\n\nBy being negative, the estimate tells us that powerplay goals are decreasing over time. Specifically, they are decreasing at a rate of 0.0166 goals per games played per year\n\nThis is statistically different from 0\n\nThe t-value is -7.34 with a p-value &lt; .001\nOne way to visualize these results is to plot each team’s predicted model from level 1. Running a linear model on these predicted estimates is equivalent to their average over time. The following code illustrates this process in ggplot2. One plot calculates the linear regression model of these estimates, while the other simply averages them over each timepoint:\n\nlevel2Fits &lt;- augment(level2) # Extracts model fits\n\n# LM plot\nlmPlot &lt;- \n  ggplot(level1Fits, aes(StartYear, .fitted, group = Team)) +\n  geom_line(alpha = 1/3) +\n  geom_smooth(aes(group = 1), method = 'lm', color = 'purple', se = F) +\n  labs(\n    x = 'Season Start Year',\n    y = 'Level 1 Estimates (Power Play Goals/Games Played)',\n    title = 'Linear Model Plot'\n    ) +\n  theme_minimal()\n\n# Average plot\navPlot &lt;- \n  ggplot(level1Fits, aes(StartYear, .fitted, group = Team)) +\n  geom_line(alpha = 1/3) +\n  stat_summary(\n    aes(group = 1), \n    fun = 'mean', \n    geom = 'line', \n    color = 'red', \n    size = 1\n    ) +\n  labs(x = 'Season Start Year', y = NULL, title = 'Average Plot') +\n  theme_minimal()\n\n# Plots them together\ngrid.arrange(lmPlot, avPlot, ncol = 2)\n\n\n\n\n\n\n\n\nAs we can see above, both these processes produce the same result. Powerplay goals are decreasing, on average, given each team’s performace across one decade of play (2007-2017)."
  },
  {
    "objectID": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-2-extended",
    "href": "posts/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data/post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#level-2-extended",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Level 2 Extended",
    "text": "Level 2 Extended\nSo far we’ve modeled each team individually using linear regression and then compared the regression coefficients from each of these 30 models. However, we’ve ignored the hierarchical structure of these data. Teams are nested within divisions that are nested within conferences that are nested within leagues (in this case, only one = the NHL).\nIn this section, we’ll explore the differences between conferences and divisions by adding these predictors to the level 2 analysis. Again, each team’s division and conference affiliation was determined using the 2013-2014 NHL realignment. This has serious consequences for interpreting the results and is shown for illustrative purposes.\nThe divisions and conferences are considered categorical variables. When using categorical variables, it is best to have a priori (planned) comparisons. Thus, we have the following planned predictions:\n\nThe Western conference will have higher powerplay goals/games played than the Eastern conference\nWithin the West, the Central division will have higher powerplay goals/games played than the Pacific\nWithin the East, the Metropolitan division will have higher powerplay goals/games played than the Atlantic\n\nIn order to incorporate these planned comparisons, we have come up with the following coding scheme:\n\n# Creates the data frame for html table\ncontrasts &lt;- \n  data.frame(\n    Contrast = c(\n      'Conference (West vs. East)', \n      'West (Pacific vs. Central)', \n      'East (Metropolitan vs. Atlantic)'\n      ),\n    Pacific = c(.5, -.5, 0),\n    Central = c(.5, .5, 0),\n    Metropolitan = c(-.5, 0, .5),\n    Atlantic = c(-.5, 0, -.5)\n    )\n\n# Prints table\nkable(contrasts) %&gt;%\n  kable_styling(bootstrap_options = c('striped', 'hover', 'responsive'))\n\n\n\n\n\nContrast\nPacific\nCentral\nMetropolitan\nAtlantic\n\n\n\n\nConference (West vs. East)\n0.5\n0.5\n-0.5\n-0.5\n\n\nWest (Pacific vs. Central)\n-0.5\n0.5\n0.0\n0.0\n\n\nEast (Metropolitan vs. Atlantic)\n0.0\n0.0\n0.5\n-0.5\n\n\n\n\n\n\n\n\nThese contrasts were made so that the estimates would be more interpretable. For example, the regression coefficient estimate for the West contrast will now represent the actual mean difference of Pacific vs. Central for powerplay goals/games played.\nUsing the level1Coef dataframe, let’s enter 3 contrast columns, one for each planned comparison, based upon the table above:\n\n# Division assignment\npacific &lt;- c('S.J', 'CGY', 'L.A', 'ANA', 'EDM', 'VAN', 'ARI')\ncentral &lt;- c('ATL.WPG', 'NSH', 'STL', 'DAL', 'COL', 'MIN', 'CHI')\nmetropolitan &lt;- c('WSH', 'N.J', 'PHI', 'CBJ', 'PIT', 'NYR', 'NYI', 'CAR')\natlantic &lt;- c('T.B', 'BOS', 'TOR', 'DET', 'MTL', 'FLA', 'OTT', 'BUF')\n\n# Assigning contrasts based on vectors above\nlevel1Coef &lt;- level1Coef %&gt;% ungroup() %&gt;%\n  mutate(conf = case_when(level1Coef$Team %in% pacific ~ .5,\n                              level1Coef$Team %in% central ~ .5,\n                              level1Coef$Team %in% metropolitan ~ -.5,\n                              level1Coef$Team %in% atlantic ~ -.5),\n         west = case_when(level1Coef$Team %in% pacific ~ -.5,\n                              level1Coef$Team %in% central ~ .5,\n                              level1Coef$Team %in% metropolitan ~ 0,\n                              level1Coef$Team %in% atlantic ~ 0),\n         east = case_when(level1Coef$Team %in% pacific ~ 0,\n                              level1Coef$Team %in% central ~ 0,\n                              level1Coef$Team %in% metropolitan ~ .5,\n                              level1Coef$Team %in% atlantic ~ -.5))\n\nWe can now run a linear regression on the regression coefficients from level 1 again, but this time we’ll add the contrast coding scheme that we just created to the model in level 2:\n\nlevel2Div &lt;- lm(estimate ~ conf + west + east, data = level1Coef)\nsummary(level2Div)\n\n\nCall:\nlm(formula = estimate ~ conf + west + east, data = level1Coef)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.024390 -0.008290  0.002767  0.010203  0.014928 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.016514   0.002293  -7.203 1.19e-07 ***\nconf         0.001077   0.004586   0.235    0.816    \nwest         0.007432   0.006698   1.110    0.277    \neast         0.005835   0.006265   0.931    0.360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01253 on 26 degrees of freedom\nMultiple R-squared:  0.07651,   Adjusted R-squared:  -0.03005 \nF-statistic: 0.718 on 3 and 26 DF,  p-value: 0.5502\n\n\nNow let’s walk through this new summary output.\nFirst, our intercept estimate from level 2 and level 2 extended are very similar. Powerplay goals/games played are still significantly decreasing at a rate of .0166 per each year of NHL gameplay.\nSecond, our three planned contrasts were named ‘conf’, ‘west’, and ‘east’. Taking a look at their estimates, they were all positive. This means that our predictions were in the right direction. However, none of these predictions were statistically significant. In other words, there is no significant difference in powerplay goals/games played between Western and Eastern conference teams, the Pacific and Central divisions, nor the Metropolitan and Atlantic divisions.\nNevertheless, let’s take a look at each of these in turn. First, we’ll extract the fitted values from the level 2 extended model for each team:\n\n# Extracts estimates from each team from level 2 extended\nlevel2DivTeam &lt;- \n  level2Div %&gt;% \n  broom::augment(se_fit = TRUE) %&gt;% \n  mutate(team = level1Coef$Team) %&gt;% \n  ungroup() %&gt;%\n  mutate(\n    division = case_when(\n      team %in% pacific ~ 'pacific',\n      team %in% central ~ 'central',\n      team %in% metropolitan ~ 'metropolitan',\n      team %in% atlantic ~ 'atlantic'\n      ),\n    division = factor(division)\n    )\n\n# Re-orders factor for plotting\nlevel2DivTeam$division &lt;- \n  factor(level2DivTeam$division, levels(level2DivTeam$division)[c(4,2,3,1)])\n\nAfter extracting this information, let’s take a look within the Western conference and compare the Pacific and Central divisions:\n\n# Gathers summary info\nwest_sum &lt;- \n  level2DivTeam %&gt;%\n  group_by(west) %&gt;%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    ) %&gt;%\n  filter(west != 0)\n\n# Plot\nggplot(west_sum, aes(factor(west), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Western Conference Divisions', \n    label = c('Pacific', 'Central')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot above depicts that, on average, the Pacific division is decreasing its powerplay goals/games played at a faster rate than the Central division. Although this is not a significant difference, it is still interesting.\nNext, let’s take a look at within the Eastern conference between the Atlantic and Metropolitan divisions:\n\n# Gathers summary info\neast_sum &lt;- \n  level2DivTeam %&gt;%\n  group_by(east) %&gt;%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    ) %&gt;%\n  filter(east != 0)\n\n# Plot\nggplot(east_sum, aes(factor(east), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Eastern Conference Divisions', \n    label = c('Atlantic', 'Metropolitan')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSimilar to the Western conference, the teams in the Atlantic division, on average, have a greater decrease in their powerplay goals/games played compared to their conference counterparts–the Metropolitan division teams. This is not a significant difference, though.\nFinally, let’s compare the Western and Eastern conferences:\n\n# Gathering summary information\nconf_sum &lt;- \n  level2DivTeam %&gt;%\n  group_by(conf) %&gt;%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    )\n\n# Plot\nggplot(conf_sum, aes(factor(conf), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'NHL Conference', \n    label = c('East', 'West')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterestingly, the Eastern conference is decreasing in their powerplay goals/games played at a faster rate than the Western conference teams. Again, not a significant difference though.\nIt may be more helpful to examine these slopes side-by-side:\n\n# Taking a look at the west contrast (Pacific -.5 vs. Central +.5)\nyax.lock &lt;- c(-.025, -.01)\nwestPlot &lt;- ggplot(west_sum, aes(factor(west), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Western Conference Divisions', \n    label = c('Pacific', 'Central')\n    ) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n# Taking a look at the east contrast (Atlantic -.5 vs. Metropolitan +.5)\neastPlot &lt;- ggplot(east_sum, aes(factor(east), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Eastern Conference Divisions',\n    label = c('Atlantic', 'Metropolitan')\n    ) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = NULL) +\n  theme_minimal()\n\n# Taking a look at the conference (East -.5 vs. West +.5)\nconfPlot &lt;- ggplot(conf_sum, aes(factor(conf), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete('NHL Conference', label = c('East', 'West')) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = NULL) +\n  theme_minimal()\n\ngrid.arrange(westPlot, eastPlot, confPlot, ncol = 3)\n\n\n\n\n\n\n\n\nOr perhaps on a single plot detailing all 4 divisions across the 2 conferences:\n\nggplot(level2DivTeam, aes(division, .fitted, group = factor(conf))) +\n  geom_pointrange(aes(ymax = .fitted + .se.fit, ymin = .fitted - .se.fit)) +\n  geom_line(alpha = 1/3) +\n  coord_cartesian(ylim = yax.lock) +\n  scale_x_discrete(\n    labels = c('Pacific', 'Central', 'Metropolitan', 'Atlantic')\n    ) +\n  labs(\n    x = 'Division', \n    y = 'Yearly Change in Power Play Goals/Games Played',\n    caption = 'SEM error bars'\n    ) +\n  theme_minimal()"
  },
  {
    "objectID": "posts/post-My-Reads/post-My-Reads.html",
    "href": "posts/post-My-Reads/post-My-Reads.html",
    "title": "My Reads",
    "section": "",
    "text": "I absolutely love reading and collecting books. Whenever I’m in a phase where I am constantly reading, I feel more creative and confident in my academic work. However, it’s easy to put reading off when life gets in the way.\nTo motivate me to read more, I tracked the number of pages I read per day through the year 2019 (I was trying for around 20 pages per day). I found the process of tracking my reading progress a daily reading motivator and I highly recommend it for those looking increase the amount you read."
  },
  {
    "objectID": "posts/post-My-Reads/post-My-Reads.html#my-favorite-reads-in-graduate-school",
    "href": "posts/post-My-Reads/post-My-Reads.html#my-favorite-reads-in-graduate-school",
    "title": "My Reads",
    "section": "My Favorite Reads (in Graduate School)",
    "text": "My Favorite Reads (in Graduate School)\nHere’s a list of books that have heavily influenced my work one way or another. I’m sure there are plenty more that I haven’t thought of, but I’ll add them along the way.\nWhat are your favorite books and why? I’d love to know if you have any recommendations of books I should read and add to my list! Just leave a comment below.\n\n\n\n\n\n\nAnalyzing Neural Time Series Data: Theory and Practice Mike X Cohen\n\nA definite must read for anyone interested in performing time frequency analyses with their EEG data or designing experiments for later time frequency decompositions. Dr. Cohen artfully explains a myriad of time-frequency methods by blending conceptual understanding and practical application with code examples/tips, equations, and wonderful figures. I really appreciated his bottom-up approach in EEG analyses, often doing things by “scratch”, i.e., writing Matlab code to perform computations on matrices of data, rather than using software packages and toolboxes (e.g., EEGLAB or ERPLAB). I must admit that I didn’t grasp every concept on my first read through, but I will definitely revisit come analysis time!\n\n\n\n\n\n\n\nAn Introduction to the Event-Related Potential Technique (Second Edition) Steven J. Luck\n\nThis is an excellent introduction into the event-related pontential (ERP) technique for anyone who is interested in using ERPs in their research. I highly recommend reading this, especially if you are new to the technique, or even if you have already done ERP work in the past. I’ve learned a great deal from Dr. Luck’s recommendations and philosophy on using ERPs to study cognition, especially about details on high/lowpass filters, time-frequency analysis, and artifact correction/rejection. His tips on collecting EEG signals have made my own EEG recordings cleaner and I look forward to implementing his techniques in my future work.\n\n\n\n\n\n\n\nThe Neuroscience of Freedom and Creativity: Our Predictive Brain Joaquin Fuster\n\nIn his latest book, Dr. Fuster argues that humans have free will and our freedom to choose is enabled by our cerebral cortex, the prefrontal cortex in particular. A concept that particularly stood out to me, among many, was his argument that consciousness is an epiphenomenon of cerebral cortex processing. “Consciousness is an obligatory byproduct of intense network activation, not its cause” (p. 31). A profound read that touches on the interaction between the brain and environment, which inludes the emotional perception-action cycle from the limbic system, and its implications in the real world, such as culture, government, and financial systems.\n\n \n\nCortex and Mind: Unifying Cognition | Memory in the Cerebral Cortex: An Empirical Approach to Neural Networks in the Human and Nonhuman Primate Joaquin Fuster\n\nThe ideas, theories, and explanations that Dr. Fuster explicates in his books serve as an excellent framework for understanding the interaction between the cerebral cortex and our environment. Memory in the Cerebral Cortex provides a nice introduction to Dr. Fuster’s theory of cortical memory and the perception action cycle, while drawing from primate and human studies. Cortex and Mind expands these ideas and breaks the discussion into chapters on cognitive functions like memory, language, perception, working memory (attention), and intelligence.\n\n\n\n\n\n\n\nData Analysis: A Model Comparison Approach to Regression, ANOVA, and Beyond (Third Ed.) Charles M. Judd, Gary H. McClelland, & Carey S. Ryan\n\nJudd, McClelland, and Ryan describe regression gradually, with each chapter building on the next, in an easy to understand way. The model comparison approach to regression and its various extensions (e.g., single predictor, non-linear, repeated measures, mixed-effects models, etc.) taught in this book helped me grasp regression at a deeper level. I highly recommend, especially if you already have some experience using regression.\n\n  \n\nThe Visual Display of Quantatative Information | Visual Explanations | Envisioning Information Edward R. Tufte\n\nAcross these three books, I fell in love with the process of data visualization and all the different ways one can clearly and effectively tell a story through data. I immediately began to re-think the way I visualize my data and implement Tufte’s principles in my own work. I especially appreciated the guidance in using color in data viz. For instance, using color palettes that we are accustomed to seeing everyday, such as the gradient of colors seen at sunrise and sunset, help elucidate patterns in a natural and ingenious way.\n\n\n\n\n\n\n\nGgplot2: Elegant Graphics for Data Analysis Hadley Wickham\n\nI highly recommend reading this, from front to back, before creating another plot in R. ggplot2 is an indespensible tool in any data scientist’s repetoire for understanding and visualizing his or her data. Part III was especially informative for my own work and gave me plenty of ideas to explore. For instance, the comination of using the broom package and plotting the regression results to explore between and within models deepened my understanding of the relationship between data analysis and data visualization.\n\n\n\n\n\n\n\nDiscovering Statistics Using R Andy Field\n\nThis was my introduction into R that really got me from a slow, painful crawl to a full on sprint into data processing and analysis. Although I would still recommend this book, new developments and packages in the R community have changed the landscape entirely. I additionally encourage beginners to read the free R for Data Science by Garrett Grolemund and Hadley Wichkam."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "",
    "text": "It’s that time of the year again–MARCH MADNESS.\nA few of us graduate students decided it would be fun to simulate this year’s NCAA Men’s National Championship tournament using R. When we all met to discuss our method to simuluate tournament performance, we quickly realized that each of us had a different method in mind. These methods differed in not only how we determined probability of win via seed, but also the techniques and packages implemented within R. Below showcases 3 different methods we used, each with different results. The methods are arranged in order of increasing use of historical data."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#simulation",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#simulation",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "Simulation",
    "text": "Simulation\nThis method (that Nick actually invented) relies on the seed of each team in the tournament. The idea is to assign a range of numbers to each seed with the ranges of lower seeds (e.g., #1, #2, #3, …) containing greater numbers than the ranges of higher seeds (e.g., …, #14, #15, #16). Just like this:\n\n\n\n\n\n\nSeed\nLower.Bound\nUpper.Bound\n\n\n\n\n1\n160\n320\n\n\n2\n150\n310\n\n\n3\n140\n300\n\n\n4\n130\n290\n\n\n5\n120\n280\n\n\n6\n110\n270\n\n\n7\n100\n260\n\n\n8\n90\n250\n\n\n9\n80\n240\n\n\n10\n70\n230\n\n\n11\n60\n220\n\n\n12\n50\n210\n\n\n13\n40\n200\n\n\n14\n30\n190\n\n\n15\n20\n180\n\n\n16\n10\n170\n\n\n\n\n\n\n\n\nThen, randomly sample from each team’s range and assign the win to the higher number (for each match-up). This is a crude method that will roughly correspond to seed. In other words, each seed has a higher probability of winning over each subsequent seed; however, it is theoretically possible, although very unlikely, for a 16th seed to win the tournament. A simulation given purely historical data would not be able to account for this because a 16th seed has never made it past the first round prior to the 2018 tournament.\nBelow is the code I used to run the simulation. I tried to avoid for loops as much as possible and learn the map() functions in the purrr package. The code is not as efficient as I would have liked it to be, but it gets the job done and allows for a variety of plots.\n\n# Sets the regions\nregions &lt;- c('South', 'East', 'West', 'Midwest')\n\n# Import the team names and their seeds from 2018\n# These data are being pulled from this website's Github repo\nlink &lt;- 'https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/ncaa-2018-teams-updated.csv'\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata &lt;- GET(link)\nncaa.2018 &lt;- \n  read_csv(content(data, \"raw\")) %&gt;% \n  rename(region = Region, team = Team1, seed = Seed)\n\n# Initializations\niters &lt;- 1000\nresults &lt;- probTableHolder &lt;- vector('list', length = iters)\n\n# Sets seed for reprodicible results\nset.seed(18)\n\nThe simulation was then ran 1000 times:\n\nfor(i in 1:iters) {\n\n  # Creates whole tournament probability set\n  probTableHolder[[i]] &lt;- probTable &lt;- regions %&gt;% \n    map_df(~tibble(seed = c(1:16),\n                   lowNum = seq(160, 10, -10),\n                   highNum = seq(320, 170, -10)) %&gt;%\n             group_by(seed) %&gt;% \n             mutate(result = sample(lowNum:highNum, 1)) %&gt;%\n             ungroup()) %&gt;%\n    mutate(region = rep(regions, each = 16)) %&gt;%\n    inner_join(ncaa.2018, by = c('seed', 'region'))\n  \n  # Round 1\n  r1 &lt;- regions %&gt;%\n    map_df(~tibble(match.up = 1:8, t1.seed = 1:8, t2.seed = 16:9) %&gt;%\n             mutate(t1.res =  probTable$result[1:8],\n                    t2.res = probTable$result[16:9],\n                    seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n             mutate(region = rep(regions, each = 8)) %&gt;%\n             inner_join(probTable, by = c('seed', 'region'))\n    \n  # Round 1 results\n  r1.res &lt;- r1 %&gt;% select(match.up, region, team, seed, result)\n  \n  # Round 2\n  r2 &lt;- regions %&gt;%\n    map_df(~tibble(r2.match.up = 1:4,\n                r2.match.up.1 = 1:4,\n                r2.match.up.2 = as.integer(seq(8, 5, -1)),\n                region = .) %&gt;%\n          left_join(r1.res, \n                    by = c('r2.match.up.1' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r1.res,\n                    by = c('r2.match.up.2' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable, by = c('region', 'seed'))\n  \n  # Round 2 results\n  r2.res &lt;- r2 %&gt;% select(r2.match.up, region, team, seed, result)\n  \n  # Round 3\n  r3 &lt;- regions %&gt;%\n    map_df(~tibble(r3.match.up = 1:2, \n                r3.match.up.1 = 1:2, \n                r3.match.up.2 = 4:3,\n                region = .) %&gt;%\n          left_join(r2.res, \n                    by = c('r3.match.up.1' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r2.res,\n                    by = c('r3.match.up.2' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 3 results\n  r3.res &lt;- r3 %&gt;% select(r3.match.up, region, team, seed, result)\n  \n  # Round 4\n  r4 &lt;- regions %&gt;%\n    map_df(~tibble(r4.match.up = 1, \n                   r4.match.up.1 = 1, \n                   r4.match.up.2 = 2,\n                   region = .) %&gt;%\n             inner_join(r3.res, \n                        by = c('r4.match.up.1' = 'r3.match.up', \n                               'region' = 'region')) %&gt;%\n             rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n             inner_join(r3.res, by = c('r4.match.up.2' = 'r3.match.up',\n                                       'region' = 'region')) %&gt;%\n             rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n             mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 4 results\n  r4.res &lt;- r4 %&gt;% select(r4.match.up, region, team, seed, result)\n  \n  # Semifinal\n  semi.final &lt;- tibble(t1.region = c('South', 'East'),\n                       t2.region = c('West', 'Midwest')) %&gt;%\n    left_join(r4.res, by = c('t1.region' = 'region')) %&gt;%\n    rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n    left_join(r4.res, by = c('t2.region' = 'region')) %&gt;%\n    rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  # Semifinal results\n  semi.final.res &lt;- semi.final %&gt;% select(region, team, seed, result)\n  \n  # Final\n  final &lt;- tibble(t1.region = semi.final.res$team[1],\n                  t2.region = semi.final.res$team[2],\n                  t1.team = semi.final.res$team[1],\n                  t2.team = semi.final.res$team[2],\n                  t1.seed = semi.final.res$team[1],\n                  t2.seed = semi.final.res$team[2],\n                  t1.res = semi.final.res$result[1],\n                  t2.res = semi.final.res$result[2]) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  final.res &lt;- final %&gt;% select(region, team, seed, result)\n  \n  results[[i]] &lt;- this.result &lt;- bind_rows(r1.res, r2.res, r3.res, r4.res,\n                                           semi.final.res, final.res, \n                                           .id = 'round')\n  \n  #print(i) # Uncomment to see progress\n}"
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#data-viz",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#data-viz",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "Data Viz",
    "text": "Data Viz\nThe results of this simulation were saved out and can be imported from this website’s Github data repository like this:\n\n# Loading probability tables from github\nprobTableLink &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/probTableHolder-updated.RData?raw=true'\nsource_data(probTableLink)\n\n# Loading simulation results from github\nresultsLink &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/results-updated.RData?raw=true'\nsource_data(resultsLink)\n\nFirst, let’s ensure that the probability table results worked across simulations and regions:\n\n# Converts into a dataframe\nprobTable.long &lt;- probTableHolder %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Calculates summary stats\nprobTable.sum &lt;- probTable.long %&gt;% \n  group_by(seed, region) %&gt;%\n  summarize(m = mean(result),\n            sd = sd(result),\n            n = n(),\n            sem = sd/sqrt(n))\n\n# Probability table plot\nggplot(probTable.long, aes(factor(seed), result, color = region)) +\n  geom_point(position = 'jitter', alpha = 1/3, shape = 1) +\n  geom_pointrange(\n    data = probTable.sum, \n    aes(factor(seed), m, color = region, ymin = m - sd, ymax = m + sd),\n    color = 'black'\n      ) +\n  geom_line(\n    data = probTable.sum, \n    aes(factor(seed), m, group = 1, color = region), \n    color = 'black', alpha = 1/2\n    ) +\n  scale_color_brewer(palette = 'Set2', name = 'Region') +\n  labs(x = 'Seed', y = 'Randomly Chosen Value', caption = 'SD Error Bars') +\n  theme_minimal() +\n  facet_wrap(~region)\n\n\n\n\n\n\n\n\nThis above visualization depicts the potential ranges from which each seed can draw an arbitrary number. This drawing occurs at the beginning of each simulation iteration and determines each team’s tournament fate. Higher seeds have an advantage of a higher possible range, but this overlap gradually decreases with increases in seed.\nNow let’s examine the results of the simulations. As a check, better seeds should win with more frequency each round than worse seeds:\n\n# Converts to long dataframe\nresults.long &lt;- results %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Win histogram\nggplot(results.long, aes(as.factor(seed), fill = region)) +\n  geom_histogram(stat = 'count') +\n  facet_grid(round~region) +\n  scale_x_discrete(breaks = seq(1, 16, 3)) +\n  labs(x = 'Seed', y = 'Win Frequency', title = 'Number of Wins Per Round') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe above plot confirms this; higher seeds win more often than lower seeds for each round.\nNow for fun, let’s see which teams won the tournament most often.\n\n# Round 6 summary information\nresults.long.sum1 &lt;- results.long %&gt;% \n  filter(round == '6') %&gt;% \n  group_by(team, region) %&gt;%\n  summarise(n = n(), seed = unique(seed))\n\n# Winner plot\nggplot(results.long.sum1, aes(reorder(team, -1*n), n, fill = region)) +\n  geom_bar(stat = 'identity') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  labs(\n    x = 'Team', y = 'NCAA Tournament Wins', \n    caption = paste('Simulation iterations:', iters)\n    ) +\n  geom_text(aes(label = seed), nudge_y = 4, color = 'grey') + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThere are some clear, but expected, patterns that emerge given a simulation based solely on seed. First, #1 seeds tend to win more often than lower seeds. Second, win frequency tracks well with seed, but in a diminishing step-wise way. The margin is closer between #3 vs #4 seeds, than #2 vs #3 seeds, than #1 vs #2. Perhaps this has to do with the NCAA men’s tournament not re-seeding teams on a round-by-round basis (see this video by Jon Bois).\nHowever, this method did allow for some upsets. As we can see, three #6 seeds won the tournament (TCU, Houston, Miami)!\nSo, does this analysis inform my bracket this year at all? Sort of, but I’d rather just have R do it for me.\nI’ll just have the computer pick a random iteration that occured and I’ll use that to fill out my bracket:\n\n# Random selection from the set of simulation iterations\n# note: seed was set earlier in script\nmyPick &lt;- sample(1:iters, 1)\n\n# Matt's bracket from a random simulation iteration\nmattsBracket &lt;- results.long %&gt;% \n  filter(iter == myPick) %&gt;% \n  select(round, region, team, seed) %&gt;%\n  rename(winner = team)\n\nSo here’s what the computer drew for me (winners of each round):\n\n\n\n\n\n\n\nround\nregion\nwinner\nseed\n\n\n\n\n1\nSouth\nVirginia\n1\n\n\n1\nSouth\nCincinnati\n2\n\n\n1\nSouth\nTennessee\n3\n\n\n1\nSouth\nArizona\n4\n\n\n1\nSouth\nKentucky\n5\n\n\n1\nSouth\nLoyola Chicago\n11\n\n\n1\nSouth\nNevada\n7\n\n\n1\nSouth\nCreighton\n8\n\n\n1\nEast\nVillanova\n1\n\n\n1\nEast\nPurdue\n2\n\n\n1\nEast\nTexas Tech\n3\n\n\n1\nEast\nWichita State\n4\n\n\n1\nEast\nWest Virginia\n5\n\n\n1\nEast\nSt. Bona\n11\n\n\n1\nEast\nArkansas\n7\n\n\n1\nEast\nVirginia Tech\n8\n\n\n1\nWest\nXavier\n1\n\n\n1\nWest\nNorth Carolina\n2\n\n\n1\nWest\nMichigan\n3\n\n\n1\nWest\nGonzaga\n4\n\n\n1\nWest\nOhio State\n5\n\n\n1\nWest\nSan Diego State\n11\n\n\n1\nWest\nTexas A&M\n7\n\n\n1\nWest\nMissouri\n8\n\n\n1\nMidwest\nKansas\n1\n\n\n1\nMidwest\nDuke\n2\n\n\n1\nMidwest\nMichigan State\n3\n\n\n1\nMidwest\nAuburn\n4\n\n\n1\nMidwest\nClemson\n5\n\n\n1\nMidwest\nSyracuse\n11\n\n\n1\nMidwest\nRhode Island\n7\n\n\n1\nMidwest\nSeton Hall\n8\n\n\n2\nSouth\nVirginia\n1\n\n\n2\nSouth\nNevada\n7\n\n\n2\nSouth\nTennessee\n3\n\n\n2\nSouth\nArizona\n4\n\n\n2\nEast\nVillanova\n1\n\n\n2\nEast\nPurdue\n2\n\n\n2\nEast\nSt. Bona\n11\n\n\n2\nEast\nWichita State\n4\n\n\n2\nWest\nXavier\n1\n\n\n2\nWest\nNorth Carolina\n2\n\n\n2\nWest\nMichigan\n3\n\n\n2\nWest\nGonzaga\n4\n\n\n2\nMidwest\nSeton Hall\n8\n\n\n2\nMidwest\nDuke\n2\n\n\n2\nMidwest\nMichigan State\n3\n\n\n2\nMidwest\nClemson\n5\n\n\n3\nSouth\nArizona\n4\n\n\n3\nSouth\nTennessee\n3\n\n\n3\nEast\nVillanova\n1\n\n\n3\nEast\nPurdue\n2\n\n\n3\nWest\nXavier\n1\n\n\n3\nWest\nNorth Carolina\n2\n\n\n3\nMidwest\nClemson\n5\n\n\n3\nMidwest\nDuke\n2\n\n\n4\nSouth\nTennessee\n3\n\n\n4\nEast\nVillanova\n1\n\n\n4\nWest\nXavier\n1\n\n\n4\nMidwest\nDuke\n2\n\n\n5\nSouth\nTennessee\n3\n\n\n5\nMidwest\nDuke\n2\n\n\n6\nSouth\nTennessee\n3\n\n\n\n\n\n\n\n\nWish me luck!"
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-basics",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-basics",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "The Basics",
    "text": "The Basics\nWe have the most historical data on the matchups in the first round because they happen four times each year. Therefore, it makes the most sense to use these data to predict each matchup, especially for future rounds where certain matchups are rare. To do this, we created a range of values for each team in such a way that when one number is drawn from that range and compared against the number generated for its first round opponent, the odds of that team’s number being higher than the opponent’s match the historical probability of that team winning. In other words, every team has a range of values that can be chosen from, and this range overlaps with every other team by a very specific amount.\n\nExample 1 - Overlap of 2\n\n\n\n\n\n\n\n\n\nIf team 1 (favorite) has a range of possible values from 5-10, and team 2 (underdog) has a range of 1-6, randomly choosing a number for each team will result in team 1 winning almost every time. The odds of team 2 winning are equal to the odds of team 1 drawing a 5 and team 2 drawing a 6. This equates to a mathematical “and” problem that can be calculated by multiplying the odds of each event happening. Since each team has 6 numbers to draw from, the odds are 1/6 for each, or 1/36 (2.78%) for both happening at the same time.\n\n\nExample 2 - Overlap of 4\n\n\n\n\n\n\n\n\n\nIf we use these same dimensions, but want to increase the odds of the underdog winning, we could increase the amount of numbers that overlap by one number. Now the favorite chooses from 4-10 and the underdog chooses from 1-7. Here, the odds of the underdog winning increase to 12.24% because they can win if they draw a 5, 6, or 7 because it is possible for the favorite to draw a 4, 5, or 6."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-math",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-math",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "The Math",
    "text": "The Math\nLegend for the following equations:\n\nn = number of overlapping values\nM = Maximum value possible\nR = Size of range\n\nTo make this method work for our purposes, we created an algorithm that defines a range of values for both teams in every first round matchup such that the odds of randomly selecting a value within these ranges matches the historical odds of that team winning. To do this, we determined the formula of the underdog winning (i.e., the number of outcomes in which the underdog would win, divided by the formula for the total number of possible outcomes).\n \\[\n\\begin{aligned}\n\\text{Probability of UnderdogWin} = \\mathbf{\\frac{Underdog Wins}{Total Outcomes}}\n\\end{aligned}\n\\] \n \\[\n\\begin{eqnarray}\n\\mathbf{P} = \\frac{(\\frac{n^2 - n}{2})}{R^2} \\Leftrightarrow \\frac{(\\frac{n^2 - n}{2})}{(\\frac{M+n}{2})^2}\n\\end{eqnarray}\n\\] \nWhen this equation is rearranged to solve for n (i.e., the range of numbers that needed to overlap for a given probability), you get a quadatratic equation that can be plugged into the quadratic formula.\n\\[\n\\begin{align*}\n& \\mathbf{(P-2)n^2 + (2PM + 2)n + P(M^2)} \\text{,   where} \\\\\n\\\\\n& \\mathbf{a = P - 2} \\\\\n& \\mathbf{b = 2PM + 2} \\\\\n& \\mathbf{c = P(M^2)}\n\\end{align*}\n\\]\nOnce n was determined, we needed to set the exact range for each team based on this overlap. To do this, we divided m by 2 and set half of n above and below this mid-point. Teams are assigned a value from this range in round 1, and then carry this value for each round to determine the winner of each matchup. Only in the event of a tie is a new number temporarily generated.\n \\[\n\\begin{align}\n\\mathbf{UnderdogHigh} = \\frac{M+n}{2} \\\\\n\\\\\n\\mathbf{Favored Low} = \\frac{M-n+2}{2}\n\\end{align}\n\\] \nAnd that’s it. Plug this into Matt’s code for each round."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-code",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-code",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "The Code",
    "text": "The Code\n\ncalcRanges &lt;- function(UnderdogWinProb, MaxVal) {\n    a &lt;- UnderdogWinProb - 2\n    b &lt;- (2 * UnderdogWinProb * MaxVal) + 2\n    c &lt;- UnderdogWinProb * (MaxVal^2)\n    \n    quad_minus &lt;- (-b - sqrt(b^2 - 4 * a * c)) / (2 * a)\n    quad_plus &lt;- (-b + sqrt(b^2 - 4 * a * c)) / (2 * a)\n    \n    if (isTRUE(sign(quad_plus) == 1)) {\n        n_overlap &lt;- quad_plus\n    } else {\n        n_overlap &lt;- quad_minus\n    }\n    \n    ## Round n_overlap to nearest even integer\n    n_overlap &lt;- 2 * round(n_overlap/2)\n    \n    UnderdogHigh &lt;- (MaxVal + n_overlap) / 2\n    FavoredLow &lt;- ((MaxVal - n_overlap) / 2) + 1\n\n    cat(paste0(\"UnderdogHigh Value: \", UnderdogHigh))\n    cat(paste0(\"\"),sep='\\n')\n    cat(paste0(\"FavoredLow Value: \", FavoredLow))\n}"
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-simulations",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#the-simulations",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "The Simulations",
    "text": "The Simulations\nWe’ve included two simulations:\n\nImplements historical match-up records of the first-round up to, but not including, the 2018 men’s NCAA tournament\nImplements all first-round match-up records that include this year’s (2018) tournament first round performance\n\nClick on the separate tabs below to see each one in turn:\n\nData Prior to 2018\nHere are the low and the high numbers matched with each seed:\n\nlowNum &lt;- c(4673, 3947, 3036, 2714, 1351, 1216, 1080, 76, 1, 1, 1, 1, 1, 1, 1, 1)\nhighNum &lt;- c(10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n             9925, 8921, 8785, 8650, 7287, 6965, 6054, 5328)\n\nThat correspond to seed like this:\n\n\n\n\n\n\nseed\nlowNum\nhighNum\n\n\n\n\n1\n4673\n10000\n\n\n2\n3947\n10000\n\n\n3\n3036\n10000\n\n\n4\n2714\n10000\n\n\n5\n1351\n10000\n\n\n6\n1216\n10000\n\n\n7\n1080\n10000\n\n\n8\n76\n10000\n\n\n9\n1\n9925\n\n\n10\n1\n8921\n\n\n11\n1\n8785\n\n\n12\n1\n8650\n\n\n13\n1\n7287\n\n\n14\n1\n6965\n\n\n15\n1\n6054\n\n\n16\n1\n5328\n\n\n\n\n\n\n\n\n\nSimulation\nThis method’s simulation code is identical to Matt’s, except that we will exchange the arbitrary sliding range with these newly created ranges based on first round historical probabilities:\n\n# Sets the regions\nregions &lt;- c('South', 'East', 'West', 'Midwest')\n\n# Import the team names and their seeds from 2018\n# These data are being pulled from this website's Github repo\nlink &lt;- 'https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/ncaa-2018-teams-updated.csv'\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata &lt;- GET(link)\nncaa.2018 &lt;- \n  read_csv(content(data, \"raw\")) %&gt;%\n  rename(region = Region, team = Team1, seed = Seed)\n\n# Initializations\niters &lt;- 1000\nresults &lt;- probTableHolder &lt;- vector('list', length = iters)\n\nThis simulation was also ran 1000 times:\n\nfor(i in 1:iters) {\n\n  # Creates whole tournament probability set\n  probTableHolder[[i]] &lt;- probTable &lt;- regions %&gt;% \n    map_df(~tibble(seed = c(1:16),\n                   lowNum = lowNum,\n                   highNum = highNum) %&gt;%\n             group_by(seed) %&gt;% \n             mutate(result = sample(lowNum:highNum, 1)) %&gt;%\n             ungroup()) %&gt;%\n    mutate(region = rep(regions, each = 16)) %&gt;%\n    inner_join(ncaa.2018, by = c('seed', 'region'))\n  \n  # Round 1\n  r1 &lt;- regions %&gt;%\n    map_df(~tibble(match.up = 1:8, t1.seed = 1:8, t2.seed = 16:9) %&gt;%\n             mutate(t1.res =  probTable$result[1:8],\n                    t2.res = probTable$result[16:9],\n                    seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n             mutate(region = rep(regions, each = 8)) %&gt;%\n             inner_join(probTable, by = c('seed', 'region'))\n    \n  # Round 1 results\n  r1.res &lt;- r1 %&gt;% select(match.up, region, team, seed, result)\n  \n  # Round 2\n  r2 &lt;- regions %&gt;%\n    map_df(~tibble(r2.match.up = 1:4,\n                r2.match.up.1 = 1:4,\n                r2.match.up.2 = as.integer(seq(8, 5, -1)),\n                region = .) %&gt;%\n          left_join(r1.res, \n                    by = c('r2.match.up.1' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r1.res,\n                    by = c('r2.match.up.2' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable, by = c('region', 'seed'))\n  \n  # Round 2 results\n  r2.res &lt;- r2 %&gt;% select(r2.match.up, region, team, seed, result)\n  \n  # Round 3\n  r3 &lt;- regions %&gt;%\n    map_df(~tibble(r3.match.up = 1:2, \n                r3.match.up.1 = 1:2, \n                r3.match.up.2 = 4:3,\n                region = .) %&gt;%\n          left_join(r2.res, \n                    by = c('r3.match.up.1' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r2.res,\n                    by = c('r3.match.up.2' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 3 results\n  r3.res &lt;- r3 %&gt;% select(r3.match.up, region, team, seed, result)\n  \n  # Round 4\n  r4 &lt;- regions %&gt;%\n    map_df(~tibble(r4.match.up = 1, \n                   r4.match.up.1 = 1, \n                   r4.match.up.2 = 2,\n                   region = .) %&gt;%\n             inner_join(r3.res, \n                        by = c('r4.match.up.1' = 'r3.match.up', \n                               'region' = 'region')) %&gt;%\n             rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n             inner_join(r3.res, by = c('r4.match.up.2' = 'r3.match.up',\n                                       'region' = 'region')) %&gt;%\n             rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n             mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 4 results\n  r4.res &lt;- r4 %&gt;% select(r4.match.up, region, team, seed, result)\n  \n  # Semifinal\n  semi.final &lt;- tibble(t1.region = c('South', 'East'),\n                       t2.region = c('West', 'Midwest')) %&gt;%\n    left_join(r4.res, by = c('t1.region' = 'region')) %&gt;%\n    rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n    left_join(r4.res, by = c('t2.region' = 'region')) %&gt;%\n    rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  # Semifinal results\n  semi.final.res &lt;- semi.final %&gt;% select(region, team, seed, result)\n  \n  # Final\n  final &lt;- tibble(t1.region = semi.final.res$team[1],\n                  t2.region = semi.final.res$team[2],\n                  t1.team = semi.final.res$team[1],\n                  t2.team = semi.final.res$team[2],\n                  t1.seed = semi.final.res$team[1],\n                  t2.seed = semi.final.res$team[2],\n                  t1.res = semi.final.res$result[1],\n                  t2.res = semi.final.res$result[2]) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  final.res &lt;- final %&gt;% select(region, team, seed, result)\n  \n  results[[i]] &lt;- this.result &lt;- bind_rows(r1.res, r2.res, r3.res, r4.res,\n                                           semi.final.res, final.res, \n                                           .id = 'round')\n  \n  #print(i) # Uncomment to see progress\n}\n\n\n\nData Viz\nThe results of this simulation were saved out and can be imported from this website’s Github data repository like this:\n\n# Loading probability tables from github\nprobTableLink2 &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/probTableHolder-linh-david-before-2018.RData?raw=true'\nsource_data(probTableLink2)\n\n# Loading simulation results from github\nresultsLink2 &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/results-linh-david-before-2018.RData?raw=true'\nsource_data(resultsLink2)\n\nFirst, let’s ensure that the probability table results worked across simulations and regions:\n\n# Converts into a dataframe\nprobTable.long &lt;- probTableHolder %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Calculates summary stats\nprobTable.sum &lt;- probTable.long %&gt;% \n  group_by(seed, region) %&gt;%\n  summarize(m = mean(result),\n            sd = sd(result),\n            n = n(),\n            sem = sd/sqrt(n))\n\n# Probability table plot\nggplot(probTable.long, aes(factor(seed), result, color = region)) +\n  geom_point(position = 'jitter', alpha = 1/3, shape = 1) +\n  geom_pointrange(data = probTable.sum, \n                  aes(factor(seed), m, color = region, \n                      ymin = m - sd, ymax = m + sd),\n                  color = 'black') +\n  geom_line(data = probTable.sum, \n            aes(factor(seed), m, group = 1, color = region), \n            color = 'black', alpha = 1/2) +\n  scale_color_brewer(palette = 'Set2', name = 'Region') +\n  labs(x = 'Seed', y = 'Randomly Chosen Value', caption = 'SD Error Bars') +\n  theme_minimal() +\n  facet_wrap(~region)\n\n\n\n\n\n\n\n\nThis above visualization depicts the potential ranges each seed can draw from using first round historical win probabilities. This drawing occurs at the beginning of each simulation iteration and determines each team’s tournament performance.\nNow let’s examine the results of the simulations.\n\n# Converts to long dataframe\nresults.long &lt;- results %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Win histogram\nggplot(results.long, aes(as.factor(seed), fill = region)) +\n  geom_histogram(stat = 'count') +\n  facet_grid(round~region) +\n  scale_x_discrete(breaks = seq(1, 16, 3)) +\n  labs(x = 'Seed', y = 'Win Frequency', title = 'Number of Wins Per Round') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHigher seeds tend to win more often than lower seeds for each round. In contrast to Matt’s simulation, there is more variability towards seed #7 and this variability becomes more pronounced with each subsequent round.\nNow for fun, let’s see which teams won the tournament most often.\n\n# Round 6 summary information\nresults.long.sum1 &lt;- results.long %&gt;% \n  filter(round == '6') %&gt;% \n  group_by(team, region) %&gt;%\n  summarise(n = n(), seed = unique(seed))\n\n# Winner plot\nggplot(results.long.sum1, aes(reorder(team, -1*n), n, fill = region)) +\n  geom_bar(stat = 'identity') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  labs(x = 'Team', y = 'NCAA Tournament Wins', \n       caption = paste('Simulation iterations:', iters)) +\n  geom_text(aes(label = seed), nudge_y = 4, color = 'grey') + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis simulation has Villanova (#1) with the most wins, while Purdue (#2) comes in second.\nInterestingly, compared to Matt’s simulation, this method allows for more variability for higher seeds to win the tournament:\n\nggplot(results.long.sum1, aes(factor(seed), n)) +\n  geom_bar(stat = 'identity') +\n  labs(x = 'Seed', y = 'NCAA Tournament Wins',\n       caption = paste('Simulation iterations:', iters)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAccording to this method, #5 and #7 seeds won the tournament fewer times than #6 and #8 seeds. Nevertheless, this method results in several upsets winning the tournament, even as high as #9 seeds.\n\n\nData Including 2018\nThis section was added after observing the first round of the 2018 Men’s NCAA Tournament because, for the first time in history, a #16 seed beat a #1 seed. Therefore, we no longer had to guess this probability because it occured once out of 136 games–a probability of 0.0076.\nThe ranges for each team were adjusted to include data from the 2018 tournament:\nHere are the low and the high numbers matched with each seed:\n\nlowNum &lt;- c(4678, 3966, 3077, 2639, 1444, 1182, 1116, 1, 1, 1, 1, 1, 1, 1, 1, 1)\nhighNum &lt;- c(10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000, 10000,\n             8885, 8819, 8557, 7363, 6924, 6035, 5323)\n\nThat correspond to seed like this:\n\n\n\n\n\n\nseed\nlowNum\nhighNum\n\n\n\n\n1\n4678\n10000\n\n\n2\n3966\n10000\n\n\n3\n3077\n10000\n\n\n4\n2639\n10000\n\n\n5\n1444\n10000\n\n\n6\n1182\n10000\n\n\n7\n1116\n10000\n\n\n8\n1\n10000\n\n\n9\n1\n10000\n\n\n10\n1\n8885\n\n\n11\n1\n8819\n\n\n12\n1\n8557\n\n\n13\n1\n7363\n\n\n14\n1\n6924\n\n\n15\n1\n6035\n\n\n16\n1\n5323\n\n\n\n\n\n\n\n\n\n\nSimulation\nThis method’s simulation code is identical to Matt’s, except that we will exchange the arbitrary sliding range with these newly created ranges based on first round historical probabilities:\n\n# Sets the regions\nregions &lt;- c('South', 'East', 'West', 'Midwest')\n\n# Import the team names and their seeds from 2018\n# These data are being pulled from this website's Github repo\nlink &lt;- 'https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/ncaa-2018-teams-updated.csv'\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata &lt;- GET(link)\nncaa.2018 &lt;- \n  read_csv(content(data, \"raw\")) %&gt;%\n  rename(region = Region, team = Team1, seed = Seed)\n\n# Initializations\niters &lt;- 1000\nresults &lt;- probTableHolder &lt;- vector('list', length = iters)\n\nThis simulation was also ran 1000 times:\n\nfor(i in 1:iters) {\n\n  # Creates whole tournament probability set\n  probTableHolder[[i]] &lt;- probTable &lt;- regions %&gt;% \n    map_df(~tibble(seed = c(1:16),\n                   lowNum = lowNum,\n                   highNum = highNum) %&gt;%\n             group_by(seed) %&gt;% \n             mutate(result = sample(lowNum:highNum, 1)) %&gt;%\n             ungroup()) %&gt;%\n    mutate(region = rep(regions, each = 16)) %&gt;%\n    inner_join(ncaa.2018, by = c('seed', 'region'))\n  \n  # Round 1\n  r1 &lt;- regions %&gt;%\n    map_df(~tibble(match.up = 1:8, t1.seed = 1:8, t2.seed = 16:9) %&gt;%\n             mutate(t1.res =  probTable$result[1:8],\n                    t2.res = probTable$result[16:9],\n                    seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n             mutate(region = rep(regions, each = 8)) %&gt;%\n             inner_join(probTable, by = c('seed', 'region'))\n    \n  # Round 1 results\n  r1.res &lt;- r1 %&gt;% select(match.up, region, team, seed, result)\n  \n  # Round 2\n  r2 &lt;- regions %&gt;%\n    map_df(~tibble(r2.match.up = 1:4,\n                r2.match.up.1 = 1:4,\n                r2.match.up.2 = as.integer(seq(8, 5, -1)),\n                region = .) %&gt;%\n          left_join(r1.res, \n                    by = c('r2.match.up.1' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r1.res,\n                    by = c('r2.match.up.2' = 'match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable, by = c('region', 'seed'))\n  \n  # Round 2 results\n  r2.res &lt;- r2 %&gt;% select(r2.match.up, region, team, seed, result)\n  \n  # Round 3\n  r3 &lt;- regions %&gt;%\n    map_df(~tibble(r3.match.up = 1:2, \n                r3.match.up.1 = 1:2, \n                r3.match.up.2 = 4:3,\n                region = .) %&gt;%\n          left_join(r2.res, \n                    by = c('r3.match.up.1' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n          left_join(r2.res,\n                    by = c('r3.match.up.2' = 'r2.match.up', \n                           'region' = 'region')) %&gt;%\n          rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n          mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 3 results\n  r3.res &lt;- r3 %&gt;% select(r3.match.up, region, team, seed, result)\n  \n  # Round 4\n  r4 &lt;- regions %&gt;%\n    map_df(~tibble(r4.match.up = 1, \n                   r4.match.up.1 = 1, \n                   r4.match.up.2 = 2,\n                   region = .) %&gt;%\n             inner_join(r3.res, \n                        by = c('r4.match.up.1' = 'r3.match.up', \n                               'region' = 'region')) %&gt;%\n             rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n             inner_join(r3.res, by = c('r4.match.up.2' = 'r3.match.up',\n                                       'region' = 'region')) %&gt;%\n             rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n             mutate(seed = if_else(t1.res &gt; t2.res, t1.seed, t2.seed))) %&gt;%\n    inner_join(probTable,  by = c('region', 'seed'))\n  \n  # Round 4 results\n  r4.res &lt;- r4 %&gt;% select(r4.match.up, region, team, seed, result)\n  \n  # Semifinal\n  semi.final &lt;- tibble(t1.region = c('South', 'East'),\n                       t2.region = c('West', 'Midwest')) %&gt;%\n    left_join(r4.res, by = c('t1.region' = 'region')) %&gt;%\n    rename(t1.team = team, t1.seed = seed, t1.res = result) %&gt;%\n    left_join(r4.res, by = c('t2.region' = 'region')) %&gt;%\n    rename(t2.team = team, t2.seed = seed, t2.res = result) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  # Semifinal results\n  semi.final.res &lt;- semi.final %&gt;% select(region, team, seed, result)\n  \n  # Final\n  final &lt;- tibble(t1.region = semi.final.res$team[1],\n                  t2.region = semi.final.res$team[2],\n                  t1.team = semi.final.res$team[1],\n                  t2.team = semi.final.res$team[2],\n                  t1.seed = semi.final.res$team[1],\n                  t2.seed = semi.final.res$team[2],\n                  t1.res = semi.final.res$result[1],\n                  t2.res = semi.final.res$result[2]) %&gt;%\n    mutate(team = if_else(t1.res &gt; t2.res, t1.team, t2.team)) %&gt;%\n    inner_join(probTable, by = 'team')\n  \n  final.res &lt;- final %&gt;% select(region, team, seed, result)\n  \n  results[[i]] &lt;- this.result &lt;- bind_rows(r1.res, r2.res, r3.res, r4.res,\n                                           semi.final.res, final.res, \n                                           .id = 'round')\n  \n  #print(i) # Uncomment to see progress\n}\n\n\n\nData Viz\nThe results of this simulation were saved out and can be imported from this website’s Github data repository like this:\n\n# Loading probability tables from github\nprobTableLink2 &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/probTableHolder-linh-david-including-2018.RData?raw=true'\nsource_data(probTableLink2)\n\n# Loading simulation results from github\nresultsLink2 &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/results-linh-david-including-2018.RData?raw=true'\nsource_data(resultsLink2)\n\nFirst, let’s ensure that the probability table results worked across simulations and regions:\n\n# Converts into a dataframe\nprobTable.long &lt;- probTableHolder %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Calculates summary stats\nprobTable.sum &lt;- probTable.long %&gt;% \n  group_by(seed, region) %&gt;%\n  summarize(m = mean(result),\n            sd = sd(result),\n            n = n(),\n            sem = sd/sqrt(n))\n\n# Probability table plot\nggplot(probTable.long, aes(factor(seed), result, color = region)) +\n  geom_point(position = 'jitter', alpha = 1/3, shape = 1) +\n  geom_pointrange(data = probTable.sum, \n                  aes(factor(seed), m, color = region, \n                      ymin = m - sd, ymax = m + sd),\n                  color = 'black') +\n  geom_line(data = probTable.sum, \n            aes(factor(seed), m, group = 1, color = region), \n            color = 'black', alpha = 1/2) +\n  scale_color_brewer(palette = 'Set2', name = 'Region') +\n  labs(x = 'Seed', y = 'Randomly Chosen Value', caption = 'SD Error Bars') +\n  theme_minimal() +\n  facet_wrap(~region)\n\n\n\n\n\n\n\n\nThis above visualization depicts the potential ranges each seed can draw from using first round historical win probabilities. This drawing occurs at the beginning of each simulation iteration and determines each team’s tournament performance.\nNow let’s examine the results of the simulations.\n\n# Converts to long dataframe\nresults.long &lt;- results %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Win histogram\nggplot(results.long, aes(as.factor(seed), fill = region)) +\n  geom_histogram(stat = 'count') +\n  facet_grid(round~region) +\n  scale_x_discrete(breaks = seq(1, 16, 3)) +\n  labs(x = 'Seed', y = 'Win Frequency', title = 'Number of Wins Per Round') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nHigher seeds tend to win more often than lower seeds for each round.\nNow for fun, let’s see which teams won the tournament most often.\n\n# Round 6 summary information\nresults.long.sum1 &lt;- results.long %&gt;% \n  filter(round == '6') %&gt;% \n  group_by(team, region) %&gt;%\n  summarise(n = n(), seed = unique(seed))\n\n# Winner plot\nggplot(results.long.sum1, aes(reorder(team, -1*n), n, fill = region)) +\n  geom_bar(stat = 'identity') +\n  scale_fill_brewer(palette = 'Set2', name = 'Region') +\n  labs(x = 'Team', y = 'NCAA Tournament Wins', \n       caption = paste('Simulation iterations:', iters)) +\n  geom_text(aes(label = seed), nudge_y = 4, color = 'grey') + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nThis simulation that incorporated the results from the first round of the 2018 men’s NCAA tournament had Virginia (#1) winning with Xavier (#1) coming in second in overall tournament wins across all 1000 simulations.\nInterestingly, compared to Matt’s simulation, this method allows for more variability for higher seeds to win the tournament:\n\nggplot(results.long.sum1, aes(factor(seed), n)) +\n  geom_bar(stat = 'identity') +\n  labs(x = 'Seed', y = 'NCAA Tournament Wins',\n       caption = paste('Simulation iterations:', iters)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAccording to this method, overall wins track well with increasing seed. Lower seeds win more than higher seeds. However, upsets are definitely allowed to occur with #11 seeds winning the tournament! This is most likely a consequence of this year’s (2018) tournament containing a variety of upsets, namely a 16th seed over a 1st seed victory."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#simulation-3",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#simulation-3",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "Simulation",
    "text": "Simulation\nWe are then going to loop through our dataset for round 1 and calculate a score for each game played by randomly selecting a decimal number between 0 and 1. After we assign our score we will then refer to the cell of our ProportionsData, where row = seed of team 1 and column = seed of team 2. This cell should give us the proportion (in decimal form) of times that a team with a seed number equal to the seed of team 1 beat a team with a seed number equal to the seed of team 2. If our score variable is less than or equal to the proportion in this cell then team 1 wins, if not then team 2 wins. In other words, the probability of team 1 beating team 2 should be equivalent to the proportion of times that team 1’s seed beat team 2’s seed in real life.\n\n## Round of 64##\nprint(\"Round of 64\")\nfor (match in 1:nrow(Round1Data)){\n  #Select 1 random decimal between 0 and 1.\n  score &lt;- runif(1,0,1)\n  print(score)\n  #Compare our 'score' variable to the real-life proportion data.\n  #If score is less than the proportion of times the seed of team 1 \n  #beat the seeed of team 2 then team 1 wins. Otherwise, team 2 wins.\n  if (score &lt;= ProportionsData[Round1Data$Team1Seed[match],\n                               Round1Data$Team2Seed[match]]) {\n    print(paste(Round1Data$Team1[match], \" beats \", \n                Round1Data$Team2[match], sep = \"\"))\n    Round1Data$Winner[match] &lt;- Round1Data$Team1[match]\n  }else {\n    print(paste(Round1Data$Team2[match], \" beats \", \n                Round1Data$Team1[match], sep = \"\"))\n    Round1Data$Winner[match] &lt;- Round1Data$Team2[match]\n  }\n}\n\nNext, we will use the results from round 1 (we now have a winner column in our Round1Data filled in with the winner of each game) to fill in our data frame for round 2.\n\n# Fill in the data frame for the next round using the results of the \n# previous round.\nfor (winner in 1:nrow(Round2Data)) {\n  team1Winner &lt;- as.character(Round1Data$Winner[winner*2-1])\n  \n  if (Round1Data$Winner[winner*2-1] %in% Round1Data$Team1){\n    team1Seed &lt;- Round1Data$Team1Seed[winner*2-1]\n  }else {\n    team1Seed &lt;- Round1Data$Team2Seed[winner*2-1]\n  }\n  \n  team2Winner &lt;- Round1Data$Winner[winner*2]\n  \n  if (Round1Data$Winner[winner*2] %in% Round1Data$Team1){\n    team2Seed &lt;- Round1Data$Team1Seed[winner*2]\n  }else {\n    team2Seed &lt;- Round1Data$Team2Seed[winner*2]\n  }\n  \n  # I discovered that the team 1 needs to be the lower seed \n  # (i.e. higher seed number).\n  if(team1Seed &gt; team2Seed){\n    Round2Data$Team1[winner] &lt;- team1Winner\n    Round2Data$Team1Seed[winner] &lt;- team1Seed\n    Round2Data$Team2[winner] &lt;- team2Winner\n    Round2Data$Team2Seed[winner] &lt;- team2Seed\n  } else {\n    Round2Data$Team1[winner] &lt;- team2Winner\n    Round2Data$Team1Seed[winner] &lt;- team2Seed\n    Round2Data$Team2[winner] &lt;- team1Winner\n    Round2Data$Team2Seed[winner] &lt;- team1Seed\n  }\n  \n}\n\nAnd finally, we’re simply going to repeat those last two steps for every remaining round in the tournament.\n\n## Round of 32 ##\nprint(\"Round of 32\")\nfor (match in 1:nrow(Round2Data)){\n  score &lt;- runif(1,0,1)\n  print(score)\n  if (score &lt;= ProportionsData[Round2Data$Team1Seed[match],\n                               Round2Data$Team2Seed[match]]) {\n    print(paste(Round2Data$Team1[match], \" beats \", \n                Round2Data$Team2[match], sep = \"\"))\n    Round2Data$Winner[match] &lt;- Round2Data$Team1[match]\n  }else {\n    print(paste(Round2Data$Team2[match], \" beats \", \n                Round2Data$Team1[match], sep = \"\"))\n    Round2Data$Winner[match] &lt;- Round2Data$Team2[match]\n  }\n}\n\n# Fill in the data frame for the next round using the results of the \n# previous round\nfor (winner in 1:nrow(Round3Data)) {\n  team1Winner &lt;- as.character(Round2Data$Winner[winner*2-1])\n  \n  if (Round2Data$Winner[winner*2-1] %in% Round2Data$Team1){\n    team1Seed &lt;- Round2Data$Team1Seed[winner*2-1]\n  }else {\n    team1Seed &lt;- Round2Data$Team2Seed[winner*2-1]\n  }\n  \n  team2Winner &lt;- Round2Data$Winner[winner*2]\n  \n  if (Round2Data$Winner[winner*2] %in% Round2Data$Team1){\n    team2Seed &lt;- Round2Data$Team1Seed[winner*2]\n  }else {\n    team2Seed &lt;- Round2Data$Team2Seed[winner*2]\n  }\n  \n  # I discovered that the team 1 needs to be the lower seed \n  # (i.e. higher seed number)\n  if(team1Seed &gt; team2Seed){\n    Round3Data$Team1[winner] &lt;- team1Winner\n    Round3Data$Team1Seed[winner] &lt;- team1Seed\n    Round3Data$Team2[winner] &lt;- team2Winner\n    Round3Data$Team2Seed[winner] &lt;- team2Seed\n  } else {\n    Round3Data$Team1[winner] &lt;- team2Winner\n    Round3Data$Team1Seed[winner] &lt;- team2Seed\n    Round3Data$Team2[winner] &lt;- team1Winner\n    Round3Data$Team2Seed[winner] &lt;- team1Seed\n  }\n  \n}\n\n\n## Round of 16 ##\nprint(\"Sweet 16\")\nfor (match in 1:nrow(Round3Data)){\n  score &lt;- runif(1,0,1)\n  print(score)\n  if (score &lt;= ProportionsData[Round3Data$Team1Seed[match],\n                               Round3Data$Team2Seed[match]]) {\n    print(paste(Round3Data$Team1[match], \" beats \", \n                Round3Data$Team2[match], sep = \"\"))\n    Round3Data$Winner[match] &lt;- Round3Data$Team1[match]\n  }else {\n    print(paste(Round3Data$Team2[match], \" beats \", \n                Round3Data$Team1[match], sep = \"\"))\n    Round3Data$Winner[match] &lt;- Round3Data$Team2[match]\n  }\n}\n\n# Fill in the data frame for the next round using the results of the \n# previous round\nfor (winner in 1:nrow(Round4Data)) {\n  team1Winner &lt;- as.character(Round3Data$Winner[winner*2-1])\n  \n  if (Round3Data$Winner[winner*2-1] %in% Round3Data$Team1){\n    team1Seed &lt;- Round3Data$Team1Seed[winner*2-1]\n  }else {\n    team1Seed &lt;- Round3Data$Team2Seed[winner*2-1]\n  }\n  \n  team2Winner &lt;- Round3Data$Winner[winner*2]\n  \n  if (Round3Data$Winner[winner*2] %in% Round3Data$Team1){\n    team2Seed &lt;- Round3Data$Team1Seed[winner*2]\n  }else {\n    team2Seed &lt;- Round3Data$Team2Seed[winner*2]\n  }\n  \n  # I discovered that the team 1 needs to be the lower seed \n  # (i.e. higher seed number)\n  if(team1Seed &gt; team2Seed){\n    Round4Data$Team1[winner] &lt;- team1Winner\n    Round4Data$Team1Seed[winner] &lt;- team1Seed\n    Round4Data$Team2[winner] &lt;- team2Winner\n    Round4Data$Team2Seed[winner] &lt;- team2Seed\n  } else {\n    Round4Data$Team1[winner] &lt;- team2Winner\n    Round4Data$Team1Seed[winner] &lt;- team2Seed\n    Round4Data$Team2[winner] &lt;- team1Winner\n    Round4Data$Team2Seed[winner] &lt;- team1Seed\n  }\n  \n}\n\n\n## Round of 8 ##\nprint(\"Elite 8\")\nfor (match in 1:nrow(Round4Data)){\n  score &lt;- runif(1,0,1)\n  print(score)\n  if (score &lt;= ProportionsData[Round4Data$Team1Seed[match],\n                               Round4Data$Team2Seed[match]]) {\n    print(paste(Round4Data$Team1[match], \" beats \", \n                Round4Data$Team2[match], sep = \"\"))\n    Round4Data$Winner[match] &lt;- Round4Data$Team1[match]\n  }else {\n    print(paste(Round4Data$Team2[match], \" beats \", \n                Round4Data$Team1[match], sep = \"\"))\n    Round4Data$Winner[match] &lt;- Round4Data$Team2[match]\n  }\n}\n\n# Fill in the data frame for the next round using the results of the \n# previous round\nfor (winner in 1:nrow(Round5Data)) {\n  team1Winner &lt;- as.character(Round4Data$Winner[winner*2-1])\n  \n  if (Round4Data$Winner[winner*2-1] %in% Round4Data$Team1){\n    team1Seed &lt;- Round4Data$Team1Seed[winner*2-1]\n  }else {\n    team1Seed &lt;- Round4Data$Team2Seed[winner*2-1]\n  }\n  \n  team2Winner &lt;- Round4Data$Winner[winner*2]\n  \n  if (Round4Data$Winner[winner*2] %in% Round4Data$Team1){\n    team2Seed &lt;- Round4Data$Team1Seed[winner*2]\n  }else {\n    team2Seed &lt;- Round4Data$Team2Seed[winner*2]\n  }\n  \n  # I discovered that the team 1 needs to be the lower seed \n  # (i.e. higher seed number)\n  if(team1Seed &gt; team2Seed){\n    Round5Data$Team1[winner] &lt;- team1Winner\n    Round5Data$Team1Seed[winner] &lt;- team1Seed\n    Round5Data$Team2[winner] &lt;- team2Winner\n    Round5Data$Team2Seed[winner] &lt;- team2Seed\n  } else {\n    Round5Data$Team1[winner] &lt;- team2Winner\n    Round5Data$Team1Seed[winner] &lt;- team2Seed\n    Round5Data$Team2[winner] &lt;- team1Winner\n    Round5Data$Team2Seed[winner] &lt;- team1Seed\n  }\n  \n}\n\n\n## Semi-Finals ##\nprint(\"Final Four\")\nfor (match in 1:nrow(Round5Data)){\n  score &lt;- runif(1,0,1)\n  print(score)\n  if (score &lt;= ProportionsData[Round5Data$Team1Seed[match],\n                               Round5Data$Team2Seed[match]]) {\n    print(paste(Round5Data$Team1[match], \" beats \", \n                Round5Data$Team2[match], sep = \"\"))\n    Round5Data$Winner[match] &lt;- Round5Data$Team1[match]\n  }else {\n    print(paste(Round5Data$Team2[match], \" beats \", \n                Round5Data$Team1[match], sep = \"\"))\n    Round5Data$Winner[match] &lt;- Round5Data$Team2[match]\n  }\n}\n\n# Fill in the data frame for the next round using the results of the \n# previous round\nfor (winner in 1:nrow(Round6Data)) {\n  team1Winner &lt;- as.character(Round5Data$Winner[winner*2-1])\n  \n  if (Round5Data$Winner[winner*2-1] %in% Round5Data$Team1){\n    team1Seed &lt;- Round5Data$Team1Seed[winner*2-1]\n  }else {\n    team1Seed &lt;- Round5Data$Team2Seed[winner*2-1]\n  }\n  \n  team2Winner &lt;- Round5Data$Winner[winner*2]\n  \n  if (Round5Data$Winner[winner*2] %in% Round5Data$Team1){\n    team2Seed &lt;- Round5Data$Team1Seed[winner*2]\n  }else {\n    team2Seed &lt;- Round5Data$Team2Seed[winner*2]\n  }\n  \n  # I discovered that the team 1 needs to be the lower seed \n  # (i.e. higher seed number)\n  if(team1Seed &gt; team2Seed){\n    Round6Data$Team1[winner] &lt;- team1Winner\n    Round6Data$Team1Seed[winner] &lt;- team1Seed\n    Round6Data$Team2[winner] &lt;- team2Winner\n    Round6Data$Team2Seed[winner] &lt;- team2Seed\n  } else {\n    Round6Data$Team1[winner] &lt;- team2Winner\n    Round6Data$Team1Seed[winner] &lt;- team2Seed\n    Round6Data$Team2[winner] &lt;- team1Winner\n    Round6Data$Team2Seed[winner] &lt;- team1Seed\n  }\n  \n}\n\n\n## Finals ##\nprint(\"National Final\")\nscore &lt;- runif(1,0,1)\nprint(score)\nif (score &lt;= ProportionsData[Round6Data$Team1Seed[1],\n                             Round6Data$Team2Seed[1]]) {\n  print(paste(Round6Data$Team1[1],  \" beats \", \n              Round6Data$Team2[1], \" in the Final Round!\", sep = ''))\n  Round6Data$Winner[1] &lt;- Round6Data$Team1[1]\n}else {\n  print(paste(Round6Data$Team2[1],  \" beats \", \n              Round6Data$Team1[1], \" in the Final Round!\", sep = ''))\n  Round6Data$Winner[1] &lt;- Round6Data$Team2[1]\n}\n\nThe above steps were repeated 1000 times to simulate performance."
  },
  {
    "objectID": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#data-viz-3",
    "href": "posts/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R/post-Simulating-2018-NCAA-March-Madness-Tournament-Performance-in-R.html#data-viz-3",
    "title": "Simulating 2018 NCAA March Madness Tournament Performance in R",
    "section": "Data Viz",
    "text": "Data Viz\nThe results from this simulation were saved into a variable named resultsHolder that can be retrieved from this website’s Github data repository like this:\n\nnickResLink &lt;- 'https://github.com/mkmiecik14/mkmiecik14.github.io/blob/master/data/resultsHolder.RData?raw=true'\nsource_data(nickResLink)\n\nLet’s see what teams won the tournament most often:\n\n# Converting list to long data frame\nresultsLong &lt;- resultsHolder %&gt;% map_df(bind_rows, .id = 'iter')\n\n# Summary information for results\nresultsLongSum &lt;- resultsLong %&gt;% \n  filter(round == 6) %&gt;%\n  group_by(Winner) %&gt;% \n  summarise(n = n(), seed = unique(WinnerSeed))\n\n# Tournament champion histogram\nggplot(resultsLongSum, aes(reorder(Winner, -1*n), n)) +\n  geom_bar(stat = 'identity') +\n  labs(x = 'Tournament Champion', \n       y = 'Wins',\n       caption = '\"Play in #\" teams were not available at time of simulation') +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\nAnd it looks like this simulation predicted Xavier to win most often!"
  },
  {
    "objectID": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html",
    "href": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html",
    "title": "Stop Using Excel to Preprocess E-Prime Data",
    "section": "",
    "text": "Excel is a great tool for processing data. Whether it’s calculating a quick average, standard deviation, or t-test, Excel is fast and simple to learn and use. In fact, most of my colleagues use Excel to preprocess their data from experiments programmed in E-Prime, a software tool for running psychological experiments. However, preprocessing your E-Prime data in Excel will:\nI’ll demonstrate a different method on how to preprocess your E-Prime data with R and the R package dplyr (Wickham & Francois, 2016) using a small data set from a real experiment of mine. This method is much faster, efficient, and saves soooooo much time when it comes to reviewer #2’s requests.\nJust a few notes about the experiment: participants completed a task that required them to reason about items on the screen and press #1 or #2 on the keyboard. There were 3 conditions randomly distributed over 4 blocks with 24 trials per block (8 trials per condition). I was interested in the participants’ accuracy and reaction time for correct solutions across the conditions and blocks. Therefore, each participant has only 1 E-Prime file."
  },
  {
    "objectID": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html#concatenating-with-e-merge",
    "href": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html#concatenating-with-e-merge",
    "title": "Stop Using Excel to Preprocess E-Prime Data",
    "section": "Concatenating with E-Merge",
    "text": "Concatenating with E-Merge\n\nOpen E-Merge and navigate to the directory where all the E-DataAid files are stored using the Folder Tree\nSelect all the E-DataAid files and click Merge…\nChoose the appropriate option and click Next &gt;:\n\nStandard Merge if all your files are in one directory (option chosen in this tutorial)\nRecursive Merge if all your files are stored in folders within folders\n\nName your file and save it as a *.emrg2 file (the default)\n\nAs long as your E-DataAids are consistent with each other, they should seamlessly merge. Next, we have to convert this to a format R and other programs can read using E-DataAid:"
  },
  {
    "objectID": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html#converting-merged-e-dataaid",
    "href": "posts/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data/post-Stop-Using-Excel-to-Preprocess-E-Prime-Data.html#converting-merged-e-dataaid",
    "title": "Stop Using Excel to Preprocess E-Prime Data",
    "section": "Converting Merged E-DataAid",
    "text": "Converting Merged E-DataAid\n\nDouble click on the *.emrg2 file that you just created\nGo to File &gt; Export\nEnsure the “Export to:” option is: StatView and SPSS\nEnsure that the Unicode box at the bottom is unchecked\nClick OK and name/save the file as a *.txt (the default)\n\nNow these data are in one central file and prepared for R. Next, let’s import into R:"
  },
  {
    "objectID": "posts/post-The-Last-Decade-NHLs-Best-and-Worst-Power-Play-Teams/post-The-Last-Decade-NHLs-Best-and-Worst-Power-Play-Teams.html",
    "href": "posts/post-The-Last-Decade-NHLs-Best-and-Worst-Power-Play-Teams/post-The-Last-Decade-NHLs-Best-and-Worst-Power-Play-Teams.html",
    "title": "The Last Decade: NHL’s Best and Worst Power Play Teams",
    "section": "",
    "text": "Important\n\n\n\nNote: This blog post presents a model of powerplay goals in the NHL over 10 years. However, this model is inaccurate. I encourage readers to read this post and then proceed to my follow-up blog post where I explain the source of these potential errors and provide a more accurate and informative model.\n\n\nSo you are watching your favorite NHL team and they finally draw a powerplay. Awesome! The ice opens up with the missing player and it shouldn’t be too hard to score…right? As a previous water polo player, I know what it is like to endure the wrath of a furious coach after a missed man advantage opportunity. In my experience, coaches semed convinced that the man-up should lead to a goal 100% of the time. But how important is this seemingly advantageous situation in the NHL?\nAs a first pass, I analyzed the NHL regular-season over the past ten years (2007-2017) on the standard 5 vs. 4 power play. These data were retrieved via Corsica shortly after the end of the 2017 regular season. The following will detail the analyses in R with steps along the way utilizing the wonderful R packages of ggplot2 (Wickham, 2009) and dplyr (Wickham & Francois, 2016).\nFirst, I loaded the data and made sure to combine the entries of the Atlanta Thrashers and the Winnipeg Jets, as the Thrashers moved to Winnipeg after the 2011 season. Therefore, the Thrashers and the Jets are considered the same team in this analysis.\n\n# Load packages\nlibrary(tidyverse); library(plotly); library(httr)\n\n# Step 1: Load data\nlink &lt;- \"https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/corsicaData_5v4.csv\"\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata &lt;- GET(link)\ndata5v4 &lt;- read_csv(content(data, \"raw\"))\n\n# Step 2: Clean up the data a little bit\n# ATL moved to WPG for 2011-2012 season\ndata5v4$Team &lt;- plyr::revalue(data5v4$Team, c(ATL = 'ATL.WPG', WPG = 'ATL.WPG'))\n\nNext, I plotted the global trend of total regular-season power play goals for each of the 30 NHL teams over the last decade.\n\n# Step 2a: Look at power play goals 5v4 for every team in the past \nggplot(data5v4, aes(factor(Season), GF, group = 1)) +\n  geom_line(aes(group = factor(Team)), alpha = 1/2) + \n  geom_line(stat = 'summary', fun.y = 'mean', colour = 'red') +\n  xlab('Season') +\n  ylab('Total Season Power Play Goals (5v4)') +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1))\n\n\n\n\n\n\n\n\nIn this above plot, each team is a grey line, while the average total power play goals for each season is plotted in the red trend line. However, we see that this doesn’t control for the shortened lockout season that occurred in 2012-2013. Let’s control for this by dividing the total power play goals (GF) by the total games played (GP) each season.\n\n# Step 2b: Control for games played due to partial lockout season (2012-2013)\nggplot(data5v4, aes(factor(Season), GF/GP, group = 1)) +\n  geom_line(aes(group = factor(Team)), alpha = 1/2) + \n  geom_line(stat = 'summary', fun.y = 'mean', colour = 'red') +\n  xlab('Season') +\n  ylab('Total Season Power Play Goals/Games Played (5v4)') +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1))\n\n\n\n\n\n\n\n\nWe now see each team’s power play performance controlled for the games played that season. What is interesting about this above plot is the seemingly decrease in power play goals across time. Let’s model this with a linear regression that predicts the total season power play goals controlled for games played as a function of time/season.\n\n# Step 3a: Linearly model this negative relationship between \n# power play goals/game and season\nmod &lt;- lm((GF/GP) ~ factor(Season), data = data5v4)\nsummary(mod) # Very significant negative trend over time\n\n&gt; \n&gt; Call:\n&gt; lm(formula = (GF/GP) ~ factor(Season), data = data5v4)\n&gt; \n&gt; Residuals:\n&gt;      Min       1Q   Median       3Q      Max \n&gt; -0.24268 -0.06463 -0.00878  0.05407  0.33333 \n&gt; \n&gt; Coefficients:\n&gt;                        Estimate Std. Error t value Pr(&gt;|t|)    \n&gt; (Intercept)             0.64176    0.01890  33.953  &lt; 2e-16 ***\n&gt; factor(Season)20082009  0.03628    0.02673   1.357 0.175726    \n&gt; factor(Season)20092010 -0.04644    0.02673  -1.737 0.083394 .  \n&gt; factor(Season)20102011 -0.09420    0.02673  -3.524 0.000494 ***\n&gt; factor(Season)20112012 -0.13776    0.02673  -5.154 4.73e-07 ***\n&gt; factor(Season)20122013 -0.12093    0.02673  -4.524 8.86e-06 ***\n&gt; factor(Season)20132014 -0.11779    0.02673  -4.407 1.48e-05 ***\n&gt; factor(Season)20142015 -0.12875    0.02673  -4.817 2.36e-06 ***\n&gt; factor(Season)20152016 -0.12591    0.02673  -4.710 3.84e-06 ***\n&gt; factor(Season)20162017 -0.12555    0.02673  -4.697 4.08e-06 ***\n&gt; ---\n&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n&gt; \n&gt; Residual standard error: 0.1035 on 290 degrees of freedom\n&gt; Multiple R-squared:  0.2464,  Adjusted R-squared:  0.223 \n&gt; F-statistic: 10.54 on 9 and 290 DF,  p-value: 4.082e-14\n\n\nAs we can see from the regression model and coefficients, power play goals are decreasing with each successive season. Here is a plot of this negative trend in blue.\n\n# Step 3b: Plot this linear trend over time\nggplot(data5v4, aes(factor(Season), GF/GP, group = 1)) +\n  geom_line(aes(group = factor(Team)), alpha = 1/2) + \n  geom_line(stat = 'summary', fun.y = 'mean', colour = 'red') +\n  xlab('Season') +\n  ylab('Total Season Power Play Goals/Games Played (5v4)') +\n  stat_smooth(method = 'lm', col = 'blue', se = F) +\n  theme(axis.text.x = element_text(angle = 15, hjust = 1))\n\n\n\n\n\n\n\n\nI then wondered which teams, despite this decrease in power play goals over time, had the best or worst power play across the last decade? To remove this negative trend over time, I extracted the residuals from the model. The residuals are simply the distance from the blue line (the predicted power play goals over time) to each team’s grey line (total power play goals/games played for each year).\n\n# Step 4a: Remove this linear trend and focus on residuals\n# This will examine teams' performance after controlling for this decrease in\n# powerplay goals in the last decade\ndata5v4$rel_ppg &lt;- resid(mod)\n\nNext, I averaged these residuals for each individual team over ten years and ran one-sample t-tests to see the teams that significantly deviated from the average.\n\n# Step 4b: Average the residuals for each team over the past decade. Then,\n# perform a one-sample t-test (i.e., from zero) to see teams that were above\n# or below the average power play goals/game despite the decrease in \n# power play goals in the last decade  \nbestPP &lt;- data5v4 %&gt;% \n  group_by(Team) %&gt;% \n  summarise_at('rel_ppg', \n               funs(mean = mean,\n                    sd = sd,\n                    sem = sd(.)/sqrt(n()),\n                    tpval = t.test(.)$p.value)) %&gt;%\n  mutate(sig = tpval &lt; .05)\nprint(bestPP)\n\n&gt; # A tibble: 30 × 6\n&gt;    Team         mean     sd    sem   tpval sig  \n&gt;    &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;lgl&gt;\n&gt;  1 ANA      0.0296   0.0814 0.0257 0.280   FALSE\n&gt;  2 ARI     -0.0556   0.0777 0.0246 0.0497  TRUE \n&gt;  3 ATL.WPG -0.0456   0.0735 0.0232 0.0811  FALSE\n&gt;  4 BOS     -0.0250   0.102  0.0323 0.459   FALSE\n&gt;  5 BUF     -0.0230   0.0987 0.0312 0.479   FALSE\n&gt;  6 CAR     -0.0102   0.0896 0.0283 0.726   FALSE\n&gt;  7 CBJ     -0.0532   0.0891 0.0282 0.0919  FALSE\n&gt;  8 CGY     -0.0112   0.0604 0.0191 0.573   FALSE\n&gt;  9 CHI     -0.000760 0.0836 0.0264 0.978   FALSE\n&gt; 10 COL     -0.0783   0.0669 0.0212 0.00493 TRUE \n&gt; # … with 20 more rows\n\n\nFinally, I plotted these results. Teams that have positive averages that are different from zero are teams that are above average in the power play, despite the decrease in power play goals scored in the last decade. The teams with negative residual averages are the opposite and are below average in the power play. Error bars represent the standard error of the mean.\n\n# Step 4c: Plot the average residuals, their standard error of the mean,\n# and color based on if significantly different from 0 (i.e., average PPG/Game)\n\n# -- To get y.axis bold conditional -- # \nbestPPSort &lt;- arrange(bestPP, mean)                       \naxisFace &lt;- ifelse(bestPPSort$sig == T,'bold', 'plain')\n# -- To get y.axis bold conditional -- #\n  \n# -- To get shading conditional -- #\nabove &lt;- bestPP %&gt;% filter(mean &gt; 0, sig == TRUE) %&gt;% arrange(mean)\naverage &lt;- bestPP %&gt;% filter(sig == FALSE)\nbelow &lt;- bestPP %&gt;% filter(mean &lt; 0, sig == TRUE) %&gt;% arrange(mean)\n# -- To get shading conditional -- #\n  \nggplot(bestPP, aes(mean, reorder(Team, mean), colour = sig)) +\n  geom_errorbarh(aes(xmin = mean - sem, xmax = mean + sem)) +\n  geom_point(size = 2) +\n  scale_x_continuous(limits = (c(-.2, .2))) +\n  scale_color_manual(\n    values = c('grey', 'black'),\n    guide = guide_legend(\n      reverse = T, title = 'Above/Below Average', title.position = 'top')\n    ) +\n  labs(\n    x = 'Mean Power Play Goals/Games Played (Residuals)',\n    y = 'NHL Team',\n    title = 'NHL 5v4 Power Play Team Performance 2007-2017',\n    caption = 'Error bars are SEM'\n    ) +\n  theme_minimal() +\n  theme(\n    axis.text.y = element_text(face = axisFace),\n    plot.title = element_text(hjust = 0.5),\n    legend.position = 'bottom',\n    legend.key = element_rect(colour = \"transparent\", fill = \"white\"),\n    legend.key.width = unit(1, 'in'),\n    legend.key.height = unit(.5, 'in'),\n    legend.title.align = .5,\n    axis.title.x = element_text(vjust = -1)\n    ) +\n  annotate(\n    'rect', \n     xmin = -Inf, \n     xmax = Inf, \n     ymax = above$Team[nrow(above)],\n     ymin = above$Team[1], \n     fill = '#33a02c', \n     alpha = 1/3\n    ) +\n  annotate(\n    'rect', \n     xmin = -Inf, \n     xmax = Inf, \n     ymax = below$Team[nrow(below)],\n     ymin = below$Team[1], \n     fill = '#e31a1c', \n     alpha = 1/3\n    ) +\n  annotate('text', x = -.1, y = above$Team[3], label = 'Best Teams') +\n  annotate('text', x = .1, y = below$Team[4], label = 'Worst Teams')\n\n\n\n\n\n\n\n\nAs we can see from this plot, the top power play teams in the past decade have been the Washington Capitals, Philadelphia Flyers, Detroit Red Wings, San Jose Sharks, and Pittsburgh Penguins. Meanwhile, the worst power play teams have been the New York Islanders, Phoenix Coyotes, Nashville Predators, New York Rangers, New Jersey Devils, Colorado Avalanche, and Florida Panthers.\nInterestingly, only 5 different teams have won the Stanley Cup in this ten year span. Of the top power play teams from this analysis, only the Red Wings and the Penguins have won the Stanley Cup, while the Blackhawks, the Kings, the Bruins all had average regular-season power plays. It is unclear how regular-season power play performance relates to Stanley Cup wins; however, this analysis demonstrated 3 key findings for this last decade of hockey (2007 – 2017):\n\nLeague-wide total power play goals per season have decreased\nTop power play teams are:\n\n\nWashington Capitals\nPhiladelphia Flyers\nDetroit Red Wings\nSan Jose Sharks\nPittsburgh Penguins\n\n\nWorst power play teams are:\n\n\nNew York Islanders\nPhoenix Coyotes\nNashville Predators\nNew York Rangers\nNew Jersey Devils\nColorado Avalanche\nFlorida Panthers\n\n\nAcknowledgments\nThis analysis was inspired by Part III of ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham.\n\n\nReferences:\nWickham, H. 2009. ggplot2: Elegant Graphics for Data Analysis. Book. Springer-Verlag New York. http://ggplot2.org.\nWickham, H., and R. Francois. 2016. dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr."
  },
  {
    "objectID": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#determining-the-parallel-port",
    "href": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#determining-the-parallel-port",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Determining the parallel port",
    "text": "Determining the parallel port\nPST provides very good instructions for determining the port address that will be used to pass your triggers that depend on your requirements (see here). I’ll briefly demonstrate with the parallel port on my PC.\nNavigate to your PCs Control Panel &gt; Hardware and Sound &gt; Device Manager &gt; Ports (COM & LPT) &gt; Printer Port (LPT) &gt; right click and select Properties &gt; Resources tab. The address on my computer is 0378, and that translates to &H378. I will use this parallel port address in E-Prime to send triggers to the EEG computer."
  },
  {
    "objectID": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#onsetoffset-signaling",
    "href": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#onsetoffset-signaling",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Onset/Offset signaling",
    "text": "Onset/Offset signaling\nThis type of signaling is the most appropriate to trigger the onset of an event (e.g., fixation cross or stimulus). I’ll demonstrate these using the Onset/Offset inline codes mixed with List Attributes. First, let’s take a look at “triggerList”:\n\n\n\n\n\n\nID\nNested\nProcedure\nPress\nstimTrig\nfixationTrig\n\n\n\n\n1\n\ntriggerProc\n1\n11\n9\n\n\n2\n\ntriggerProc\n1\n11\n9\n\n\n3\n\ntriggerProc\n2\n22\n9\n\n\n4\n\ntriggerProc\n2\n22\n9\n\n\n5\n\ntriggerProc\n1\n11\n9\n\n\n6\n\ntriggerProc\n2\n22\n9\n\n\n7\n\ntriggerProc\n2\n22\n9\n\n\n8\n\ntriggerProc\n2\n22\n9\n\n\n9\n\ntriggerProc\n1\n11\n9\n\n\n10\n\ntriggerProc\n2\n22\n9\n\n\n\n\n\n\n\n\nIn this experiment, 9 will denote the onset of a fixation cross (fixationTrig), 11 will denote the onset of the “Press 1” condition stimulus, and 22 will denote the onset of the “Press 2” condition stimulus. Accuracy will be determined via the “Press” column.\nThe basic concept is that a trigger will be passed at the onset of a stimulus, but must be reset to zero to toggle between triggers.\nTo pass the fixation crosss trigger (9), the following inline code should be placed before the fixation cross TextDisplay (in this example “fixTrigs”):\n' Triggers for fixation onset\nfixation.OnsetSignalEnabled = True\nfixation.OnsetSignalPort = &H378\nfixation.OnsetSignalData = c.GetAttrib(\"fixationTrig\")\n\n' Triggers for fixation offset\nfixation.OffsetSignalEnabled = True\nfixation.OffsetSignalPort = &H378\nfixation.OffsetSignalData = 0\nTo pass the triggers conditional on the correct stimulus comdition (11 or 22), the following inline code should be placed before the stimulus (in this example “stimTrigs”):\n' Triggers for stim onset\nstim.OnsetSignalEnabled = True\nstim.OnsetSignalPort = &H378\nstim.OnsetSignalData = c.GetAttrib(\"stimTrig\")\n\n' Triggers for stim offset\nstim.OffsetSignalEnabled = True\nstim.OffsetSignalPort = &H378\nstim.OffsetSignalData = 0"
  },
  {
    "objectID": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#using-writeport",
    "href": "posts/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime/post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#using-writeport",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Using WritePort",
    "text": "Using WritePort\nPassing triggers for participant responses conditional on their accuracy is a little tricky. In other words, I want to pass a trigger, say with a value of “1”, if the participant gets the trial correct, and “2” if he or she answers incorrectly. Additionally, I do not want the response to terminate the trial due to potential EEG artifacts from the screen changing so close to the response.\nThanks go to David McFarlane for a solution to this issue (see here). To satisfy the above needs of my experiment, I need to set the following properties of my stim TextDisplay:\n\nDuration: 3000ms (I’m limiting participants to 3 seconds)\nInput Device: Keyboard\nPreRelease: (same as duration) – this is very important (see here)\nCorrect: [Press] – will automatically determine accuracy\nTime Limit: (same as duration)\nEnd Action: (none) – this will not terminate the stimulus\n\n\n\n\nstim TextDisplay configuration.\n\n\nThe basic concept for this is that as soon as “stim” is presented on screen, we must program E-Prime to constantly look for a response. If one is detected, send triggers as soon as possible given the trial accuracy (i.e., 1 = correct, 2 = incorrect).\nTo do this I slightly modified David McFarlane’s script and placed it as an inline code following stim called “respTrigs”:\nDo While stim.InputMasks.IsPending()\n  Sleep 2 ' Will check every 2ms\nLoop\n\nIf Len(stim.RESP) &gt; 0 Then\n  \n  If stim.Acc = 1 Then\n  \n    Debug.Print \"Got it right!\"\n    WritePort &H378, cor ' Signals correct\n  \n  Else\n    \n    Debug.Print \"Got it wrong!\"\n    WritePort &H378, icor ' Signals incorrect\n    \n  End If\n  \nElse\n\n  Debug.Print \"No Response!\"\n  WritePort &H378, nrp ' Signals no response\n  \nEnd If\n\nSleep 10 'Takes a break before resetting port\n\n' Reserts port\nWritePort &H378, 0\nAlternatively, if there are no events that require triggers following “stim”, “Sleep 10” could be removed and “WritePort &H378, 0” could be placed as the first line of code in the “fixTrigs” inline code.\nThe variables cor, icor, and nrp were set as global variables to be easily modified and stay consitent across the experiment:\nDim cor   As Integer\nDim icor  As Integer\nDim nrp   As Integer\nAnd then defined at the beginning of the experiment in the inline code “defTrigs”:\ncor   = 1\nicor  = 2\nnrp   = 3"
  },
  {
    "objectID": "posts/post-Writing-a-Literature-Review/post-Writing-a-Literature-Review.html",
    "href": "posts/post-Writing-a-Literature-Review/post-Writing-a-Literature-Review.html",
    "title": "Writing a Literature Review",
    "section": "",
    "text": "“Writing a literature review was so fun and easy!” - No one ever\n\nI recently completed my first comprehensive literature review as part of a graduate school milestone project. Before starting, I sought guidance from online blogs and university workshops held by our library. The tips I learned from these resources were extremely helpful, but not everything worked for me. This is simply a list of things that worked/did not work for me during the reading and writing process of my literature review. Feel free to comment below on what worked & and what didn’t work for you!\n\nStart!\nIf you are anything like me, you are reading this because you are apprehensive about starting the daunting task of a literature review, and understandably so. I wanted to be as efficient as possible, so I scoured websites, attended workshops, and signed up for writing groups. After you are done reading this, take the first step and START! Although it’s great to seek advice and guidance for something like this, everyone has their own unique experience when writing. Starting is the best way to learn what works and does not work for you.\n\n\nTake digital notes\nI learned about 2 months into the process that physically writing notes into legal pads about the papers I was reading was inefficient and simply not working for me. You always hear the concept that “Writing notes on paper helps you remember it later on.” For me, not true. I type much faster than I write, so writing notes was painfully slow for me and I would often write illegibly. So I began taking notes in a single Word document, often limiting myself to one page of notes per paper. Importantly, the first line of every page had the authors, year, and title of the paper. Therefore, if I needed to find my notes on a particular paper, I could simply type COMMAND + F (I’m a mac user), and search the paper using the title. This is much faster than rifling through dozens of legal pad pages and paper notes. Also, searching for terms used in papers, such as keywords, was much easier in electronic format.\nWhen doing a literature review, COMMAND + F is your friend!\n\n\nHave a system\nDevelop a system and be ready to change it if you notice something not working for you. Briefly, here’s the system I used:\nI would find a paper that I wanted to read and enter its information into an Excel document. The spreadsheet looked something like this\n\n\n\n\n\n\nAuthor\nYear\nTitle\nType\nCategory\nSubCategory\nSummary\n\n\n\n\nTolchok & Krovvy\n1962\nMilkbars and Ultra-Violence\nreview\nCategory A\nCategory A-1\nThis article is about...\n\n\nHoagey, Ray & Miranda\n2017\nThe Woes of Graduate Life\nexperimental\nCategory A\nCategory A-2\nThis article is about...\n\n\nMulligan & Belmont\n2016\nAhead of their Time\nmeta-analysis\nCategory B\nCategory B-1\nThis article is about...\n\n\n\n\n\n\n\n\nI then would import the paper into EndNote and ensure the citation was correct. I then would read the paper and take notes in a Word document, with roughly 1 Word doc page per paper.\n\n\n\n\n\n\nTip\n\n\n\nTip: pressing COMMAND + SHIFT + 4 on a Mac will allow you make a selection on your screen for a screen shot. Adding screenshots of helpful figures or tables to your Word document emphasizes the important results and helps jog your memory.\n\n\nI learned that a note taking system is a critical component of a lit review. Being able to find your notes quickly, as well as other papers that are similar, is made very easy with electronic notes and an efficient system.\nI took notes using this format in my Word document:\n\nAuthor(s) - date - title of paper\nKeywords of the paper (tip: create your own if they are not in the paper already)\nIntro - things I learned\nMethods - important info to understand the results\nResults/Discussion - main findings and bottom line\n\nUsing this format made it very easy to COMMAND + F the paper I was looking for and other similar papers by searching keywords\n“Wait, why use the Excel document if you are taking notes in a Word document?” The Excel sheet is a great way to organize your materials globally by keywords, category, authors, and year of publication. Sorting your articles based on category and year helps group papers that are similar in topic and provides a historical perspective on a body of work. This may elucidate recurrent themes in your literature, such as debates across time, or provide unique insights into your review. I also used the Excel sheet as a way to mark down papers that I wanted to read, but wasn’t ready to find them yet. I simply entered TO READ in the cells to remind me.\n\n\nUse a citation manager\nAs mentioned in the “Have a System” section, I used EndNote to manage my citations. Citation managers are software programs that organize your papers/citations for easy citing while writing. Some other common reference managers are Mendeley and RefWorks.\nI strongly advise against doing citations “by hand.” That is, manually typing or copying from online citation generators into your reference page. Citation managers streamline this process by building your in-text citations and reference pages for you. An added bonus is that you can switch citation styles (e.g., APA to MLA) very easily. I don’t even want to think about the pain of doing that by hand!\nCitation managers will also ensure that your in-line and reference page citations are correctly furnished and ready for submission. Of course, I recommend checking your work and editing style guides as necessary to be in accordance with journal requirements. Not having to worry about citations is a HUGE advantage when writing a literature review. I encourage those that are apprehensive about making the switch to using citation managers to do it now. Putting the time in now to learn one of these programs will save you time and stress later!\n\n\nRead/Write everyday\nWriting a literature review is similar to preparing for a marathon. Training for the big race requires a slow and steady approach of practice everyday that gradually builds until race day. When reading and writing for a lit review, it is best to take a similar approach and try reading and/or writing every day for a certain amount of time (adjusted to your deadline, of course).\n\n\n\n\n\n\nTip\n\n\n\nTry to write every day and develop a rhythm.\n\n\nInitially, I began with a 1 hour per day approach. It was quite tough at the beginning and I would begin to fatigue fairly quickly, often not even finishing a whole paper or article. I’m also a slow reader and get distracted easily. However, as time went on, I began building my lit review endurance and I noticed my ability to read multiple papers and write more sentences increased. By the end, I was well exceeding the 1 hour per day benchmark I had set for myself. I think this was because I learned a lot about the literature, which in turn made paper reading faster, which in turn led me to organize and write more effectively, which in turn led to excitement that the end was near! Also, my deadline was approaching :)\n\n\nLimit distractions\nNo matter if you follow my advice about reading/writing everyday, not limiting distractions will prolong the process. In my experience, 2 hours of work with distractions was roughly equivalent to 1 hour of work with no distractions.\nI noticed a huge increase in the amount of work I was able to get done in 1 hour when I turned off notifications from my electronic devices and worked in an enviornment when no one could bother me. Turning my phone on silent helped, but turning off badge & banner e-mail notifications on my Mac REALLY helped. Try it out!\n\n\nFocus on the main idea\nI really wish someone would have told this to me when first starting. At the beginning, I began with review articles to introduce myself into the field and to see what has been already reviewed (this is something I highly recommend). I certainly didn’t want to reinvent the wheel, but contribute new things that I thought were important to the field.\nHowever, some of the reviews and book chapters I read were very well written and covered a wide range of topics that were important, but on the periphery of what I wanted to study. So I thought, “Well, if I’m going to do a literature review, it’s gotta be big and comprehensive. I should include this background information to make sure I have all my bases covered.” This was defintely not the way to go.\nI began reading too off topic and straying from my main ideas. This ended up making a few of my early sections seem unorganized and less connected. It wasn’t until I really focused in on my topic that I began gaining confidence and feeling less overwhelmed. Therefore, focusing in on my topic helped me, while worrying too much about the background work of others detracted from the paper’s coherence. Start focused, then zoom out if needed.\n\n\nEnjoy the process\nI know this sounds crazy, but look on the bright side! You are committing all this time and energy into learning how our understanding of a specific topic has evolved over time. You are going to come out confident and well-versed in an area of research—an expert of sorts. Realize that it takes time to get there and be patient with the process.\nHopefully this was helpful and don’t be shy to comment below on things that worked/did not work for you!"
  }
]