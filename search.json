[
  {
    "objectID": "academicblog.html",
    "href": "academicblog.html",
    "title": "Academic Blog",
    "section": "",
    "text": "Posts\n\n\n\n\n Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis\n\n March 7, 2019 Academic Hockey Analytics R\nIn this post we explore 11 seasons (2007 - 2018) of team summary data from the Chicago Blackhawks of the National Hockey League (NHL). Our question was, “Are there any summary measures, such as goals scored or save percentage, that predict playoff performance or championship wins?”…\n\n\n\n My Reads\n\n January 3, 2019 Academic R\nI absolutely love reading and collecting books. Whenever I’m in a phase where I am constantly reading, I feel more creative and confident in my academic work. However, in the busy mix of an academic semester, it’s easy to put reading off to the side in lieu of data analysis, writing, and conferences. Below is a live tracker of my reading progress (I’m trying for around 20 pages per day) to help motivate me to read every day. I’ll do my best to update this weekly…\n\n\n\n Bootstrapping and Permutation Testing: A Shiny Dashboard\n\n September 2, 2018 Academic R Shiny Dashboard\nWe upgraded our shiny app to a shiny dashboard.\n\n\n\n A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury\n\n July 22, 2018 Academic Paper Review\nLast summer we published a paper in Psychological Assessment that presented methods to better characterize verbal responses on neuropsychological assessments. These methods were applied to a specific cognitive test called the Hayling Sentence Completion Test. We demonstrated that these new methods provide additional insights into the cognitive performance of individuals with mild to moderate traumatic brain injury (TBI)…\n\n\n\n Bootstrapping and Permutation Testing: A Shiny App\n\n June 7, 2018 Academic R Shiny App\nThis post serves as a brief overview of the difference between bootstrapping and permutation testing with a shiny app to visualize their differences. The example statistic we use here is a correlation; however, these techniques can be extended to a variety of statistics (e.g., t-test, ANOVA, PCA)…\n\n\n\n Simulating 2018 NCAA March Madness Tournament Performance in R\n\n March 20, 2018 Academic R\nA few of us graduate students decided it would be fun to simulate this year’s NCAA Men’s National Championship tournament using R. When we all met to discuss our method to simuluate tournament performance, we quickly realized several methods emerged…\n\n\n\n Multilevel Modeling in R with NHL Power Play Data\n\n February 11, 2018 Academic Hockey Analytics R\nThis post serves as both a tutorial on how to perform multilevel modeling in R and an analysis that provides insight to how powerplay goals in the NHL have changed across 10 years. Readers will learn how to explore data using plots, use R to fit linear models, and how to extract inferences from the results.\n\n\n\n Writing a Literature Review\n\n September 25, 2017 Academic\nI recently completed my first comprehensive literature review as part of a graduate school milestone project. Before starting, I sought guidance from online blogs and university workshops held by our library. The tips I learned from these resources were extremely helpful, but not everything worked for me. This is simply a list of things that worked/did not work for me during the reading and writing process of my literature review…\n\n\n\n Using Xbox Controllers and Sending EEG Triggers with E-Prime\n\n June 12, 2017 Academic R E-Prime EEG\nI just spent an entire week wrangling the E-Prime beast and I’ve lived to tell the tale. I was able to successfully integrate an Xbox controller to accept participant responses as well as communicate these to an EEG system as triggers. This wouldn’t have been possible without the help of benevolent bloggers and discussions on E-Prime’s Google group, so thank you all! I’ll point out these information sources along the way…\n\n\n\n ggplot2 Histogram Logo Fail\n\n May 15, 2017 Academic R\nWhen I first started creating this website (in WordPress), I thought it would be cool have have my own logo. Why not, right? I figured I’d go super nerdy and have a cool histogram plot with a density curve through it. I had just finished reading Hadley Wickham’s ggplot2 book and I was excited to put some of those newly learned skills to use…\n\n\n\n Stop Using Excel to Preprocess E-Prime Data\n\n May 15, 2017 Academic R E-Prime\nExcel is a great tool for processing data. Whether it’s calculating a quick average, standard deviation, or t-test, Excel is fast and simple to learn and use. In fact, most of my colleagues use Excel to preprocess their data from experiments programmed in E-Prime, a software tool for running psychological experiments. However, preprocessing your E-Prime data in Excel will…"
  },
  {
    "objectID": "cv.html#papers",
    "href": "cv.html#papers",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Papers",
    "text": "Papers\n\nJones, L. L., Kmiecik, M. J., Irwin, J. L., Morrison, R. G. (2022). Differential effects of semantic distance, distractor salience, and relations in verbal analogy. Psychonomic Bulletin & Review, 29, 1480–1491. https://doi.org/10.3758/s13423-022-02062-8 OSF Project\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., Hellman, K. M. (2022). Cortical Mechanisms of Visual Hypersensitivity in Women at Risk for Chronic Pelvic Pain. PAIN, 163(6), 1035-1048. doi: 10.1097/j.pain.0000000000002469 GitHub Repo OSF Project\nKmiecik, M. J., Perez, R., Krawczyk, D.C. (2021). Navigating Increasing Levels of Relational Complexity: Perceptual, Analogical, and System Mappings. Journal of Cognitive Neuroscience, 33(3), 357-376. https://doi.org/10.1162/jocn_a_01618 GitHub Repo\nKrawczyk, D. C., Han, K., Martinez, D., Rakic, J., Kmiecik, M. J., Chang, Z., Nguyen, L., Lundie, M., Cole, R., Nagele, M., Didehbani, N. (2019). Executive Function Training in Chronic Traumatic Brain Injury Patients: Study Protocol. TRIALS. 20:435 https://doi.org/10.21203/rs.2.273/v1 PDF\nKmiecik, M. J., Brisson, R. J., & Morrison, R. G. (2019). The time course of semantic and relational processing during verbal analogical reasoning. Brain and Cognition, 129, 25-34. https://doi.org/10.1016/j.bandc.2018.11.012\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nStockdale, L. A., Morrison, R. G., Kmiecik, M. J., Garbarino, J., & Silton, R. L. (2015). Emotionally anesthetized: media violence induces neural changes during emotional face processing. Social Cognitive and Affective Neuroscience, 10(10), 1373-1382. doi: 10.1093/scan/nsv025"
  },
  {
    "objectID": "cv.html#books",
    "href": "cv.html#books",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Books",
    "text": "Books\n\nPongpipat, E. E., Miranda, G. G., Kmiecik, M. J. (2019). A Practical Extension of Introductory Statistics in Psychology using R. Free online textbook written in bookdown and R. Read it here: https://rpsystats.com/"
  },
  {
    "objectID": "cv.html#preprints",
    "href": "cv.html#preprints",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Preprints",
    "text": "Preprints\n\nKmiecik, M. J., Tu, F. F., Clauw, D. J., Hellman, K. M. (2022). Multimodal Hypersensitivity Derived from Quantitative Sensory Testing Predicts Long-Term Pelvic Pain Outcome. MedRxiv, 2022.04.01.22272964. doi: https://doi.org/10.1101/2022.04.01.22272964 GitHub Repo OSF Project\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2020). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. MedRxiv, 2020.12.03.20242032. doi: https://doi.org/10.1101/2020.12.03.20242032"
  },
  {
    "objectID": "cv.html#dissertation",
    "href": "cv.html#dissertation",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Dissertation",
    "text": "Dissertation\n\nKmiecik, M. J. (2019). The Time Course of Meaning Construction with Varying Expectations."
  },
  {
    "objectID": "cv.html#papers-1",
    "href": "cv.html#papers-1",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Papers",
    "text": "Papers\n\nKmiecik, M. & Morrison, R.G. (2013). Semantic Distance Modulates the N400 Event-Related Potential in Verbal Analogical Reasoning. In M. Knauff, M. Pauen, & N. Sebanz (Eds.), Proceedings of the 35th Annual Conference of the Cognitive Science Society. Austin, TX: Cognitive Science Society."
  },
  {
    "objectID": "cv.html#talks",
    "href": "cv.html#talks",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Talks",
    "text": "Talks\n\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (Sept. 2021). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Flash talk presented at The University of Chicago Postdoctoral Symposium.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (Dec. 2021). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Data blitz talk presented at the International Association for the Study of Pain (IASP) Neuropathic Pain SIG (NeuPSIG) webinar series. Link to talk here.\nKmiecik, M. J. (November, 2019). Effective Presentations of Quantitative Information: A Study in Edward Tufte. Talk presented at the inaugural UT Dallas BrainHack, Dallas, TX.\nKmiecik, M. J., Nagele, M., Sharma, R., Vaughn, A. Thakur, P., & Krawczyk, D. C. (December, 2018). A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury. Talk presented at the Brain Performance Institute’s Inaugural TBI Research Showcase. Dallas, TX.\nKmiecik, M. J. & Krawczyk, D. C. (2017, October). Reasoning With Complex Relational Structures. Talk given at the UT Dallas School of Behavioral and Brain Sciences Cognition and Neuroscience program annual Fall retreat. Dallas, TX.\nKmiecik, M. & Morrison, R.G. (2013, August). Semantic Distance Modulates the N400 Event-Related Potential in Verbal Analogical Reasoning. Paper presented at the 35th Annual Conference of the Cognitive Science Society, Berlin, Germany."
  },
  {
    "objectID": "cv.html#workshops",
    "href": "cv.html#workshops",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Workshops",
    "text": "Workshops\n\nDutcher, A., Kmiecik, M.J., & Beaton, D. (2016, April). Analyzing multi-block data: A tutorial of Multiple Factor Analysis in R. Workshop presented at the Society for Applied Multivariate Research, Dallas, Texas. (MFA Workshop Slides / MFA Workshop R Code )"
  },
  {
    "objectID": "cv.html#poster-presentations",
    "href": "cv.html#poster-presentations",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Poster Presentations",
    "text": "Poster Presentations\n\nKmiecik, M. J., Tu, F. F., Darnell, S., Harber, K., Hellman, K. M. (2022, Nov.). Early cortical mechanisms of visual discomfort in premenarchal adolescents. Poster presented at the Society for Neuroscience Annual Meeting, San Diego, California.\nKmiecik, M. J., Tu, F. F., Clauw, D. J., Hellman, K. M. (2022, Sept.). Multimodal Hypersensitivity Predicts Pelvic Pain Outcome 4 Years Later. Poster presented at the International Association for the Study of Pain (IASP) World Congress on Pain, Toronto, Canada.\nDarnell, S., Tu, F. F., Harber, K., Sain, C., Kmiecik, M. J., Hellman, K. M. (2022, Sept.). Menstrual, Sensory, and Psychological Factors Predict New Onset Chronic Pelvic Pain. Poster to be presented at the International Association for the Study of Pain (IASP) World Congress on Pain, Toronto, Canada.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2022, March). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Poster presented at the Chicago Chapter of the Society for Neuroscience annual meeting, Chicago, IL.\nKmiecik, M. J., Tu, F. F., Silton, R. L., Dillane, K. E., Roth, G. E., Harte, S. E., & Hellman, K. M. (2021, Oct.). Cortical Mechanisms of Visual Hypersensitivity in Women At Risk for Chronic Pelvic Pain. Poster presented at the 24th annual scientific meeting of the International Pelvic Pain Society (IPPS), Baltimore, MD.\nKmiecik, M. J., Kim, L. M., Maguire, M. J., Hart, J., Krawczyk, D. C. (2020, May). The Time Course of Meaning Construction with Varying Expectations. Poster presented virtually at the Cognitive Neuroscience Society’s annual conference, Boston, MA.\nKim, L. M., Lundie, M. J., Kmiecik, M. J., Dasara, H., & Krawczyk, D. C. (2020, May). The Nuances of Norepinephrine: Salivary Alpha Amylase’s Role as a Biomarker in tDCS - Directed Judgment & Decision Making. Poster presented virtually at the Cognitive Neuroscience Society’s annual conference, Boston, MA.\nKmiecik, M. J., Kim, L. M., Maguire, M. J., Hart, J., Krawczyk, D. C. (2020, March). The Time Course of Meaning Construction with Varying Expectations. Poster was to be presented at the Society for Neuroscience Chicago Chapter annual meeting, Chicago, IL. * Cancelled due to COVID-19\nKim, L. M., Lundie, M. J., Kmiecik, M. J., Dasara, H., & Krawczyk, D. C. (2020, February). A Nudge of Norepinephrine: Investigating Salivary Alpha Amylase as a Biomarker of tDCS - Augmented Decision Making. Poster to be presented at the Society for Personality and Social Psychology’s annual conference, New Orleans, LA.\nNagele, M., Kmiecik, M. J., Chang, Z., Martinez, D., Juarez, M., Khachaturyan, M., Lundie, M., Kim, L., Didehbani, N., & Krawczyk, D. C. (2019, August). Using the Virtual Multiple Errands Task (VMET) to Assess Executive Functioning in Traumatic Brain Injury. Poster presented at the Military Health System Research Symposium annual meeting, Kissimmee, FL.\nChang, Z., Kmiecik, M. J., Martinez, D., Nagele, M., Lundie, M., Cole, R., Kim, L., Khachaturyan, M., Balloun, B., Juarez, M., Didehbani, N., Clover, M., Rule, G., Scott, G., & Krawczyk, D. C. (2019, August). Using a Virtual-Reality Based Rehabilitation Program to Enhance Daily Functioning in Veterans Surviving Traumatic Brain Injury. Poster presented at the Military Health System Research Symposium annual meeting, Kissimmee, FL.\nNagele, M. M., Kmiecik, M. J., & Krawczyk, D. C. (2019, April). Self-Awareness of Executive Dysfunction in Traumatic Brain Injury. Poster presented at UT Dallas School of Behavioral and Brain Sciences Psychological Sciences annual meeting, Dallas, TX.\nKmiecik, M. J., Martin A. D., Kim, L. M., Perez, R., Martinez, D. M., Pongpipat, E. E., & Krawczyk, D. C. (2019, March). The Influence of Reasoning Ability and Relational Cueing in Solving Relational Match-to-Sample Problems. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nKim, L. M., Kmiecik, M. J., Martinez, D. M., Martin A. D., & Krawczyk, D. C. (2019, March). The Similar Situations Task: Measuring Differing Levels of Reasoning Using Scene Analogies. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nKais, L.A., Lee, C., Kmiecik, M.J., Silton, R.L. (2018, February). The Influence of Affect on Interference Processing in Blocked and Mixed Presentations of a Stroop Color-Word Task. Poster presented at the annual meeting of the International Neuropsychological Society: Washington, D.C.\nKmiecik, M. J., Perez, R., Dandu, H., Krawczyk, D. C. (2017, August). Reasoning with Complex Relational Structures. Poster presented at the Fourth International Conference on Analogical Reasoning, Paris, France.\nMartinez, D. M., Kmiecik, M. J., Kamat, P. S., Schauer, G. F., Krawczyk, D. C. (2017, August). Aspects of Cognition and Clinical Symptomology in Analogical Reasoning. Poster presented at the Fourth International Conference on Analogical Reasoning, Paris, France.\nKmiecik, M.J., Martinez, D., Young, L.R., Krawczyk, D.C. (2016, November). Functional and Structural Neural Patterns in Mild-Moderate Chronic-Phase Traumatic Brain Injury. Archives of Physical Medicine and Rehabilitation, 97(10), e55. doi: 10.1016/j.apmr.2016.08.167\nMartinez, D., Kmiecik, M.J., Chapman, S., Krawczyk, D.C. (2016, November). Observing Changes in Cognition, Mood, and White Matter in Chronic TBI Using Multiple Factor Analysis After Cognitive Intervention. Archives of Physical Medicine and Rehabilitation, 97(10), e75. doi: 10.1016/j.apmr.2016.08.229\nKmiecik, M.J., Schauer, G.F., Martinez, D., & Krawczyk, D.C. (2016, April). The Similar Situations Task: An Assessment of Analogical Reasoning in Healthy and Clinical Populations. Poster presented at the Cognitive Neuroscience Society annual meeting, New York, NY.\nKmiecik, M. J., Chapman, S., & Krawczyk, D., (2015). Executive Functioning in Traumatic Brain Injury: A Detailed Investigation of the Hayling Test. Archives of Physical Medicine and Rehabilitation, 96(10), e97-e98. doi: 10.1016/j.apmr.2015.08.326\nStockdale, L., Morrison, R. G., Kmiecik, M., Palumbo, R., Garbarino, J., & Silton, R. L. (September, 2014). Negative valence systems distinctly influence bottom-up and top-down attentional processes. Paper presented at the Society for Psychophysiological Research, Atlanta, GA.\nPalumbo, R. T., Stockdale, L., Kmiecik, M. J., Silton, R. L., Morrison, R. G. (2014, July). The effect of short-term exposure to film violence on emotional facial processing. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nBrisson, R., Kmiecik, M.J., & Morrison, R. G. (2014, July). The effect of semantic and relational similarity on the N400 event-related potential in verbal analogical reasoning. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nIrwin, J.L., Jones, L.L., Kmiecik, M.J., Unsworth, N., & Morrison, R. G. (2014, July). Attention to detail predicts better verbal analogy performance. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nJones, L.L., Irwin, J.L., Kmiecik, M.J., Unsworth, N., & Morrison, R. G. (2014, July). Working memory and interference control in verbal analogy. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nMorrison, R. G., Kmiecik, M.J., Irwin, J.L., Unsworth, N., Jones, L.L. (2014, July). Working memory and crystallized knowledge in visual analogy. Poster presented at the 36th annual meeting of the Cognitive Science Society, Quebec City, Canada.\nBrisson, R.J., Kmiecik, M.J., Sweis, A.S. & Morrison, R.G. (2014, April). The Effect of Semantic and Relational Similarity on the N400 in Verbal Analogical Reasoning. Poster presented at the Cognitive Neuroscience Society annual meeting, Boston, MA.\nStockdale, L., Palumbo, R., Kmiecik, M., Silton, R.L. & Morrison, R.G. (2014, April). The Effects of Media Violence on the Neural Correlates of Emotional Facial Processing: An ERP Investigation. Poster presented at the Cognitive Neuroscience Society annual meeting, Boston, MA.\nKmiecik, M.J., Brisson, R.J., & Morrison, R.G. (2013, April). Semantic distance in verbal analogical reasoning modulates the N400 event-related potential. Poster presented at the Cognitive Neuroscience Society annual meeting, San Francisco, CA.\nMorrison, R.G., Kmiecik, M. & Bharani, K.L. (2012, April). When analogy is like priming: The N400 in verbal analogical reasoning. Poster presented at the Cognitive Neuroscience Society annual meeting, Chicago, IL.\nMorrison, R.G., Kmiecik, M. & Bharani, K.L. (2012, March). When analogy is like priming: The N400 in verbal analogical reasoning. Poster presented at the Society for Neuroscience Chicago Chapter 2012 annual meeting, Chicago, IL."
  },
  {
    "objectID": "cv.html#conference-organization",
    "href": "cv.html#conference-organization",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Conference Organization",
    "text": "Conference Organization"
  },
  {
    "objectID": "cv.html#paper-reviews",
    "href": "cv.html#paper-reviews",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Paper Reviews",
    "text": "Paper Reviews"
  },
  {
    "objectID": "cv.html#committee-involvement",
    "href": "cv.html#committee-involvement",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Committee Involvement",
    "text": "Committee Involvement"
  },
  {
    "objectID": "cv.html#professor-of-record",
    "href": "cv.html#professor-of-record",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Professor of Record",
    "text": "Professor of Record"
  },
  {
    "objectID": "cv.html#teaching-assistant",
    "href": "cv.html#teaching-assistant",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant"
  },
  {
    "objectID": "cv.html#guest-lectures",
    "href": "cv.html#guest-lectures",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "Guest Lectures",
    "text": "Guest Lectures"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew J. Kmiecik, PhD",
    "section": "",
    "text": "About Me\n\nI’m a cognitive neuroscientist doing a (second) postdoc at 23andMe researching genotypic and phenotypic markers of Parkinson’s Disease. In my first postdoc, I studied how individuals’ sensitivity across multiple sensory modalities predicted their long-term pelvic pain. In graduate school, I studied volitional control of semantic processing, executive dysfunction in chronic-phase traumatic brain injury, and human reasoning. My undergraduate work utilized EEG to understand the neural correlates and timecourse of analogical reasoning. In my spare time I enjoy reading, exercising, crossword puzzles, and the endless pursuit of new hobbies. Click here for my CV.\n\n\n\n My Blogs\n\n\n Academic Blog\n\nAcademic R E-Prime EEG Paper Review Shiny\nAs a cognitive neuroscientist, my projects are a blend of cognitive psychology, computer science, statistics, neuroimaging, and neuroscience principles. This blog serves mainly as resource to myself (so I can refer to how I did something), but also aims to teach others.\n\n The Blue Line\n\nHockey Analytics R\nI love the sport of hockey. In my spare time, I like to apply some of the statistical techniques that I’ve learned in my classes and research projects to hockey data. This hockey analytics blog serves to help hone my data science skills, teach R, and (hopefully) provide new insights to this amazing sport.\n\n\n\n Blog Posts\n\n\n\n Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis\n\n March 7, 2019 Academic Hockey Analytics R\nA collaboration post about using PCA to explore NHL data.\n\n\n My Reads\n\n January 3, 2019 Academic R\nA weekly updated tracker of my reading progress to motivate me to read more. Also, includes some of my favorite reads.\n\n\n Bootstrapping and Permutation Testing: A Shiny Dashboard\n\n September 2, 2018 Academic R Shiny\nA Shiny dashboard visualizing the differences between bootstrapping and permutation testing.\n\n\n A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury\n\n July 22, 2018 Academic Paper Review\nA blog post about a paper we published last summer. Topics: traumatic brain injury, cognition, and neuropsychology.\n\n\n Bootstrapping and Permutation Testing: A Shiny App\n\n June 7, 2018 Academic R Shiny\nA Shiny application visualizing the differences between bootstrapping and permutation testing.\n\n\n Simulating 2018 NCAA March Madness Tournament Performance in R\n\n March 20, 2018 Academic R\nWhen a bunch of grad students get together to simulate college basketball tournament performance using R.\n\n\n Multilevel Modeling in R with NHL Power Play Data\n\n February 11, 2018 Academic Hockey Analytics R\nA collaboration post about multilevel modeling in R using professional hockey data from the NHL.\n\n\n Writing a Literature Review\n\n September 25, 2017 Academic\nA post about writing my first literature review. Things that worked/did not work for me.\n\n\nUsing Xbox Controllers and Sending EEG Triggers with E-Prime\n\n June 12, 2017 Academic R E-Prime EEG\nA post about interfacing E-Prime 2.0 with an Xbox controller and passing stimulus/response triggers.\n\n\n ggplot2 Histogram Logo Fail\n\n May 15, 2017 Academic R\nA (probably) pointless post about forcing spaces in ggplot2 histograms.\n\n\n Stop Using Excel to Preprocess E-Prime Data\n\n May 15, 2017 Academic R E-Prime\nA post demonstrating an efficient data pipeline for E-prime users.\n\n\n The Last Decade: NHL’s Best and Worst Power Play Teams\n\n May 15, 2017 Academic Hockey Analytics R\nA hockey analytics post about 5 vs. 4 NHL powerplays from 2007-2017."
  },
  {
    "objectID": "post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury.html",
    "href": "post-A-Method-for-Characterizing-Semantic-and-Lexical-Properties-of-Sentence-Completions-in-Traumatic-Brain-Injury.html",
    "title": "A Method for Characterizing Semantic and Lexical Properties of Sentence Completions in Traumatic Brain Injury",
    "section": "",
    "text": "Short and Sweet\nLast summer we published a paper in Psychological Assessment that presented methods to better characterize verbal responses on neuropsychological assessments. These methods were applied to a specific cognitive test called the Hayling Sentence Completion Test. We demonstrated that these new methods provide additional insights into the cognitive performance of individuals with mild to moderate traumatic brain injury (TBI). Furthermore, these new measures were related to other cognitive abilities, such as verbal knowledge, processing speed, inhibitory control, working memory, and task switching. These methods can be extended to other assessments that involve word generations and clinical populations that exhibit cognitive impariments.\nTo read our paper, see:\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nYou can also read it on ResearchGate.\n\nIf you are interested in learning more…\n\n\nLong and Drawn (Out)\nTraumatic brain injuries (TBIs) occur when the head receives a harmful blow that results in disruptions to normal brain functioning. Individuals can recover following these injuries; however, some individuals’ symptoms persist long after the injury (> 3 months post-injury) and become chronic TBI.\nIndividuals with chronic TBI often report difficulties in completing everyday activities, such as going to the grocery store or holding a job. In terms of what is required cognitively, these tasks are quite complex and require planning, blocking distractions, knowledge, and sustained attention. Neuropscyhologists call these complex cognitive abilities executive functions and a large focus of TBI research is understanding how these cognitive functions are impacted and how these deficits affect behavior, such as difficulties in holding a job.\nDespite these self-reported impairments, it is difficult to capture these symptoms in a lab setting. In other words, an individual with TBI may report having difficulties in everyday life, but he or she may perform well on standardized neuropsychological assessments. Are the tests not sensitive enough? Are we measuring the right thing? Is TBI more difficult to assess?\nThe difficulties in understanding the effects of TBI is likely a mixture of the above concerns. The paper that this blog post covers addresses a specific concern: the sensitivity of existing tests. We took an existing neuropsychological assessment of executive functioning, called the Hayling Sentence Completion Test (Burgess & Shallice, 1996, 1997), and applied methods from the fields of cognitive science and linguistics to gain further insights about how TBI impacts performance.\nWhen taking the Hayling Test, individuals are read sentences with the last word missing. In the first section, participants are instructed to complete the sentence with a logical word. In the second section, participants are instructed to complete the sentence with an illogical word. For example:\n\n\n\n\n \n  \n    Section \n    Completion \n    Sentence Stem \n    Correct Completion \n  \n \n\n  \n    1 \n    Sensible \n    He mailed a letter without a \n    stamp. \n  \n  \n    2 \n    Unconnected \n    The captain wanted to stay with the sinking \n    banana. \n  \n\n\n\n\n\nPerformance on this test is guaged by the time to complete each section (faster is better) and the number of errors committed on section two (fewer errors is better). The frontal lobe is responsible for inhibiting distractions and following task instructions. The thinking behind this test is that those with damage to the frontal lobe will commit more errors on section two.\nOne potential source of improvement we saw was to introduce more objective methods in scoring errors, as unclear responses are often given.\nWe measured 2 different properties of the words:\n\nHow similar the sentence stem (He mailed a letter without a) and the generated word (stamp) were to each other using a technique called Latent Semantic Analysis (LSA; Landauer, Foltz, & Laham, 1998)\nHow frequently the generated words are used in everyday speech (word frequency) using the SUBTLEXus database (Brysbaert & New, 2009)\n\nWhat we discovered was that these measurements provide additional metrics that can be used to guage performance on the Hayling test. More specifically, other measures of cognitive performance were related to performance on these additional measurements. These additional measures were verbal ability, processing speed, inhibitory control, working memory, and task switching. Taken together, these results suggest that these additional measurements are closely related to high-level cognitive functions and can be used to better understand solving strategies and response patterns in clinical populations.\nNext steps in this line of research include further validating these measurements. Using our new measurements, we are currently conducting a study to compare those with chronic mild/moderate TBI to those without a history of TBI to further understand the effects of TBI on cognitive functioning. Our hope is that these methods can be extended to other linguistically based neuropsychological assessments and other clinical populations that experience cognitive impairments to better understand, diagnose, and formulate treatment plans.\n\n\n\nReferences\n\nBrysbaert, M., & New, B. (2009). Moving beyond Kucˇera and Francis: A critical evaluation of current word frequency norms and the introduction of a new and improved word frequency measure for American English. Behavior Research Methods, Instruments and Computer, 41, 977–990. http://dx.doi.org/10.3758/BRM.41.4.977\nBurgess, P. W., & Shallice, T. (1996). Response suppression, initiation and strategy use following frontal lobe lesions. Neuropsychologia, 34(4), 263-272. 10.1016/0028-3932(95)00104-2\nBurgess, P. W., & Shallice, T. (1997). The Hayling and Brixton tests. London, England: Pearson.\nKmiecik, M. J., Rodgers, B. N., Martinez, D. M., Chapman, S. B., & Krawczyk, D. C. (2018). A method for characterizing semantic and lexical properties of sentence completions in traumatic brain injury. Psychological Assessment, 30(5), 645-655. http://dx.doi.org/10.1037/pas0000510\nLandauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse Processes, 25, 259–284. http://dx .doi.org/10.1080/01638539809545028\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#purpose",
    "href": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#purpose",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "Purpose",
    "text": "Purpose\n\nThis post serves as a brief overview of the difference between bootstrapping and permutation testing with a shiny app to visualize their differences. The example statistic we use here is a correlation; however, these techniques can be extended to a variety of statistics (e.g., t-test, ANOVA, PCA)."
  },
  {
    "objectID": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#shiny-app",
    "href": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#shiny-app",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "Shiny App",
    "text": "Shiny App\n\nHere’s the shiny app used to illustrate these concepts. Click here to see the app in full screen.\n\nknitr::include_app(\"https://mattkmiecik.shinyapps.io/boot-perm-app/\", \n                   height = \"1400px\")\n\n\n\n\n\nBootstrapping\nWhen you think of boostrapping, think confidence intervals. Bootstrapping samples observations with replacement without breaking the relationship between measures (e.g., X and Y). The number of samples is equal to the number of observations (i.e., sample size). After sampling with replacement is finished, the statistic of interest, such as a correlation, is computed and stored.\n\nWhen you think of boostrapping, think confidence intervals.\n\nThis process explained above is then repeated hundreds or thousands of iterations resulting in a distribution of values for your statistic of interest. This distibution will be centered about the original statistical value that you computed before any resampling occured. In other words, the mean of these stored values will equal your observed statistic.\nAs with any distribution, you can calculate what are the lower bounds and upper bounds of values for a given percentage. This percentage is determined by the researcher/statistician/hockey analyst enthusiast and is called a confidence interval (95% is a common confidence interval). These bounds are usually reported in square brackets in the format: confidence interval % [lowerbound, upper bound]. For example, “There was a positive correlation observed between X and Y, r = .31, 95% CI [.21, .41].”\n\n\n\nPermutation Testing\nWhen you think of permutation testing, think of p-values. Permutation testing does not sample observations with replacement, but instead breaks the relationship between measures (e.g., X and Y). This is done by shuffling/randomizing/sampling the observed data points for one variable, while keeping the other (or others) intact. In terms of correlation, this would mean that X would be shuffled within the sample, while Y remained the original values. After the responses for one variable are randomized the statistic of interest, such as a correlation, is computed and stored.\n\nWhen you think of permutation testing, think of p-values.\n\nThis process explained above is then repeated hundreds or thousands of iterations resulting in a distribution of values for your statistic of interest. This distibution will not be centered about the original statistic value that you computed before any shuffling occured, but rather will be centered around the null. In terms of correlation, a null distribution would center about r = 0; meaning no linear relationship between variables.\nIn other words, a null distribution is created by shuffling the values in X but not Y. This is because the relationship has been broken between X and Y.\nA p-value is calculated by first counting the number of statistical values that are more extreme than your observed statistic. Put another way, how many times did the statistical value that emerged from a “null distribution” surpass your original computed statistic (before any shuffling). Then, you take the number of times that the null distribution is more extreme than your original value and divide it by the number of permutation iterations (number of observations in your null distribution).\nFor example, let’s say I ran a permutation test on a correlation of r = .5 and shuffled X, kept Y, computed their correlation, stored this value, and repeated this 100 times. Out of 100 times, there were 4 correlations that emerged that were greater than .5. Therefore, my p-value for this correlation would be 4/100 = 0.04."
  },
  {
    "objectID": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#shiny-app-code",
    "href": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#shiny-app-code",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "Shiny App Code",
    "text": "Shiny App Code\n\nBelow is the code that runs the shiny app:\n\n# Packages required ----\nlibrary(shiny) \nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tibble)\nlibrary(purrr)\nlibrary(broom)\nlibrary(MASS)\nlibrary(modelr)\nlibrary(shinythemes)\nlibrary(shinycssloaders)\n\n# Global aesthetics ----\n\n# Main color palette (Flatly bootstrap theme)\nflatlyPal <- c('#2C3E50', '#18BC9C', '#94A5A6', '#3498DC', '#F39C13', '#E74C3C') \n\n# loading spinner options\noptions(spinner.color = flatlyPal[1], spinner.type = 8)\n\n# Plot constant\nplotFinish <- theme(plot.title = element_text(hjust = 0.5, size = 15),\n                    text = element_text(size = 15),\n                    plot.caption = element_text(hjust = .5)\n                    )\n\n# Turning off scientific notation for p-values\noptions(scipen = 999)\n\n# UI ----\n\n# Define UI for application that draws a histogram\nui <- fluidPage(theme = shinytheme(\"flatly\"),\n  \n  # Title\n  h2(\"Bootstrapping and Permutation Testing\"),\n  \n  # Author info\n  h4(\"Matthew J. Kmiecik & Ekarin E. Pongpipat\"),\n  \n  # Blogpost info\n  p(\"See our \", \n    a(\"blog post\", href=\"https://mattkmiecik.com/post-Bootstrapping-and-Permutation-Testing-Shiny-App.html\"), \n    \"for more information about this shiny app.\"\n    ),\n  \n  # Sidebar with a slider input for number of bins \n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(inputId = \"rUser\",\n                  label = \"Correlation (r): \",\n                  min = -1,\n                  max = 1,\n                  value = .3,\n                  step = .1,\n                  ticks = FALSE),\n      sliderInput(inputId = \"sampleN\",\n                  label = \"Sample size (N): \",\n                  min = 10,\n                  max = 200,\n                  value = 50,\n                  step = 10,\n                  ticks = FALSE),\n      sliderInput(inputId = \"bootIters\",\n                  label = \"Bootstrap Iterations: \",\n                  min = 100,\n                  max = 2000,\n                  value = 100,\n                  step = 100,\n                  ticks = FALSE),\n      sliderInput(inputId = \"ciUser\",\n                  label = \"Bootstrap Confidence Interval: \",\n                  min = .80,\n                  max = .99,\n                  value = .90,\n                  step = .01,\n                  ticks = FALSE),\n      sliderInput(inputId = \"permIters\",\n                  label = \"Permutation Iterations: \",\n                  min = 100,\n                  max = 2000,\n                  value = 100,\n                  step = 100,\n                  ticks = FALSE), \n      width = \"2\"),\n    \n    # Plots\n    mainPanel(\n      flowLayout(\n        plotOutput(\"scatterPlot\") %>% withSpinner(),\n        plotOutput(\"bootPlot\")  %>% withSpinner(),\n        plotOutput(\"permPlot\")  %>% withSpinner()\n      )\n    )\n  )\n)\n\n# Server ----\nserver <- function(input, output) {\n\n  # Defined variables\n  vars    <- 2 # Minimum of 2 variables are required for correlation\n  mu <- rep(0, vars) # Means of all the vars are 0\n  \n  # Calculates sigma\n  re_sigma <- reactive({matrix(input$rUser, nrow = vars, ncol = vars)})\n  \n  # Establihses sigma = 1\n  re_sigma2 <- reactive({re_sigma() - diag(vars)*(input$rUser) + diag(vars)}) \n  \n  # Gathers correlations\n  re_rawvars <- reactive({as.tibble(mvrnorm(n = input$sampleN, \n                                            mu = mu, \n                                            Sigma = re_sigma2(), \n                                            empirical = T\n                                            )\n                                    ) %>%\n      rename(x = V1, y = V2)}\n      )\n  \n  # Bootstrapping procedure ----\n  re_bootResults <- reactive({re_rawvars() %>%\n    broom::bootstrap(input$bootIters) %>%\n    do(tidy(lm(scale(y) ~ scale(x), .))) %>%\n    filter(term != '(Intercept)')})\n   \n  # Confidence interval calculations ----\n  re_ciUserL <- reactive({(1-input$ciUser)/2})\n  re_ciUserU <- reactive({input$ciUser + re_ciUserL()})\n  re_ciObs <- reactive({quantile(re_bootResults()$estimate, \n                                 c(re_ciUserL(), re_ciUserU()))})\n\n  # Permutation testing procedure ----\n  # From: https://www.rdocumentation.org/packages/modelr/versions/0.1.1/topics/permute\n  re_perms <- reactive({re_rawvars() %>% permute(input$permIters, y)})\n  re_models <- reactive({map(re_perms()[[\"perm\"]], \n                             ~ lm(scale(y) ~ scale(x), data = .))})\n  re_tidyd <- reactive({map_df(re_models(), broom::tidy, .id = 'id') %>% \n      filter(term != '(Intercept)')})\n  \n  # Calculates p value from permutation testing\n  re_permPVal <- reactive({\n    if (input$rUser >= 0) {\n      (sum(re_tidyd()[[\"estimate\"]] >= input$rUser) + 1)/input$permIters\n    } else if ( input$rUser < 0 ) {\n      (sum(re_tidyd()[[\"estimate\"]] <= input$rUser) + 1)/input$permIters\n    }\n  }\n  )\n    \n  # Scatterplot ----\n  \n   output$scatterPlot <- renderPlot({\n     \n     # Caption for Scatterplot\n     corResP <- round(cor.test(re_rawvars()$x, re_rawvars()$y)$p.value, 3)\n     corResPFinal <- ifelse(corResP > .001, corResP, .001)\n     pSign <- ifelse(corResP > .001, '= ', '< ')\n     scatRes <- paste0('r (', input$sampleN-2, ') = ', input$rUser,\n                       ', p ', pSign,corResPFinal)\n     \n     # Plot\n     ggplot(re_rawvars(), aes(x, y)) +\n       geom_smooth(method = 'lm', se = T, \n                   color = flatlyPal[6], fill = flatlyPal[3]) +\n       geom_point(color = flatlyPal[1], alpha = 2/3) +\n       labs(x = 'X', y = 'Y', \n            title = 'Scatterplot', caption = scatRes) +\n       theme_classic() +\n       plotFinish\n     \n        })\n\n  # Bootstrap Plot ----\n   output$bootPlot <- renderPlot({\n     ggplot(re_bootResults(), aes(estimate)) +\n       geom_histogram(binwidth = .05, fill = flatlyPal[1]) +\n       geom_errorbarh(aes(xmin = re_ciObs()[[1]], xmax = re_ciObs()[[2]], \n                          x = input$rUser, y = nrow(re_bootResults())/10), \n                      height = 0, color = flatlyPal[2], size = 1) +\n       geom_point(aes(x = input$rUser, y = nrow(re_bootResults())/10),\n                  size = 4, color = flatlyPal[2]) +\n       labs(x = 'Correlation', y = 'Frequency', \n            title = 'Bootstrapping', \n            caption = paste0(input$ciUser*100, '% CI [', \n                             round(re_ciObs()[[1]], 2), ', ',\n                             round(re_ciObs()[[2]], 2), ']')\n            ) +\n       theme_classic() +\n       plotFinish\n   })\n   \n   # Permutation plot ----\n   output$permPlot <- renderPlot({\n     ggplot(re_tidyd(), aes(estimate)) +\n       geom_histogram(binwidth = .05, fill = flatlyPal[1]) +\n       geom_vline(aes(xintercept = input$rUser), \n                  color = flatlyPal[6], linetype = 2, size = .75) +\n       labs(x = 'Correlation', y = 'Frequency', \n            title = 'Permutation Testing',\n            caption = paste0('r = ', input$rUser, ', p = ', round(re_permPVal(), 5))\n            ) +\n       theme_classic() +\n       plotFinish\n     \n   })\n}\n\n# Application ----\nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#references",
    "href": "post-Bootstrapping-and-Permutation-Testing-Shiny-App.html#references",
    "title": "Bootstrapping and Permutation Testing: A Shiny App",
    "section": "References",
    "text": "References\n\nReaders who would like more theoretical or technical explanations of boostrapping and permutation testing are encouraged to read:\n\nEfron, B. (1979). Bootstrap methods: Another look at the jacknife. The Annals of Statistics, 7(1), 1-26.\nLudbrook, J., & Dudley, H. (1998). Why Permutation Tests are Superior to t and F Tests in Biomedical Research. The American Statistician, 52(2), 127-132. doi:10.1080/00031305.1998.10480551\nWright, D. B., London, K., & Field, A. P. (2018). Using Bootstrap Estimation and the Plug-in Principle for Clinical Psychology Data. Journal of Experimental Psychopathology, 2(2). doi:10.5127/jep.013611\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "",
    "text": "In this post we explore 11 seasons (2007 - 2018) of team summary data from the Chicago Blackhawks of the National Hockey League (NHL). Our question was, “Are there any summary measures, such as goals scored or save percentage, that predict playoff performance or championship wins?”\nWe explore these data using a variety of techniques, such as:\nZ-scores across time\nCorrelations\nPrincipal Components Analysis (PCA)"
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-import",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-import",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Data Import",
    "text": "Data Import\nLet’s first prepare the data for analysis. These data were downloaded from Corsica’s team stats tool. We’ve prepared these data for you and are available for import into R like this:\n\n# Link to raw data on Github\nlink <- \"https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/nhl-team-data-corsica.csv\"\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata <- GET(link)\nnhl_data <- read_csv(content(data, \"raw\"))\n\nRows: 331 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Team, Season\ndbl (26): GP, TOI, CF, CA, C+/-, CF%, CF/60, CA/60, GF, GA, G+/-, GF%, GF/60...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-preparation",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#data-preparation",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo make things simple, without losing information, we’ll use Chicago Blackhawk’s data from the 2007-2008 season up through the 2017-2018 season (11 years of data) and only a subset of the available metrics. These metrics include:\n\nGames Played (GP)\nTime on Ice (TOI)\nCorsi For (CF) = Shot attempts for at even strength: Shots + Blocks + Misses\nCorsi Against (CA) = Shot attempts against at even strength: Shots + Blocks + Misses\nGoals For (GF)\nGoals Against (GA)\nPenalty minutes served (PENT)\nPenalty minutes drawn (PEND)\nShooting Percentage (ShootPerc)\nSave Percentage (SavePerc)\n\n\n# Preparing Hawks data ----\nhawks_data <-  nhl_data %>%\n  select(Team:CA, GF, GA, PENT, PEND, ShootPerc = `Sh%`, SavePerc = `Sv%`) %>%\n  filter(Team == \"CHI\") %>%\n  separate(Season, into = c(\"Start_Year\", \"Season\")) %>%\n  mutate(Team = NULL, \n         Start_Year = NULL,\n         Season = as.numeric(Season)\n         )\n\n# Prints data\nnice_table(hawks_data)\n\n\n\n \n  \n    Season \n    GP \n    TOI \n    CF \n    CA \n    GF \n    GA \n    PENT \n    PEND \n    ShootPerc \n    SavePerc \n  \n \n\n  \n    2008 \n    80 \n    3423.00 \n    2600 \n    2629 \n    143 \n    130 \n    368 \n    365 \n    9.04 \n    91.92 \n  \n  \n    2009 \n    82 \n    3646.73 \n    3286 \n    2655 \n    146 \n    124 \n    335 \n    368 \n    7.45 \n    92.42 \n  \n  \n    2010 \n    82 \n    3881.33 \n    3784 \n    2907 \n    178 \n    147 \n    276 \n    299 \n    8.30 \n    90.53 \n  \n  \n    2011 \n    82 \n    3955.85 \n    3704 \n    3314 \n    167 \n    144 \n    239 \n    277 \n    8.06 \n    92.03 \n  \n  \n    2012 \n    82 \n    3944.77 \n    3698 \n    3290 \n    166 \n    164 \n    270 \n    287 \n    8.15 \n    90.99 \n  \n  \n    2013 \n    48 \n    2300.80 \n    2103 \n    1783 \n    105 \n    68 \n    151 \n    159 \n    8.97 \n    92.94 \n  \n  \n    2014 \n    82 \n    3979.35 \n    3890 \n    3129 \n    182 \n    149 \n    244 \n    231 \n    8.44 \n    91.39 \n  \n  \n    2015 \n    82 \n    3980.10 \n    4003 \n    3462 \n    150 \n    128 \n    225 \n    248 \n    6.87 \n    93.54 \n  \n  \n    2016 \n    82 \n    3985.37 \n    3680 \n    3585 \n    135 \n    141 \n    216 \n    234 \n    6.81 \n    92.81 \n  \n  \n    2017 \n    82 \n    4053.47 \n    3759 \n    3692 \n    164 \n    136 \n    206 \n    222 \n    8.17 \n    93.31 \n  \n  \n    2018 \n    82 \n    3940.41 \n    4199 \n    3821 \n    156 \n    173 \n    214 \n    261 \n    7.08 \n    91.82 \n  \n\n\n\n\n\nLet’s also prepare a table of notable events of every year in this data set for the Chicago Blackhawks, including their Stanley Cup wins (3) and their playoff success:\n\n# Initializing Chicago Blackhawks notable events table ----\n# SCW = Stanley Cup Wins\n# PF = Playoff Finish:\n#   0 = Did not make playoffs\n#   1 = Lost in first round\n#   2 = Lost in second round\n#   3 = Lost in conference finals\n#   4 = Lost in Stanley Cup final\n#   5 = Won Stanley Cup\nhawks_events <- tibble(Season = hawks_data$Season,\n                       SCW = factor(c(0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0)),\n                       PF  = factor(c(0, 3, 5, 1, 1, 5, 3, 5, 1, 1, 0))\n                       )"
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#z-scores",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#z-scores",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Z-Scores",
    "text": "Z-Scores\nPrior to exploring these data and how they’ve changed over time, we have to:\n\nAdjust all scores by the number of games played due to a shorted 2012-2013 season. We do this by dividing each metric by the number of games played\nCompute z-scores of all measures to facilitate comparisons\n\n\n# Preprocesses, adjusts, and z-scores hawks data ----\nhawks_data_long <- hawks_data %>% \n  gather(Meas, Val, -Season, -GP) %>%\n  group_by(Meas) %>%\n  mutate(Val_Adj = Val/GP,               # adjusts based on games played\n         Val_Zscore = scale(Val_Adj)     # computes z-scores\n         ) %>%\n  ungroup() %>%\n  mutate(sig = factor(ifelse(abs(Val_Zscore) > 1.96, \"p < .05\", \"p > .05\"))) %>% # z score > 1.96\n  inner_join(., hawks_events, by = \"Season\") # adds notable hawks events\n\nPlotting the z-scores of each measure across time allows us to compare across measures using a standardized unit:\n\n# Plots all measures together ----\nggplot(hawks_data_long, aes(factor(Season), Val_Zscore)) +\n  geom_path(aes(group = 1), color = rdgy[8]) +\n  geom_point(aes(color = sig, shape = SCW), size = 1.75) +\n  scale_color_manual(values = c(rdgy[3], rdgy[10]), name = \"Z-Score\") +\n  scale_shape_discrete(name = \"Stanley Cup Wins\") +\n  scale_y_continuous(breaks = c(-2, 0, 2), minor_breaks = NULL) +\n  coord_cartesian(ylim = c(-3,3)) +\n  theme_minimal() + \n  labs(x = \"\\n Season\", \n       y = \"\\n Measurement (Z-Score)\",\n       title = \"Chicago Blackhawk's Performance 2007-2018\"\n       ) +\n  facet_wrap(~Meas, nrow = 3) + \n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        legend.position = \"bottom\"\n        )\n\n\n\n\n\n\n\n\nWhen inspecting these plots in relation to the Chicago Blackhawk’s 3 Stanley Cup wins (triangles), we see very little that stands out. It seems like the 2012-2013 season was a unique one, such that it was a Stanley Cup winning team and their regular season Save Percentage and Shooting Percentage were much greater than their other years. Perhaps these two metrics are important for winning the Presidents’ Trophy (awarded to the NHL team finishing with the highest total points), which the Blackhawk’s received at the end of their 2012-2013 season.\nAdditionally, the Blackhawk’s 2007-2008 season seemed their worst season with the team recording statistically the least amount of time on ice (TOI), the most penalty minutes served (PENT), and the lowest Corsi For (CF) compared to the following decade of play. The 2007-2008 season was a tumultuous time for the Hawks, including a change in ownership and the later hire of Coach Joel Quennevill (see NYT article). It is likely that the change in ownership, hiring of Coach Q, and the acquisition of Jonathon Towes and Patrick Kane that led to the drastic improvement of team stats in the years following."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#correlations",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#correlations",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Correlations",
    "text": "Correlations\nNow let’s examine how the various team stats/measurements relate to each other by computing their correlations and visualizing them with a heatmap:\n\n# Converts back to wide format\nhawks_data_wide <- hawks_data_long %>% \n  select(Season, Meas, Val_Zscore) %>% \n  spread(Meas, Val_Zscore)\n\n# Computes correlations\nhawks_cors <- cor(hawks_data_wide)\n\n# Correlations to long format for plotting\nhawks_cors_long <- hawks_cors %>%\n  reshape2::melt() %>%\n  arrange(Var1, Var2)\n\n# Correlation heatmap\nggplot(hawks_cors_long, aes(x = Var1, y = Var2, fill = value)) +\n  geom_raster() + \n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.title = element_blank()\n        ) +\n  scale_fill_distiller(palette = \"BrBG\", \n                       limits = c(-1, 1), \n                       name = \"Correlation\"\n                       )\n\n\n\n\n\n\n\n\nAn interesting section that sticks out in the heatmap is the negative relationships between penalties, both served and drawn, and corsi, both for and against. This suggests that as the Hawks improved at drawing penalties, as well worsened at taking penalties, the amount of shots taken at the net decreased. This also came with a decrease in shots let up by the Hawks. My only explanation for this could be that as teams are on the powerplay, they become more selective with their shots, thus decreasing Corsi For and Corsi Against. This sort of aligns with our previous post that determined a decrease in NHL powerplay goals in the last decade. How is decline relates to Corsi is yet to be determined."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#eigen-values",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#eigen-values",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Eigen Values",
    "text": "Eigen Values\nOur original data matrix was 11 rows (years) x 9 columns (measures); therefore, the SVD of that matrix will produce 9 singular values/components – with the smaller side dictating the number of components produced.\nWhich components are important? Which ones explain the most variance? Are some of the components just noise?\nA good first pass at answering these questions is to examine the scree plot – plotting the components as a function of variability explained.\nTo do this, let’s first square the singular values (\\(\\Delta\\)) and sum them together. This is called inertia.\n \\[\nI = \\Sigma\\lambda = \\Sigma\\Delta^2\n\\] \n\ninertia <- sum(hawks_data_svd$d^2) # Calculates inertia\n\nNext, we’ll use the inertia to calculate the percentage of variability explained for each component and plot the scree:\n\n# Calculates values for the scree plot\nscree <- tibble(eigs = hawks_data_svd$d^2, \n                perc_explained = (eigs/inertia)*100,\n                comps = 1:length(eigs)\n                )\n\n# Scree plot\nggplot(scree, aes(factor(comps), eigs)) +\n  geom_point() +\n  geom_path(aes(group = 1)) +\n  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, \n                                         name = \"Explained Variance (%) \\n\"\n                                         )\n                     ) +\n  labs(x = \"\\n Components\", y = \"Eigenvalues \\n\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs we can see from the scree plot, the first 3 components comprise 91.75% of the total variance, suggesting that these first three components are important to understanding the structure of the data set. The remaining components explain only 8.25% of the variability and may perhaps be noise.\nIs there any way to statistically show that these components are important? One method is permutation testing. If you are unfamiliar with permutation testing, we recommend checking out our Shiny dashboard here.\nBriefly, to perform permutation testing we will scramble the information down the columns, thus breaking the relationship between the years and their measures. Then, we will re-compute the SVD for the new scrambled data matrix. We will repeat these steps 2,000 times, forming a null distribution of eigenvalues from which to compare our originally observed eigenvalues.\n\nperm_iters <- 2000  # number of permutation iterations\nset.seed(2019)      # sets seed for reproducible results\n\n# Initializes matrix to hold permutation results\nperm_res <- matrix(data = 0, nrow = perm_iters, ncol = length(hawks_data_svd$d))\n\nfor(i in 1:perm_iters){\n  \n  this_matrix <- apply(hawks_data_mat, 2, sample) # scrambles down columns\n  perm_res[i,] <- svd(this_matrix)$d^2 # saves eigenvalues to perm_res\n  \n}\n\nNow let’s visualize these results against the observed values. We’ll determine that a component is significant if its original observed eigenvalue is greater than 95% of the values derived from the null distribution (i.e., permutation testing).\n\n# Converts to long format for plotting\nperm_res_long <- as_tibble(perm_res) %>% \n  gather(comps, eigen) %>%\n  mutate(comps = as.numeric(gsub(\"V\", \"\", comps)))\n\n# Plots permutation results\nggplot(perm_res_long, aes(eigen)) +\n  geom_histogram(binwidth = 1) +\n  geom_vline(data = scree, \n             aes(xintercept = eigs), \n             color = rdgy[3], \n             linetype = 2\n             ) +\n  coord_cartesian(ylim = c(0, 800)) +\n  scale_y_continuous(minor_breaks = NULL) +\n  scale_x_continuous(minor_breaks = NULL) +\n  labs(x = \"\\n Eigenvalue\", \n       y = \"Frequency \\n\", \n       caption = \"\\n Note: Originally observed eigenvalues denoted by red dashed line.\"\n       ) +\n  facet_wrap(~comps) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nIt looks like the only components that have a shot at being greater than 95% of the null distribution are components 1 and 2. Let’s see if this is the case:\n\nscree_sig <- perm_res_long %>% \n  group_by(comps) %>% \n  summarise(ul = quantile(eigen, .975)) %>% # computes upper limit of 95%\n  inner_join(., scree, by = \"comps\") %>%\n  mutate(sig = ifelse(eigs>ul, \"p < .05\", \"p > .05\"))\n\nggplot(scree_sig, aes(factor(comps), eigs)) +\n  geom_path(aes(group = 1), color = rdgy[8], linetype = 2) +\n  geom_point(aes(color = sig), size = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~./inertia * 100, \n                                         name = \"Explained Variance (%) \\n\"\n                                         )\n                     ) +\n  scale_color_manual(values = c(rdgy[3], rdgy[9]), name = NULL) +\n  labs(x = \"\\n Components\", y = \"Eigenvalues \\n\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\nWhat we suspected was correct: given a null distribution/hypothesis, only components 1 and 2 were greater than 95% permuted eigenvalues. In other words, there is less than a 5% chance that the pattern of the results seen on components 1 and 2 are this extreme given that the null hypothesis is true (i.e., no relationship). Therefore, we’ll pay special attention to components 1 and 2.\nNow that we know what components may be more important than others, let’s take a look at what we can learn from examining factor scores for the rows (years) and the columns (measures).\n\n\nRow-wise Factor Scores\nWe can explore how the years are seen through the components by first scaling (multiplying) the U matrix by the singular values.  \\[F_{years} = U\\Delta\\]  The %*% operator performs matrix algebra:\n\nyears <- hawks_data_svd$u %*% diag(hawks_data_svd$d)  # scaling years data\nrownames(years) <- rownames(hawks_data_mat)           # adds rownames\n\nNow let’s visualize the factor scores for components 1 and 2:\n\n# Builds dataframe of row-wise factor scores with notable events\nyears_fs <- as_tibble(years) %>%\n  mutate(Season = hawks_data_wide$Season) %>%\n  left_join(., hawks_events, by = \"Season\") # imports notable events\n\n# Plots factor scores colored by Stanley Cup wins\n# tip: surrounding an action in R with () will automatically plot to screen\n(years_fs_scw <- ggplot(years_fs, aes(V1, V2, color = SCW)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  scale_color_manual(values = c(rdgy[9], rdgy[2]), name = \"Stanley Cup Wins\") +\n  geom_text_repel(aes(label = Season), segment.alpha = 0, show.legend = FALSE) +\n  pca_furnish +\n  theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\nThe above graph plots the years as they are seen through the first 2 dimensions of the PCA and colors the points based on Stanley Cup wins. The years cluster together into three distinct groups\n\nThe 2007-2008 and 2008-2009 seasons\nThe 2013 season\nThe 2010-2018 seasons\n\nwhile the 2010 season is near the origin – meaning it doesn’t contribute much to either group. Interestingly, there seems to be no clear pattern in regards to regular season play and Stanley Cup wins because these winning seasons are not clustered together.\nA pattern that does seem to emerge is the improvement of the team over time across principal component 1 (x-axis). Principal component 2 is dominated by the 2012-2013 season, probably due to the uniqueness of this season: shortened season due to lockout, Presidents’ trophy winners, and Stanley Cup champions.\nLet’s now color the points based on playoff performance:\n\n# Plots factor scores colored by playoff performance\n# tip: surrounding an action in R with () will automatically plot to screen\n(years_fs_pf <- ggplot(years_fs, aes(V1, V2, color = PF)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  scale_color_brewer(palette = \"Dark2\", direction = -1) +\n  geom_text_repel(aes(label = Season), segment.alpha = 0, show.legend = FALSE) +\n  pca_furnish +\n  theme(legend.position = \"bottom\")\n)\n\n\n\n\n\n\n\n\nOne pattern of results that emerges, somewhat unexpectedly, is that the 2007-2008 and 2017-2018 seasons are contrasted on principal component 1 (x-axis). In both seasons, the Hawks missed the playoffs (did not qualify); however, the 2018 season is more similar to the seasons in which the Hawks made the playoffs. These results also highlight the 2015 Stanley Cup winning season, which is surrounded by seasons with early first round playoff exits. This may be indicative of an average regular season in 2015 (in respect to the Hawks), but an exceptional post season performance."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-factor-scores",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-factor-scores",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Column-wise Factor Scores",
    "text": "Column-wise Factor Scores\nThrough the PCA, we are also able to examine the factor scores from the team metrics/stats (i.e., columns). Let’s scale the V matrix by the singular values to obtain the factor scores. \\[F_{metric} = V\\Delta\\] Again, the %*% operator performs matrix algebra:\n\nmetric <- hawks_data_svd$v %*% diag(hawks_data_svd$d) # scaling years data\nrownames(metric) <- colnames(hawks_data_mat)          # adds rownames\n\n# Converts to long format for plotting\nmetric_fs <- as_tibble(metric) %>% mutate(Metric = colnames(hawks_data_mat))\n\nNow let’s visualize the factor scores for components 1 and 2:\n\n# Plots the column-wise factor scores\n(metric_plot <- ggplot(metric_fs, aes(V1, V2)) +\n  geom_vline(xintercept = 0, alpha = 1/3) +\n  geom_hline(yintercept = 0, alpha = 1/3) +\n  geom_point() +\n  coord_cartesian(xlim = c(-7,7), ylim = c(-7,7)) +\n  geom_text_repel(aes(label = Metric), segment.alpha = 0) +\n  pca_furnish\n)\n\n\n\n\n\n\n\n\nSimilar to the year-wise factor scores, we obtained 3 clusters of team-wise metrics across components 1 and 2. Component 1 contrasts penalty minutes (both drawn and taken) with Corsi (both For and Against), time on ice (TOI) and goals against (GA).\nThis contrast suggests a negative relationship with these metrics (e.g., as penalty minutes served increased, Corsi against decreased). This is sort of paradoxical, but it is similar to what we saw above with the correlation plot.\nAdditionally, the second component is dominated by Shooting Percentage, Save Percentage, and Goals For – and contrasted by GA. It makes sense that increases in Shooting Percentage would be related to increases in Goals For, which would also be related to decreases in Goals Against."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#factor-score-comparisons",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#factor-score-comparisons",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Factor Score Comparisons",
    "text": "Factor Score Comparisons\nTo provide more context, let’s compare the year-wise and metric-wise factor scores side-by-side:\n\ngrid_furnish <- theme(legend.position = \"none\",\n                      axis.text = element_blank()\n                      )\n# Plots side-by-side\ngrid.arrange(years_fs_pf + grid_furnish, \n             metric_plot + grid_furnish, \n             nrow = 1\n             )\n\n\n\n\n\n\n\n\nFrom here we gain a rich perspective of how each year-wise clustering is described through patterns in team stats:\n\nCluster 1: the 2007-2008 and 2008-2009 seasons are well described by elevated penalty minutes (both drawn and taken)\nCluster 2: the 2011-2018 seasons are well described by elevated Corsi (both For and Against), Goals Against (GA), and time on ice (TOI)\nCluster 3: the 2012-2013 season was characterized by a high powered offense (elevated Shooting % and Goals For) and excellent defense (high Save % and low Goals Against)"
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#contributions",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Contributions",
    "text": "Contributions\nWe can also take a look at how much each season (row) or metric (column) contributes to each component. To calculate the contribution of each component, we square each factor score and divide it by the sum of squared factor, which is also the eigenvalue.\n \\[contribution = \\frac{f^2}{\\Sigma f^2} = \\frac{f^2}{\\lambda}\\] \nHere is a function that will compute the contributions:\n\n# Contribution calculator function ----\ncontribution <- function(vector, sign = TRUE) {\n  \n  vector_sq <- vector^2\n  vector_sq_sum <- sum(vector_sq)\n  \n  if (sign == TRUE) {\n    vector <- vector_sq/vector_sq_sum*sign(vector)\n  } else {\n    vector <- vector_sq/vector_sq_sum\n  }\n  \n}"
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#row-wise-contributions",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#row-wise-contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Row-wise Contributions",
    "text": "Row-wise Contributions\nNow let’s apply the above function to calculate the contributions of the years (rows):\n\n# calculates the contribution of each row (e.g., season) ----\ncontributions_years <- apply(years, 2, contribution)\n\n# add column names\ncolnames(contributions_years) <- paste0(\"Component_\",1:ncol(contributions_years))\n\n# converts contributions to long format ----\ncontributions_years_long <- contributions_years %>% \n  as_tibble() %>%                                   \n  mutate(Season = rownames(years)) %>%              # add Seasons \n  reshape2::melt(value.name = \"Contributions\") %>%  # ensure values are called contributions\n  group_by(variable) %>%\n  mutate(Contributes = ifelse(abs(Contributions) > abs(mean(Contributions)), \n                              \"Yes\", \n                              \"No\")\n         ) %>%\n  ungroup()\n\nNow let’s plot these contributions. Any years that have contributions greater than the mean are said to contribute to the variability on that component:\n\n# calculate the mean of each component for graphing ----\ncontributions_years_means <- contributions_years_long %>%\n  group_by(variable) %>%\n  summarise(mean = mean(Contributions)) %>%\n  ungroup()\n\n# looking only at first two components ----\nyear_contrib <- contributions_years_long %>% \n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n  \nyear_contrib_means <- contributions_years_means %>%\n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n\n\n# plot the contribution bars with the +/- mean\n# filtering only the first two components \nggplot(year_contrib, aes(x = Season, y = Contributions, fill = Contributes)) +             \n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip(ylim = c(-.9, .9)) +\n  geom_hline(data = year_contrib_means, \n             aes(yintercept = mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +  \n  geom_hline(data = year_contrib_means, \n             aes(yintercept = -mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +\n  scale_fill_manual(values = c(rdgy[8], rdgy[3])) +\n  labs(x = \"Season \\n\",\n       y = \"\\n Contributions\",\n       caption = \"\\nNote: Dashed lines represent the mean contribution.\"\n       ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\"\n        ) +\n  facet_wrap(~variable, \n             labeller = as_labeller(c(`Component_1` = \"Component 1\\n\", \n                                      `Component_2` = \"Component 2\\n\"\n                                      )\n                                    )\n             )\n\n\n\n\n\n\n\n\nSeason 2008 and 2009 contribute more than the mean season on the negative side of component 1, while seasons 2016 to 2018 contribute more than the mean season on the positive side of component 1. In other words, the 2008 and 2009 seasons are in stark contrast to the 2016-2018 seasons on component 1.\nFor component 2, only season 2013 contributes more than the mean season and on the negative side of component 2. Thus, component 2 likely reflects the 2013 season."
  },
  {
    "objectID": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-contributions",
    "href": "post-Exploring-11-Years-of-Chicago-Blackhawks-Data-using-Principal-Components-Analysis.html#column-wise-contributions",
    "title": "Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis",
    "section": "Column-wise Contributions",
    "text": "Column-wise Contributions\nNow let’s repeat this procedure for the metrics (columns):\n\n# calculates the contribution of each row (e.g., metric)\ncontributions_metric <- apply(metric, 2, contribution)\n\n# add column names\ncolnames(contributions_metric) <- paste0(\"Component_\",1:ncol(metric)) \n\n# converts contributions to long format\ncontributions_metric_long <- as_tibble(contributions_metric) %>%\n  mutate(Metric = rownames(contributions_metric)) %>%\n  reshape2::melt(value.name = \"Contributions\") %>%\n  group_by(variable) %>%\n  mutate(Contributes = ifelse(abs(Contributions) > abs(mean(Contributions)), \n                              \"Yes\", \n                              \"No\")\n         ) %>%\n  ungroup()\n\nAnd here are the contribution plots for the metrics on components 1 and 2:\n\n# calculate the mean of each component for graphing\ncontributions_metric_means <- contributions_metric_long %>%\n  group_by(variable) %>%\n  summarise(mean = mean(Contributions))\n\n# looking only at first two components ----\nmetric_contrib <- contributions_metric_long %>% \n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n  \nmetric_contrib_means <- contributions_metric_means %>%\n  filter(variable %in% c(\"Component_1\", \"Component_2\"))\n\n# plot the contributions bars with +/-mean\n# filtering only the first two components\nggplot(metric_contrib, aes(x = reorder(Metric, Contributions), y = Contributions, fill = Contributes)) +             \n  geom_bar(stat = \"identity\", color = \"black\") +\n  coord_flip(ylim = c(-.9, .9)) +\n  geom_hline(data = metric_contrib_means, \n             aes(yintercept = mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +  \n  geom_hline(data = metric_contrib_means, \n             aes(yintercept = -mean), \n             linetype = \"dashed\", \n             alpha = 0.5\n             ) +\n  scale_fill_manual(values = c(rdgy[8], rdgy[3])) +\n  labs(x = \"Metric \\n\",\n       y = \"\\n Contributions\",\n       caption = \"\\nNote: Dashed lines represent the mean contribution.\"\n       ) +\n  theme_minimal() +\n  theme(legend.position = \"bottom\"\n        ) +\n  facet_wrap(~variable, \n             labeller = as_labeller(c(`Component_1` = \"Component 1\\n\", \n                                      `Component_2` = \"Component 2\\n\"\n                                      )\n                                    )\n             )\n\n\n\n\n\n\n\n\nPenalty minutes drawn (PEND) and penalty minutes served (PENT) contribute more than the mean metric to the negative side of component 1; while Corsi Against (CA), Corsi For (CF), Goals Against (GA), and Time on Ice (TOI) contribute more than the mean metric to the positive side of component 1.\nGoals For (GF), Save Percentage (SavePerc), and Shooting Percentage (ShootPerc) contribute more than the mean metric to the negative side of component 2; while Goals Against (GA) contributes more than the mean metric to the positive side of component 2.\nGoals Against (GA) appears to contribute to both the positive side of component 1 and 2.\nTaking into both the row-wise and column-wise contributions confirm the factor score plots:\nComponent 1\n\nSeason 2008 and 2009 are associated with penalty minutes drawn and served (PEND and PENT, respectively) and both contribute negatively to component 1.\nOn the other hand, seasons 2016 to 2018 are associated with Corsi For (CF), Corsi Against (CA), Goals Against (GA), and Time on Ice (TOI) and contribute to the positively to component 1.\n\nComponent 2\n\nSeason 2013 is associated with Goals For (GF), Save Percentage (SavePerc), and Shooting Percentage (ShootPerc) and contribute negatively to component 2.\nOn the other hand, Goals Against (GA) contributes positively to component 2."
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "",
    "text": "In a previous post, I was interested in determining if there were any fluctuations in the frequency of powerplay goals (5 on 4) in the NHL across one decade (2007-2017). The statistial technique of choice was linear regression.\nHowever, there were a few issues with this statistical modeling procedure:\n\nThis model did not account for the variability within each team. In other words, the model did not appreciate each team’s change in powerplay goals across time individually. When we do not account for this variability (within-team variance), the unsystematic error is seemingly reduced, and subsequently an increase in our test statistic, decrease in p-value, and ultimately an increase in Type I error rate\nTime was treated as a factor (i.e., categorical variable). However, time is actually a continuous construct and may reveal more informaton when treated as such\n\n\nI had a feeling something was off here. So, I turned to one of my fellow classmates, Ekarin Pongpipat, to teach me a technique that could remedy this situation. The following is what we produced after two Saturday afternoons! Thanks again Ekarin!\n\nTo improve this model of NHL 5v4 powerplay goals, we utilized a multi-level linear regression model. The purpose of this post is three-fold:\n\nCreate a more accurate model of NHL powerplay goal fluctuations across the 2007-2017 decade\nExplore these data with plenty of plots\nProvide a tutorial of multi-level linear modeling in R"
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#setup",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#setup",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Setup",
    "text": "Setup\n\nLet’s first load some useful packages for analysis and plotting:\n\nlibrary(tidyverse); library(httr); library(broom); \nlibrary(gridExtra); library(knitr); library(kableExtra);\nlibrary(RColorBrewer)\n\nThese data were retrieved via Corsica shortly after the end of the 2017 regular season and are also available on this website’s Github repository. Feel free to pull these data from the repository like this:\n\nlink <- 'https://raw.githubusercontent.com/mkmiecik14/mkmiecik14.github.io/master/data/corsicaData_5v4.csv'\n\n# from https://stackoverflow.com/questions/60714074/download-csv-file-from-github-using-httr-get-request\ndata <- GET(link)\ndata5v4 <- read_csv(content(data, \"raw\"))\n\nThe Atlanta Thrashers moved to Winnipeg (Jets) after the 2011 season. Therefore, we combined the entries of the Atlanta Thrashers and the Winnipeg Jets. The Thrashers and the Jets are considered the same team in this analysis.\n\n# Combining ATL and WPG as ATL.WPG\ndata5v4$Team <- plyr::revalue(data5v4$Team, c(ATL = 'ATL.WPG', WPG = 'ATL.WPG'))\n\nFor the regression models, it is more desirable to have the year be represented as a continuous variable. This way an increase of 1 year is the equivalent of 1 year of hockey played. This will translate to more interpretable regression coefficents. Year was recoded as the starting year of the season (e.g., 20162017 = 2016):\n\ndata5v4 <- \n  data5v4 %>% \n  separate(Season, c('StartYear', 'EndYear'), sep = 4, remove = F) %>%\n  mutate_at(c('StartYear', 'EndYear'), funs(as.numeric))\n\nNext, we coded each team into its division following the 2013-2014 realignment scheme.\nNote: This has serious implications for our later analysis of conference and divisions. This realignment took place 6 years from the first season in this analysis. Therefore, identifying the teams using this realignment scheme is used for illustrative purposes.\n\n# Initializing variables to store team names\npacific <- c('S.J', 'CGY', 'L.A', 'ANA', 'EDM', 'VAN', 'ARI')\ncentral <- c('ATL.WPG', 'NSH', 'STL', 'DAL', 'COL', 'MIN', 'CHI')\nmetropolitan <- c('WSH', 'N.J', 'PHI', 'CBJ', 'PIT', 'NYR', 'NYI', 'CAR')\natlantic <- c('T.B', 'BOS', 'TOR', 'DET', 'MTL', 'FLA', 'OTT', 'BUF')\n\n# Creating a column to identify each team's division\ndata5v4 <- data5v4 %>%\n  mutate(\n    division = case_when(\n      data5v4$Team %in% pacific ~ 'Pacific',\n      data5v4$Team %in% central ~ 'Central',\n      data5v4$Team %in% metropolitan ~ 'Metropolitan',\n      data5v4$Team %in% atlantic ~ 'Atlantic')\n    )\n\nHere is a table detailing the 2013 realignment:\n\n# Creates the data frame for html table\ndivisions <- \n  data.frame(\n    Pacific = c(pacific, ''),\n    Central = c(central, ''),\n    Metropolitan = metropolitan,\n    Atlantic = atlantic\n    )\n\n# Prints table\nkable(divisions) %>%\n  kable_styling(bootstrap_options = c('striped', 'hover', 'responsive'))\n\n\n\n \n  \n    Pacific \n    Central \n    Metropolitan \n    Atlantic \n  \n \n\n  \n    S.J \n    ATL.WPG \n    WSH \n    T.B \n  \n  \n    CGY \n    NSH \n    N.J \n    BOS \n  \n  \n    L.A \n    STL \n    PHI \n    TOR \n  \n  \n    ANA \n    DAL \n    CBJ \n    DET \n  \n  \n    EDM \n    COL \n    PIT \n    MTL \n  \n  \n    VAN \n    MIN \n    NYR \n    FLA \n  \n  \n    ARI \n    CHI \n    NYI \n    OTT \n  \n  \n     \n     \n    CAR \n    BUF"
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#data-exploration",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#data-exploration",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Data Exploration",
    "text": "Data Exploration\n\nBefore analysis, let’s first explore these data via various plots. One question we are interested in is, “Are the amount of standard 5 vs. 4 regular season power play goals changing over time across 10 years (2007-2017) in the NHL?”\n\nggplot(data5v4, aes(StartYear, GF, group = 1)) +\n  geom_line(aes(group = Team, color = division), alpha = 2/3) + \n  geom_line(stat = 'summary', fun.y = 'mean', size = .9, color = 'red') +\n  scale_color_brewer(palette = 'Blues', direction = -1) +\n  labs(x = 'Season (Start Year)',\n       y = 'Total Season Power Play Goals (5v4)',\n       caption = 'Each team is a separate line; red line is the average') +\n  guides(color = guide_legend(title = 'Division')) +\n  scale_x_continuous(limits = c(2007, 2016), breaks = c(2007:2016)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot above demonstrates a dramatic decrease in power play goals, league-wide, during the 2012-2013 season. This is clearly a result of the shortened season, which would greatly reduce the amount of powerplay goal attempts, due to the 2012-2013 NHL lockout.\nLet’s control for this shortended season by dividing the powerplay goals (GF) by the amount of games played (GP) per season:\n\nggplot(data5v4, aes(StartYear, GF/GP, group = 1)) +\n  geom_line(aes(group = Team, color = division), alpha = 2/3) + \n  geom_line(stat = 'summary', fun.y = 'mean', size = .9, color = 'red') +\n  scale_color_brewer(palette = 'Blues', direction = -1) +\n  labs(x = 'Season (Start Year)',\n       y = 'Total Season Power Play Goals/Games Played (5v4)',\n       caption = 'Each team is a separate line; red line is the average') +\n  guides(color = guide_legend(title = 'Division')) +\n  scale_x_continuous(limits = c(2007, 2016), breaks = c(2007:2016)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nNow that we’ve controlled for the number of games played per season, we can see that there seems to be a decrease in the average powerplay goals (red line) scored per season for each subsequent year of play.\nFurther ways to plot these data include a boxplot and an average trend line:\n\nyaxlim <- c(.25,1) # Locks y-axis\n\n# Box plot\nboxplot <- \n  ggplot(data5v4, aes(factor(StartYear), GF/GP, group = StartYear)) +\n  geom_boxplot() +\n  labs(\n    x = 'Season (Start Year)',\n    y = 'Total Season Power Play Goals/Games Played (5v4)',\n    caption = ' '\n    ) +\n  coord_cartesian(ylim = yaxlim) +\n  theme_minimal()\n\n# Summary for line plot\ndata5v4_sum <- \n  data5v4 %>% \n  group_by(StartYear) %>% \n  summarise(\n    mean = mean(GF/GP),\n    sd = mean(GF/GP),\n    n = n(),\n    sem = sd/sqrt(n)\n    )\n\n# Line plot\nlineplot <- \n  ggplot(data5v4_sum, aes(factor(StartYear), mean, group = 1)) +\n  geom_line(alpha = 1/3) +\n  geom_pointrange(aes(ymax = mean + sem, ymin = mean - sem)) +\n  coord_cartesian(ylim = yaxlim) +\n  labs(x = 'Season (Start Year)', caption = 'Error bars are SEM') +\n  theme_minimal() +\n  theme(axis.title.y = element_blank())\n  \n# Arranges plots side-by-side\ngrid.arrange(boxplot, lineplot, ncol = 2)"
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#multi-level-linear-modeling",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#multi-level-linear-modeling",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Multi-level Linear Modeling",
    "text": "Multi-level Linear Modeling\n\nBefore we begin modeling these data, let’s visualize how these data are structured hierarchically (or in multiple levels).\n\n\n\n\n\n\n\n\n\nAccording to this hierarchy, the NHL has two conferences that each have two divisions, each of which have 7 or 8 distinct teams.\nWe’ll first model each team’s powerplay performance across 10 years separately in level 1. Then, we’ll model these performances between and within each conference and division in level 2.\n\nLevel 1\nThe level 1 modeling ignores the conference and division distinction. Each team will be modeled individually using a linear regression to predict powerplay goals per games played as a function of time (across 10 years). There are 30 teams in this dataset. Therefore, 30 distinct linear regressions will be performed; one for each team in the NHL. This allows us account for the variability of power plays within each team.\nThe code for this is made extremely efficient thanks to dplyr pipe-ing (%>%):\n\nmodTeam <-  \n  data5v4 %>% \n  nest_by(Team) %>%\n  mutate(level1 = list(lm((GF/GP) ~ StartYear, data = data)))\n\nExtracting omnibus level statistics, such as R2, for each of the 30 linear regressions and placing them in a easy-to-use dataframe was done using the ‘broom’ package function ‘glance’:\n\nlevel1Omni <- \n  modTeam %>% \n  summarise(broom::glance(level1)) %>% \n  ungroup() %>% \n  mutate(sig = p.value < .05)\n\nWe’ll use these omnibus estimates to examine all 30 regression models simultaneously via R2 estimates. These allow us to see how much variability in powerplay goals per games played was explained by time:\n\n# Color palette\nsigColorPal <- brewer.pal(11,'RdGy') # display.brewer.pal(11,'RdGy')\n\n# R^2 Plot\nggplot(level1Omni, aes(r.squared, reorder(Team, r.squared), color = sig)) +\n  geom_point(size = 2) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  labs(x = 'R-squared', y = 'Team') +\n  guides(color = guide_legend(title = 'p < .05')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe can also similarly examine the regression coefficient of time for each NHL team. This coefficient tells us what was the predicted change in powerplay goals per games played across the ten years (2007-2017).\nLet’s first extract these coefficients using dplyr pipeing (%>%) and the ‘broom’ package function ‘tidy’:\n\n# Extracting level 1 coefficients\nlevel1Coef <- \n  modTeam %>% \n  summarise(broom::tidy(level1)) %>% \n  ungroup() %>%\n  filter(term == 'StartYear') %>%   # Facilitates plotting\n  mutate(sig = p.value < .05)       # For later plotting\n\nNow let’s plot these coefficients ordered by size:\n\nggplot(level1Coef, aes(estimate, reorder(Team, -1*estimate), color = sig)) +\n  geom_point(size = 2) +\n  geom_errorbarh(\n    aes(xmin = estimate - std.error, xmax = estimate + std.error),\n    alpha = 1/2\n    ) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  labs(\n    x = 'Estimate (Yearly Change in Power Play Goals/Game)', \n    y = 'Team',\n    caption = 'SEM error bars'\n    ) +\n  guides(color = guide_legend(title = 'p < .05')) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nAs we can see from the plot above, each team had a different predicted rate of change in powerplay goals per games played from 2007-2017. The x-axis here represents the regression coefficient of time. For example, the Chicago Blackhawks had an estimate close to -.02. This means that for every increase in 1 year (every season) the model predicted a decrease of .02 powerplay goals per games played. However, this estimate is colored grey. This means that it was not found to be significantly different from zero.\nFor a more comprehensive picture, let’s look at all the teams in the NHL plotted with both the observed (actual) data and model predicted trendlines. To do this, let’s first extract the model fits by using dplyr pipeing (%>%) and the ‘broom’ package function ‘augment’:\n\n# Extracting level 1 model fits\nlevel1Fits <- \n  modTeam %>% \n  summarise(broom::augment(level1)) %>%\n  ungroup() %>%\n  mutate(sig = rep(level1Coef$sig, each = length(unique(StartYear))))\n\nAnd then plot them using ggplot:\n\nggplot(level1Fits, aes(StartYear, color = sig)) +\n  geom_line(aes(y = .fitted), alpha = 2/3) +\n  scale_color_manual(values = sigColorPal[c(9,2)]) +\n  geom_line(aes(y = `(GF/GP)`), color = 'black') +\n  labs(\n    x = 'Season (Start Year)',\n    y = 'Power Play Goals/Games Played (5v4)'\n    ) +\n  scale_x_continuous(breaks = c(2008, 2015)) +\n  facet_wrap(~Team, ncol = 5) +\n  guides(color = guide_legend(title = 'p < .05')) +\n  theme_minimal() +\n  theme(legend.position = 'bottom')\n\n\n\n\n\n\n\n\n\n\nLevel 2\nHere’s what we’ve learned from level 1:\n\nThe rate of change in powerplay goals per games played seems different from team to team\nThere seems to be a general decrease in powerplay goals per games played across time\n\nWe can take this a step further by asking the question: “Are powerplay goals per games played changing as a function of time across all NHL teams on average.”\nOne way to test this question is to see if the regression coefficients of time from level 1 across all 30 NHL teams, on average, differ from zero.\nOur coefficients are already in a dataframe named ‘level1Coef’, so let’s use this dataframe to test whether these estimates are different from zero:\n\nlevel2 <- lm(estimate ~ 1, data = level1Coef)\nsummary(level2)\n\n\nCall:\nlm(formula = estimate ~ 1, data = level1Coef)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.027532 -0.004719  0.003220  0.007924  0.017478 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.016550   0.002254  -7.342 4.35e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01235 on 29 degrees of freedom\n\n\nAs we can see from this summary output of the level 2 model, there is a significant decrease in powerplay goals per games played over time on average across teams.\nThis information was gleaned in the following ways:\n\nThe estimate is -0.0166\n\nBy being negative, the estimate tells us that powerplay goals are decreasing over time. Specifically, they are decreasing at a rate of 0.0166 goals per games played per year\n\nThis is statistically different from 0\n\nThe t-value is -7.34 with a p-value < .001\nOne way to visualize these results is to plot each team’s predicted model from level 1. Running a linear model on these predicted estimates is equivalent to their average over time. The following code illustrates this process in ggplot2. One plot calculates the linear regression model of these estimates, while the other simply averages them over each timepoint:\n\nlevel2Fits <- augment(level2) # Extracts model fits\n\n# LM plot\nlmPlot <- \n  ggplot(level1Fits, aes(StartYear, .fitted, group = Team)) +\n  geom_line(alpha = 1/3) +\n  geom_smooth(aes(group = 1), method = 'lm', color = 'purple', se = F) +\n  labs(\n    x = 'Season Start Year',\n    y = 'Level 1 Estimates (Power Play Goals/Games Played)',\n    title = 'Linear Model Plot'\n    ) +\n  theme_minimal()\n\n# Average plot\navPlot <- \n  ggplot(level1Fits, aes(StartYear, .fitted, group = Team)) +\n  geom_line(alpha = 1/3) +\n  stat_summary(\n    aes(group = 1), \n    fun = 'mean', \n    geom = 'line', \n    color = 'red', \n    size = 1\n    ) +\n  labs(x = 'Season Start Year', y = NULL, title = 'Average Plot') +\n  theme_minimal()\n\n# Plots them together\ngrid.arrange(lmPlot, avPlot, ncol = 2)\n\n\n\n\n\n\n\n\nAs we can see above, both these processes produce the same result. Powerplay goals are decreasing, on average, given each team’s performace across one decade of play (2007-2017).\n\n\nLevel 2 Extended\nSo far we’ve modeled each team individually using linear regression and then compared the regression coefficients from each of these 30 models. However, we’ve ignored the hierarchical structure of these data. Teams are nested within divisions that are nested within conferences that are nested within leagues (in this case, only one = the NHL).\nIn this section, we’ll explore the differences between conferences and divisions by adding these predictors to the level 2 analysis. Again, each team’s division and conference affiliation was determined using the 2013-2014 NHL realignment. This has serious consequences for interpreting the results and is shown for illustrative purposes.\nThe divisions and conferences are considered categorical variables. When using categorical variables, it is best to have a priori (planned) comparisons. Thus, we have the following planned predictions:\n\nThe Western conference will have higher powerplay goals/games played than the Eastern conference\nWithin the West, the Central division will have higher powerplay goals/games played than the Pacific\nWithin the East, the Metropolitan division will have higher powerplay goals/games played than the Atlantic\n\nIn order to incorporate these planned comparisons, we have come up with the following coding scheme:\n\n# Creates the data frame for html table\ncontrasts <- \n  data.frame(\n    Contrast = c(\n      'Conference (West vs. East)', \n      'West (Pacific vs. Central)', \n      'East (Metropolitan vs. Atlantic)'\n      ),\n    Pacific = c(.5, -.5, 0),\n    Central = c(.5, .5, 0),\n    Metropolitan = c(-.5, 0, .5),\n    Atlantic = c(-.5, 0, -.5)\n    )\n\n# Prints table\nkable(contrasts) %>%\n  kable_styling(bootstrap_options = c('striped', 'hover', 'responsive'))\n\n\n\n \n  \n    Contrast \n    Pacific \n    Central \n    Metropolitan \n    Atlantic \n  \n \n\n  \n    Conference (West vs. East) \n    0.5 \n    0.5 \n    -0.5 \n    -0.5 \n  \n  \n    West (Pacific vs. Central) \n    -0.5 \n    0.5 \n    0.0 \n    0.0 \n  \n  \n    East (Metropolitan vs. Atlantic) \n    0.0 \n    0.0 \n    0.5 \n    -0.5 \n  \n\n\n\n\n\nThese contrasts were made so that the estimates would be more interpretable. For example, the regression coefficient estimate for the West contrast will now represent the actual mean difference of Pacific vs. Central for powerplay goals/games played.\nUsing the level1Coef dataframe, let’s enter 3 contrast columns, one for each planned comparison, based upon the table above:\n\n# Division assignment\npacific <- c('S.J', 'CGY', 'L.A', 'ANA', 'EDM', 'VAN', 'ARI')\ncentral <- c('ATL.WPG', 'NSH', 'STL', 'DAL', 'COL', 'MIN', 'CHI')\nmetropolitan <- c('WSH', 'N.J', 'PHI', 'CBJ', 'PIT', 'NYR', 'NYI', 'CAR')\natlantic <- c('T.B', 'BOS', 'TOR', 'DET', 'MTL', 'FLA', 'OTT', 'BUF')\n\n# Assigning contrasts based on vectors above\nlevel1Coef <- level1Coef %>% ungroup() %>%\n  mutate(conf = case_when(level1Coef$Team %in% pacific ~ .5,\n                              level1Coef$Team %in% central ~ .5,\n                              level1Coef$Team %in% metropolitan ~ -.5,\n                              level1Coef$Team %in% atlantic ~ -.5),\n         west = case_when(level1Coef$Team %in% pacific ~ -.5,\n                              level1Coef$Team %in% central ~ .5,\n                              level1Coef$Team %in% metropolitan ~ 0,\n                              level1Coef$Team %in% atlantic ~ 0),\n         east = case_when(level1Coef$Team %in% pacific ~ 0,\n                              level1Coef$Team %in% central ~ 0,\n                              level1Coef$Team %in% metropolitan ~ .5,\n                              level1Coef$Team %in% atlantic ~ -.5))\n\nWe can now run a linear regression on the regression coefficients from level 1 again, but this time we’ll add the contrast coding scheme that we just created to the model in level 2:\n\nlevel2Div <- lm(estimate ~ conf + west + east, data = level1Coef)\nsummary(level2Div)\n\n\nCall:\nlm(formula = estimate ~ conf + west + east, data = level1Coef)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.024390 -0.008290  0.002767  0.010203  0.014928 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.016514   0.002293  -7.203 1.19e-07 ***\nconf         0.001077   0.004586   0.235    0.816    \nwest         0.007432   0.006698   1.110    0.277    \neast         0.005835   0.006265   0.931    0.360    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.01253 on 26 degrees of freedom\nMultiple R-squared:  0.07651,   Adjusted R-squared:  -0.03005 \nF-statistic: 0.718 on 3 and 26 DF,  p-value: 0.5502\n\n\nNow let’s walk through this new summary output.\nFirst, our intercept estimate from level 2 and level 2 extended are very similar. Powerplay goals/games played are still significantly decreasing at a rate of .0166 per each year of NHL gameplay.\nSecond, our three planned contrasts were named ‘conf’, ‘west’, and ‘east’. Taking a look at their estimates, they were all positive. This means that our predictions were in the right direction. However, none of these predictions were statistically significant. In other words, there is no significant difference in powerplay goals/games played between Western and Eastern conference teams, the Pacific and Central divisions, nor the Metropolitan and Atlantic divisions.\nNevertheless, let’s take a look at each of these in turn. First, we’ll extract the fitted values from the level 2 extended model for each team:\n\n# Extracts estimates from each team from level 2 extended\nlevel2DivTeam <- \n  level2Div %>% \n  broom::augment(se_fit = TRUE) %>% \n  mutate(team = level1Coef$Team) %>% \n  ungroup() %>%\n  mutate(\n    division = case_when(\n      team %in% pacific ~ 'pacific',\n      team %in% central ~ 'central',\n      team %in% metropolitan ~ 'metropolitan',\n      team %in% atlantic ~ 'atlantic'\n      ),\n    division = factor(division)\n    )\n\n# Re-orders factor for plotting\nlevel2DivTeam$division <- \n  factor(level2DivTeam$division, levels(level2DivTeam$division)[c(4,2,3,1)])\n\nAfter extracting this information, let’s take a look within the Western conference and compare the Pacific and Central divisions:\n\n# Gathers summary info\nwest_sum <- \n  level2DivTeam %>%\n  group_by(west) %>%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    ) %>%\n  filter(west != 0)\n\n# Plot\nggplot(west_sum, aes(factor(west), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Western Conference Divisions', \n    label = c('Pacific', 'Central')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe plot above depicts that, on average, the Pacific division is decreasing its powerplay goals/games played at a faster rate than the Central division. Although this is not a significant difference, it is still interesting.\nNext, let’s take a look at within the Eastern conference between the Atlantic and Metropolitan divisions:\n\n# Gathers summary info\neast_sum <- \n  level2DivTeam %>%\n  group_by(east) %>%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    ) %>%\n  filter(east != 0)\n\n# Plot\nggplot(east_sum, aes(factor(east), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Eastern Conference Divisions', \n    label = c('Atlantic', 'Metropolitan')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSimilar to the Western conference, the teams in the Atlantic division, on average, have a greater decrease in their powerplay goals/games played compared to their conference counterparts–the Metropolitan division teams. This is not a significant difference, though.\nFinally, let’s compare the Western and Eastern conferences:\n\n# Gathering summary information\nconf_sum <- \n  level2DivTeam %>%\n  group_by(conf) %>%\n  summarise(\n    mean = mean(.fitted),\n    sd = sd(.fitted),\n    n = n(),\n    sem = sd/sqrt(n)\n    )\n\n# Plot\nggplot(conf_sum, aes(factor(conf), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'NHL Conference', \n    label = c('East', 'West')\n    ) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterestingly, the Eastern conference is decreasing in their powerplay goals/games played at a faster rate than the Western conference teams. Again, not a significant difference though.\nIt may be more helpful to examine these slopes side-by-side:\n\n# Taking a look at the west contrast (Pacific -.5 vs. Central +.5)\nyax.lock <- c(-.025, -.01)\nwestPlot <- ggplot(west_sum, aes(factor(west), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Western Conference Divisions', \n    label = c('Pacific', 'Central')\n    ) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = 'Yearly Change in Power Play Goals/Games Played') +\n  theme_minimal()\n\n# Taking a look at the east contrast (Atlantic -.5 vs. Metropolitan +.5)\neastPlot <- ggplot(east_sum, aes(factor(east), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete(\n    'Eastern Conference Divisions',\n    label = c('Atlantic', 'Metropolitan')\n    ) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = NULL) +\n  theme_minimal()\n\n# Taking a look at the conference (East -.5 vs. West +.5)\nconfPlot <- ggplot(conf_sum, aes(factor(conf), mean, group = 1)) +\n  geom_point(size = 2) +\n  geom_line(alpha = 1/3) +\n  scale_x_discrete('NHL Conference', label = c('East', 'West')) +\n  coord_cartesian(ylim = yax.lock) +\n  labs(y = NULL) +\n  theme_minimal()\n\ngrid.arrange(westPlot, eastPlot, confPlot, ncol = 3)\n\n\n\n\n\n\n\n\nOr perhaps on a single plot detailing all 4 divisions across the 2 conferences:\n\nggplot(level2DivTeam, aes(division, .fitted, group = factor(conf))) +\n  geom_pointrange(aes(ymax = .fitted + .se.fit, ymin = .fitted - .se.fit)) +\n  geom_line(alpha = 1/3) +\n  coord_cartesian(ylim = yax.lock) +\n  scale_x_discrete(\n    labels = c('Pacific', 'Central', 'Metropolitan', 'Atlantic')\n    ) +\n  labs(\n    x = 'Division', \n    y = 'Yearly Change in Power Play Goals/Games Played',\n    caption = 'SEM error bars'\n    ) +\n  theme_minimal()"
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#conclusion",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#conclusion",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nHere are the following things we learned about 5 on 4 powerplays in the NHL from 2007-2017 in the regular season:\n\nThe number of goals scored (controlled for the amount of games played per season) significantly decreased across the decade by a rate of .0166\nThese decreases did not depend on division nor conference"
  },
  {
    "objectID": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#acknowledgements",
    "href": "post-Multilevel-Modeling-in-R-with-NHL-Power-Play-Data.html#acknowledgements",
    "title": "Multilevel Modeling in R with NHL Power Play Data",
    "section": "Acknowledgements",
    "text": "Acknowledgements\n\nA huge thank you to Ekarin Pongpipat who graciously and generously taught me multi-level linear modeling & helped write this blog post. His patience for my random tangents about hockey, bitcoin, and data visualization was unparalleled! Thank you!\nEkarin is a first year PhD student at the University of Texas at Dallas in the Cognition and Neuroscience program. He obtained his statistics training using a model comparison approach (MCA) during his time at San Diego State University. He also has taught and continues to teach statistics conceptually, mathmatically, and using statistics software such as SPSS and R. He has recently written a function to write a statistical results template in a APA format and will be adding more to his github repository soon. If you’re also interested in his research, check out his ResearchGate page.\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html",
    "href": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "",
    "text": "I just spent an entire week wrangling the E-Prime beast and I’ve lived to tell the tale. I was able to successfully integrate an Xbox controller to accept participant responses as well as communicate these to an EEG system as triggers. This wouldn’t have been possible without the help of benevolent bloggers and discussions on E-Prime’s Google group, so thank you all! I’ll point out these information sources along the way.\nI’ve programmed a simple experiment that checks the passing of triggers to a local EEG system that is available for download on this website’s github repo/data/. Click here to download the experiment file titled triggerTutorialExperiment.es2.\nMy general software/hardware specifications:\n\nE-Prime 2.0 Standard (2.0.10.356)\nPC: Windows 8.1 Pro (2GB RAM, 32-bit OS)\nEEG: ANT neuro eego mylab EEG amplifier"
  },
  {
    "objectID": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#determining-the-parallel-port",
    "href": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#determining-the-parallel-port",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Determining the parallel port",
    "text": "Determining the parallel port\nPST provides very good instructions for determining the port address that will be used to pass your triggers that depend on your requirements (see here). I’ll briefly demonstrate with the parallel port on my PC.\nNavigate to your PCs Control Panel > Hardware and Sound > Device Manager > Ports (COM & LPT) > Printer Port (LPT) > right click and select Properties > Resources tab. The address on my computer is 0378, and that translates to &H378. I will use this parallel port address in E-Prime to send triggers to the EEG computer."
  },
  {
    "objectID": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#onsetoffset-signaling",
    "href": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#onsetoffset-signaling",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Onset/Offset signaling",
    "text": "Onset/Offset signaling\nThis type of signaling is the most appropriate to trigger the onset of an event (e.g., fixation cross or stimulus). I’ll demonstrate these using the Onset/Offset inline codes mixed with List Attributes. First, let’s take a look at “triggerList”:\n\n\n\n\n \n  \n    ID \n    Nested \n    Procedure \n    Press \n    stimTrig \n    fixationTrig \n  \n \n\n  \n    1 \n     \n    triggerProc \n    1 \n    11 \n    9 \n  \n  \n    2 \n     \n    triggerProc \n    1 \n    11 \n    9 \n  \n  \n    3 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n  \n    4 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n  \n    5 \n     \n    triggerProc \n    1 \n    11 \n    9 \n  \n  \n    6 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n  \n    7 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n  \n    8 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n  \n    9 \n     \n    triggerProc \n    1 \n    11 \n    9 \n  \n  \n    10 \n     \n    triggerProc \n    2 \n    22 \n    9 \n  \n\n\n\n\n\nIn this experiment, 9 will denote the onset of a fixation cross (fixationTrig), 11 will denote the onset of the “Press 1” condition stimulus, and 22 will denote the onset of the “Press 2” condition stimulus. Accuracy will be determined via the “Press” column.\nThe basic concept is that a trigger will be passed at the onset of a stimulus, but must be reset to zero to toggle between triggers.\nTo pass the fixation crosss trigger (9), the following inline code should be placed before the fixation cross TextDisplay (in this example “fixTrigs”):\n' Triggers for fixation onset\nfixation.OnsetSignalEnabled = True\nfixation.OnsetSignalPort = &H378\nfixation.OnsetSignalData = c.GetAttrib(\"fixationTrig\")\n\n' Triggers for fixation offset\nfixation.OffsetSignalEnabled = True\nfixation.OffsetSignalPort = &H378\nfixation.OffsetSignalData = 0\nTo pass the triggers conditional on the correct stimulus comdition (11 or 22), the following inline code should be placed before the stimulus (in this example “stimTrigs”):\n' Triggers for stim onset\nstim.OnsetSignalEnabled = True\nstim.OnsetSignalPort = &H378\nstim.OnsetSignalData = c.GetAttrib(\"stimTrig\")\n\n' Triggers for stim offset\nstim.OffsetSignalEnabled = True\nstim.OffsetSignalPort = &H378\nstim.OffsetSignalData = 0"
  },
  {
    "objectID": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#using-writeport",
    "href": "post-Using-Xbox-Controllers-and-Sending-EEG-Triggers-with-E-Prime.html#using-writeport",
    "title": "Using Xbox Controllers and Sending EEG Triggers with E-Prime",
    "section": "Using WritePort",
    "text": "Using WritePort\nPassing triggers for participant responses conditional on their accuracy is a little tricky. In other words, I want to pass a trigger, say with a value of “1”, if the participant gets the trial correct, and “2” if he or she answers incorrectly. Additionally, I do not want the response to terminate the trial due to potential EEG artifacts from the screen changing so close to the response.\nThanks go to David McFarlane for a solution to this issue (see here). To satisfy the above needs of my experiment, I need to set the following properties of my stim TextDisplay:\n\nDuration: 3000ms (I’m limiting participants to 3 seconds)\nInput Device: Keyboard\nPreRelease: (same as duration) – this is very important (see here)\nCorrect: [Press] – will automatically determine accuracy\nTime Limit: (same as duration)\nEnd Action: (none) – this will not terminate the stimulus\n\n\n\n\nstim TextDisplay configuration.\n\n\nThe basic concept for this is that as soon as “stim” is presented on screen, we must program E-Prime to constantly look for a response. If one is detected, send triggers as soon as possible given the trial accuracy (i.e., 1 = correct, 2 = incorrect).\nTo do this I slightly modified David McFarlane’s script and placed it as an inline code following stim called “respTrigs”:\nDo While stim.InputMasks.IsPending()\n  Sleep 2 ' Will check every 2ms\nLoop\n\nIf Len(stim.RESP) > 0 Then\n  \n  If stim.Acc = 1 Then\n  \n    Debug.Print \"Got it right!\"\n    WritePort &H378, cor ' Signals correct\n  \n  Else\n    \n    Debug.Print \"Got it wrong!\"\n    WritePort &H378, icor ' Signals incorrect\n    \n  End If\n  \nElse\n\n  Debug.Print \"No Response!\"\n  WritePort &H378, nrp ' Signals no response\n  \nEnd If\n\nSleep 10 'Takes a break before resetting port\n\n' Reserts port\nWritePort &H378, 0\nAlternatively, if there are no events that require triggers following “stim”, “Sleep 10” could be removed and “WritePort &H378, 0” could be placed as the first line of code in the “fixTrigs” inline code.\nThe variables cor, icor, and nrp were set as global variables to be easily modified and stay consitent across the experiment:\nDim cor   As Integer\nDim icor  As Integer\nDim nrp   As Integer\nAnd then defined at the beginning of the experiment in the inline code “defTrigs”:\ncor   = 1\nicor  = 2\nnrp   = 3"
  },
  {
    "objectID": "post-Writing-a-Literature-Review.html",
    "href": "post-Writing-a-Literature-Review.html",
    "title": "Writing a Literature Review",
    "section": "",
    "text": "“Writing a literature review was so fun and easy!” - No one ever\n\nI recently completed my first comprehensive literature review as part of a graduate school milestone project. Before starting, I sought guidance from online blogs and university workshops held by our library. The tips I learned from these resources were extremely helpful, but not everything worked for me. This is simply a list of things that worked/did not work for me during the reading and writing process of my literature review. Feel free to comment below on what worked & and what didn’t work for you!\n\n\nStart!\n\nIf you are anything like me, you are reading this because you are apprehensive about starting the daunting task of a literature review, and understably so. I wanted to be as efficient as possible, so I scoured websites, attended workshops, and signed up for writing groups. After you are done reading this, take the first step and START! Although it’s great to seek advice and guidance for something like this, everyone has their own unique experience when writing. Starting is the best way to learn what works and does not work for you.\n\n\n\nTake digital notes\n\nI learned about 2 months into the process that physically writing notes into legal pads about the papers I was reading was inefficient and simply not working for me. You always hear the concept that “Writing notes on paper helps you remember it later on.” For me, not true. I type much faster than I write, so writing notes was painfully slow for me and I would often write illegibly. So I began taking notes in a single Word document, often limiting myself to one page of notes per paper. Importantly, the first line of every page had the authors, year, and title of the paper. Therefore, if I needed to find my notes on a particular paper, I could simply type COMMAND + F (I’m a mac user), and search the paper using the title. This is much faster than rifling through dozens of legal pad pages and paper notes. Also, searching for terms used in papers, such as keywords, was much easier in electronic format.\nWhen doing a literature review, COMMAND + F is your friend!\n\n\n\nHave a system\n\nDevelop a system and be ready to change it if you notice something not working for you. Briefly, here’s the system I used:\nI would find a paper that I wanted to read and enter its information into an Excel document. The spreadsheet looked something like this\n\n\n\n\n \n  \n    Author \n    Year \n    Title \n    Type \n    Category \n    SubCategory \n    Summary \n  \n \n\n  \n    Tolchok & Krovvy \n    1962 \n    Milkbars and Ultra-Violence \n    review \n    Category A \n    Category A-1 \n    This article is about... \n  \n  \n    Hoagey, Ray & Miranda \n    2017 \n    The Woes of Graduate Life \n    experimental \n    Category A \n    Category A-2 \n    This article is about... \n  \n  \n    Mulligan & Belmont \n    2016 \n    Ahead of their Time \n    meta-analysis \n    Category B \n    Category B-1 \n    This article is about... \n  \n\n\n\n\n\nI then would import the paper into EndNote and ensure the citation was correct. I then would read the paper and take notes in a Word document, with roughly 1 Word doc page per paper.\n\nTip: pressing COMMAND + SHIFT + 4 on a Mac will allow you make a selection on your screen for a screen shot. Adding screenshots of helpful figures or tables to your Word document emphasizes the important results and helps jog your memory.\n\nI learned that a note taking system is a critical component of a lit review. Being able to find your notes quickly, as well as other papers that are similar, is made very easy with electronic notes and an efficient system.\nI took notes using this format in my Word document:\n\nAuthor(s) - date - title of paper\nKeywords of the paper (tip: create your own if they are not in the paper already)\nIntro - things I learned\nMethods - important info to understand the results\nResults/Discussion - main findings and bottom line\n\nUsing this format made it very easy to COMMAND + F the paper I was looking for and other similar papers by searching keywords\n“Wait, why use the Excel document if you are taking notes in a Word document?” The Excel sheet is a great way to organize your materials globally by keywords, category, authors, and year of publication. Sorting your articles based on category and year helps group papers that are similar in topic and provides a historical perspective on a body of work. This may elucidate recurrent themes in your literature, such as debates across time, or provide unique insights into your review. I also used the Excel sheet as a way to mark down papers that I wanted to read, but wasn’t ready to find them yet. I simply entered TO READ in the cells to remind me.\n\n\n\nUse a citation manager\n\nAs mentioned in the “Have a System” section, I used EndNote to manage my citations. Citation managers are software programs that organize your papers/citations for easy citing while writing. Some other common reference managers are Mendeley and RefWorks.\nI strongly advise against doing citations “by hand.” That is, manually typing or copying from online citation generators into your reference page. Citation managers streamline this process by building your in-text citations and reference pages for you. An added bonus is that you can switch citation styles (e.g., APA to MLA) very easily. I don’t even want to think about the pain of doing that by hand!\nCitation managers will also ensure that your in-line and reference page citations are correctly furnished and ready for submission. Of course, I recommend checking your work and editing style guides as necessary to be in accordance with journal requirements. Not having to worry about citations is a HUGE advantage when writing a literature review. I encourage those that are apprehensive about making the switch to using citation managers to do it now. Putting the time in now to learn one of these programs will save you time and stress later!\n\n\n\nRead/Write everyday\n\nWriting a literature review is similar to preparing for a marathon. Training for the big race requires a slow and steady approach of practice everyday that gradually builds until race day. When reading and writing for a lit review, it is best to take a similar approach and try reading and/or writing every day for a certain amount of time (adjusted to your deadline, of course).\n\nTry to write every day and develop a rhythm.\n\nInitially, I began with a 1 hour per day approach. It was quite tough at the beginning and I would begin to fatigue fairly quickly, often not even finishing a whole paper or article. I’m also a slow reader and get distracted easily. However, as time went on, I began building my lit review endurance and I noticed my ability to read multiple papers and write more sentences increased. By the end, I was well exceeding the 1 hour per day benchmark I had set for myself. I think this was because I learned a lot about the literature, which in turn made paper reading faster, which in turn led me to organize and write more effectively, which in turn led to excitement that the end was near! Also, my deadline was approaching :)\n\n\n\nLimit distractions\n\nNo matter if you follow my advice about reading/writing everyday, not limiting distractions will prolong the process. In my experience, 2 hours of work with distractions was roughly equivalent to 1 hour of work with no distractions.\nI noticed a huge increase in the amount of work I was able to get done in 1 hour when I turned off notifications from my electronic devices and worked in an enviornment when no one could bother me. Turning my phone on silent helped, but turning off badge & banner e-mail notifications on my Mac REALLY helped. Try it out!\n\n\n\nFocus on the main idea\n\nI really wish someone would have told this to me when first starting. At the beginning, I began with review articles to introduce myself into the field and to see what has been already reviewed (this is something I highly recommend). I certainly didn’t want to reinvent the wheel, but contribute new things that I thought were important to the field.\nHowever, some of the reviews and book chapters I read were very well written and covered a wide range of topics that were important, but on the periphery of what I wanted to study. So I thought, “Well, if I’m going to do a literature review, it’s gotta be big and comprehensive. I should include this background information to make sure I have all my bases covered.” This was defintely not the way to go.\nI began reading too off topic and straying from my main ideas. This ended up making a few of my early sections seem unorganized and less connected. It wasn’t until I really focused in on my topic that I began gaining confidence and feeling less overwhelmed. Therefore, focusing in on my topic helped me, while worrying too much about the background work of others detracted from the paper’s coherence. Start focused, then zoom out if needed.\n\n\n\nEnjoy the process\n\nI know this sounds crazy, but look on the bright side! You are committing all this time and energy into learning how our understanding of a specific topic has evolved over time. You are going to come out confident and well-versed in an area of research—an expert of sorts. Realize that it takes time to get there and be patient with the process.\nHopefully this was helpful and don’t be shy to comment below on things that worked/did not work for you!\n\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "post-ggplot2-Histogram-Logo-Fail.html",
    "href": "post-ggplot2-Histogram-Logo-Fail.html",
    "title": "ggplot2 Histogram Logo Fail",
    "section": "",
    "text": "When I first started creating this website (in WordPress), I thought it would be cool have have my own logo. Why not, right? I figured I’d go super nerdy and have a cool histogram plot with a density curve through it. I had just finished reading Hadley Wickham’s ggplot2 book and I was excited to put some of those newly learned skills to use.\nFirst off, I’m not a design guy (if you can’t tell already), but I had a vision of this plot and I just went with it. After making the basic design, I couldn’t figure out how to get spaces between the bars in a ggplot2 histogram. After searching everywhere, the best advice I got was, “Histograms don’t have spaces between them, so why would you want them?”\nThen it dawned on me that I could plot two histograms, and then set their position to dodge and make one of the histograms invisible. So first, let’s plot two histograms:\n\nrm(list = ls())   # Clear workspace\nlibrary(ggplot2)  # Load ggplot2\n\nset.seed(14)      # Set seed for reproducible example\nsamplesA <- 400   # 400 samples in the distribution\n\n# Create dataframe for plotting\nlogoData <- data.frame(dist = rep(c('A', 'B'), each = samplesA), \n                       x = rnorm(2*samplesA))\n# Logo plot\nggplot(logoData, aes(x, fill = dist, colour = dist)) +\n  geom_histogram(aes(y = ..density..), \n                 position = 'dodge', \n                 bins = 13) +\n  geom_density(alpha = 1/5) +\n  scale_fill_manual(values = c('#1191e0', 'lightslategrey')) +\n  scale_color_manual(values = c('#222222', '#222222')) +\n  xlim(-4 , 4) +\n  theme_classic() +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title = element_blank(),\n        legend.position = 'none',\n        panel.background = element_rect(fill = '#eaeaea'))\n\n\n\n\n\n\n\n\nNow, let’s make one of the histograms invisible by setting its fill and color to NA, giving the illusion that there is only one histogram plotted:\n\nggplot(logoData, aes(x, fill = dist, colour = dist)) +\n  geom_histogram(aes(y = ..density..), \n                 position = 'dodge', \n                 bins = 13) +\n  geom_density(alpha = 1/5) +\n  scale_fill_manual(values = c('#1191e0', NA)) +  # Setting fill to NA\n  scale_color_manual(values = c('#222222', NA)) + # Setting color to NA\n  xlim(-4 , 4) +\n  theme_classic() +\n  theme(axis.text.x = element_blank(),\n        axis.text.y = element_blank(),\n        axis.title = element_blank(),\n        legend.position = 'none',\n        panel.background = element_rect(fill = '#eaeaea'))\n\n\n\n\n\n\n\n\nNow we are left with a single histogram density plot with spaces between the bars. However, when I tried to make this my website logo, I couldn’t figure out a way to get a high resolution image at 100 x 100 pixels. Instead, my brother made me a sweet bar graph with hockey sticks through it in Photoshop! Either way, through this journey I was able to come up with a solution to having spaces between histogram bars in ggplot2. Hopefully this is useful to someone!\n\n\n\n\n\n\nPlease enable JavaScript to view the comments powered by Disqus."
  },
  {
    "objectID": "post-my-experience-in-industry-so-far.html",
    "href": "post-my-experience-in-industry-so-far.html",
    "title": "My Experience in Industry (so far)",
    "section": "",
    "text": "I’m sitting on a flight from San Fransisco to Chicago, a trip I will frequently be taking in the next 5 months, and feeling somewhat compelled to begin blogging again. There’s an intense debate out there about academia vs. industry and it’s something I’ve given so much thought about since transitioning from an academic postdoc to an industry postdoc. There’s this overwhelming pressure building inside that I feel I must express, but I have no idea why. Is it to help others that were in a similar situation that I was one year ago? Is it to justify this decision to go to industry after pouring everything I had into an academic career that did not pan out? Is it to overcome the guilt and feeling of letting others down that supported me and encouraged me to continue in academia? Is it to show others how much pain and internal turmoil it can take on someone when going through this thought process? It perhaps is a mixture of these things; but somewhat a reflection of how academia was not the dream that was sold to me. Now if you are looking for a piece bashing academia, this is not the post for you. I have many wonderful friends and colleagues in academia and I respect and I think a lot of good can come from academia. However, currently the system is, in my mind, unsustainable, counterproductive, and incredibly difficult to navigate if you are first generation. Given that I am a white male, I feel inequipped to speak on behalf of those that are women, minorities, or part of the LGBTQ+ community; however, I imagine their struggles in academia are even more difficult than mine.\nHere are just some of my thoughts on the process of my transition to industry from academic.\n\nUndergrad\nFirst, let me provide some background on my experiences as everyones journey is different.\nI entered my undergraduate education at Loyola University Chicago on a premed track with ambitions to become a pediatrician, a goal I set for myself in the 5th grade. All my childhood I was told that I was smart, to stay in school, and to become a physician so that I can make a lot of money. [footnote here about how dad told me to stay in school when we were painting my condo after phd] I’m sure many first generation children of immigrant parents can relate. My parents were incredibly financially supportive of my undergraduate education and I luckily only had to take out a very reasonable amount of student loans; however, we had no idea what we were doing when applying to colleges. Which colleges were good? Which ones are big? Which have good acceptance rates for future medicine? Which had good research programs? Which have strong programs? I could not help feeling that I was surrounded with peers that were always a step ahead at knowing these things. It’s hard to imagine the advantages that having parents that went to university have on these formative years [I’ve heard that some people actually give their parents draft of their papers to proofread…what!]; the vocabulary was on a different level for me. I don’t think I even knew what a PhD was and what it entailed; how one received a PhD. I heard of this thing called graduate school, but it seemed mystical.\nNevertheless, I was told that in order to get into med school, that I should do research on campus. I had no idea how this enterprise worked and how to get started. Fortunately, I was taking an upper level psychology class (my major) and the professor emailed the class asking if anyone was interested in being a research assistant in her lab. This was it! This was my in! I immediately emailed back, and within a week I was getting trained in the lab and running participants. [annecdote here about rude encounter during testing]. I worked with these older students called “grad students”. I didn’t quite understand what a grad student, but assumed that they had graduated from college and they were doing extra school. My role was quite minimal, just running subjects through surveys (i.e., no data analysis or writing). I did this for a year, but did not return the subsequent Fall so that I can focus on increasing my GPA. [annecdote here about learning how to study]\nMy sophmore year was quite formative; I learned how to study properly and began discovering subjects in psychology that were fascinating to me, mainly cognitive psychology [Tim Miura footnote] and judgment and decision making. After gaining confidence in my study techniques and with my grades up, I decided it was time for me to join a lab again. I now knew the system; professors at a university do research in addition to their teaching and they need help to do so. So I got connected in a cognitive neuroscience laboratory that changed my life. I discovered that you could study someone’s cognition by presenting them with carefully designed experiments recording their brainwaves using scalp EEG. Out would come a flurry of numbers that you could organize and analyze in excel and various software programs. It was like solving a puzzle and I loved it. I found myself working late in the night on spreadsheets and data analysis, and studying seemed so boring. I was studying the unknown and it was thrilling. My undergraduate mentor gave me guidance on projects and I was largely independent in creating experiments, running participants, analyizng data. I learned about presenting at conferences, making posters, publishing papers, and what a grad studnet was. A creeping realization was happening; I no longer wanted to be a med student. I wanted to be a cognitive neuroscientist. I tried to suppress this as much as I could to appease my parents and everyone I told before that I was going to be a pediatrician. I even enrolled in an MCAT course and was seriosuly considering the MD/PhD. The thought of studying and memorizing things ad nauseum in med school was truly unappealing to me relative to what I could be doing in cogntiive neuroscience. I am glad that I followed my heart and chose cognitive neuroscience; although this dissapointed my parents, I knew I made the right decision. Since that day forward, I can say with confidence that ever since that day I have never “worked”. When I wake up each morning and take a shower, I am thinkng about the next step in my projects and am excited to work on them every single day. I tell people that I go to work, but it never feels like that. I realize that’s a rare thing to have, but I’m grateful to have found it so early in my life.\n\n\nGap Year\n[text here about lab manager position]\nSimilarly to applying to college, applying to grad school I was more in the dark. Although I had the help of my undergraduate mentors, I had no idea what I was doing. I was carving a path for myself. I had no idea what an R1 university was, how it was different than an R2. Which programs were good? What is the NIH? What are grants? I knew of famous schools, like Harvard or Yale, and I of course wasn’t going to apply to those, but I realize now that where you go to graduate school makes a huge difference. All 20% of acadmeic jobs are filled by these 6 univerisites [cite that nautre paper]. I’m not saying that you will be a unsuccessful academic if you do not attend these schools, but it makes you think about your odds or chances. Knowing these will help better prepare you for alternative career paths in graduate school something I highly recommend and think is what is broken in academia. I’ve realized that academia is a pyramid scheme[reference to wikipedia or something] and if you want to be sucessful you are going to have to play along. Hoepfully I do a good job eluciating how acadmeia is a ponzi scheme with examples I’ve noticed throughout.\nMy GRE scores were quite average and therefore didn’t get considered for programs with cutoffs. However, my research experience was quite strong: I had several conference posters, a conference paper with an oral presentation[footnote about how travel is cool in academia; point is to salute acadmia a little], which also won an award at my undergraduate institution, and a paper in press (middle author). I applied to 6 schools, got 3 interviews, and 1 acceptance (which I went). My undergraduate mentor went to graduate school with my graduate advisor which probably increased my chances as well. I was also a good fit for the lab so it worked out.\nDuring my 5 years in grad school my institution and department had immense success and were well funded through the NIH, DOD, and private donors, and reached R1 status quickly. I think I was decently productive in graduate school, publishing four papers during my 5.5 years. Of these, 2 were first author from independent projects, 1 was a first author paper I finished up from undergrad, and 1 was a protocol paper that I contributed a small piece and therefore 4th author. In comparison to some other bigger labs, I feel like my experience with papers and publishing has been an uphill battle. It seemed that if I wanted to have a paper I would have to be a first author. I an envious of the labs that are very collaborative and although projects are mainly spearheadded"
  },
  {
    "objectID": "theblueline.html",
    "href": "theblueline.html",
    "title": "The Blue Line",
    "section": "",
    "text": "My infatuation (some say obsession) with hockey began during my undergrad in the city of Chicago. I was fortunate enough to see the Blackhawks win the Stanley Cup when I was in Chicago not once, but TWICE! As I have grown to appreciate and love hockey, I found another outlet to further submerse myself in the sport — statistics! A few ideas I have played around with have been the power play, face-offs, potential trends across time, and seeing if there is anything that can predict the likelihood of a Stanley Cup victory!\nDoors closing… Welcome to the Blue Line.\n\n Posts\n\n\n\n\n Chicago Blackhawks: Scoring Density Analysis for the 2018-2019 Season\n\n July 19th, 2019 Hockey Analytics R\nWe’ve all heard Wayne Gretzky’s famous quote, “You miss 100% of the shots you don’t take.” Historically, the Chicago Blackhawks have taken Gretzky’s advice to heart and are one of the top four most efficient offenses since 2007 (for a statistical analysis, see my previous article)…\n\n\n Chicago Blackhawks: An Efficient Offensive Machine\n\n June 27th, 2019 Hockey Analytics R\nWhen watching Game 7 of the Dallas and St. Louis playoff series this year, I received a text from a friend of mine commenting on the sheer discrepancy of shots on goal between the teams…\n\n\n Coach Q vs. Jeremy Colliton: First 67 Games As Blackhawks’ Head Coach\n\n May 15th, 2019 Hockey Analytics R\nCoach Q had become Chicago’s steadfast leader and genius architect, constructing synergistic line combinations and masterfully adapting to player matchups throughout his 3 Stanley Cup winning tenure…\n\n\n Chicago Blackhawks: analyzing Patrick Kane’s career scoring tendencies\n\n April 9th, 2019 Hockey Analytics R\nThe Chicago Blackhawks‘ own Patrick “Showtime” Kane. The breaker of records, the daddy of dangles, the lighter of lamps. No matter what you call him, Kaner has had a tremendous year and is deserving of some hardware at the end of the season…\n\n\n Chicago Blackhawks 2018-19 Player Grades: Patrick Kane\n\n April 19th, 2019 Hockey Analytics R\nPatrick Kane battled through adversity at every corner this season. His ability to take over a game, frequent double shifts (including powerplays) and his knack for finding teammates with precise passes injected hope into the bloodstream of ’Hawks fans every time he took the ice…\n\n\n\n Exploring 11 Years of Chicago Blackhawk’s Data using Principal Components Analysis\n\n March 7, 2019 Academic Hockey Analytics R\nIn this post we explore 11 seasons (2007 - 2018) of team summary data from the Chicago Blackhawks of the National Hockey League (NHL). Our question was, “Are there any summary measures, such as goals scored or save percentage, that predict playoff performance or championship wins?”…\n\n\n\n Multilevel Modeling in R with NHL Power Play Data\n\n February 11, 2018 Academic Hockey Analytics R\nThis post serves as both a tutorial on how to perform multilevel modeling in R and an analysis that provides insight to how powerplay goals in the NHL have changed across 10 years. Readers will learn how to explore data using plots, use R to fit linear models, and how to extract inferences from the results.\n\n\n\n The Last Decade: NHL’s Best and Worst Power Play Teams\n\n May 15, 2017 Hockey Analytics R\nSo you are watching your favorite NHL team and they finally draw a powerplay. Awesome! The ice opens up with the missing player and it shouldn’t be too hard to score…right? As a previous water polo player, I know what it is like to endure the wrath of a furious coach after a missed man advantage opportunity. In my experience, coaches semed convinced that the man-up should lead to a goal 100% of the time. But how important is this seemingly advantageous situation in the NHL?…"
  }
]